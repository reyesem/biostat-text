# Generalized Estimating Equations {#sec-rm-gee}

{{< include _setupcode.qmd >}}

@sec-rm-terminology discussed how studies with repeated measures induce a correlation structure on the data.  The previous chapter addressed this correlation structure by developing a hierarchical model in stages, capturing the correlation structure as a by-product.  That is, we did not model the correlation directly; instead, by first describing the individual-level model and then allowing the parameters of that model to vary across individuals in the population, the correlation structure was handled naturally.  In this chapter, we consider an alternate approach where we focus on modeling the overall average trajectory and the the correlation structure directly.  While very general, this approach is particularly popular in longitudinal studies (@def-longitudinal-study).


## Correlation Structrues
@sec-rm-terminology defined the correlation structure as a summary of the relationship among the errors in the response.  In a mixed effects model (@def-mixed-effects-model), we considered the various sources of variability as contributing to the correlation structure; in this chapter, we are interested in modeling the structure directly.  As a result, we are interested in the overall impact of the sources of variability on this structure.  By specifying this structure, at least approximately, we are able to adjust the standard errors of our parameter estimates to obtain appropriate inference.

We can think of the correlation on the error terms of our model as a combination of between-subject and within-subject sources of variability.  While we may not be discussing the specific sources of variability, they are just as important as before to help us determine an appropriate form of the correlation structure.  We are generally willing to assume that observations from different subjects (or blocks) are independent; therefore, when we describe the correlation structure of the errors, we need only focus on the correlation of the observations from the same subject.  Further, we assume that the correlation structure is the same for every subject.  Therefore, there is only one correlation structure to be specified, and it will be shared across all subjects.

Recall from your introductory course that the correlation coefficient captures the strength and direction of the linear relationship between two variables and must be a value between -1 and 1.  If each subject has five observations (for example), then we need to describe the relationship between any pair of these five observations.  That is, we need $\binom{5}{2} = 10$ correlation coefficients.  We store these in a matrix

$$\Gamma = \begin{pmatrix} 
1 & \rho_{1,2} & \rho_{1,3} & \rho_{1,4} & \rho_{1,5} \\
\cdot & 1 & \rho_{2,3} & \rho_{2,4} & \rho_{2,5} \\
\cdot & \cdot & 1 & \rho_{3,4} & \rho_{3,5} \\
\cdot & \cdot & \cdot & 1 & \rho_{4,5} \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix},$$ {#eq-rm-correlation}

where the lower-half is determined from the upper-half since the correlation between the $i$-th and $j$-th observations is the same as the correlation between the $j$-th and $i$-th observations; that is, $\rho_{i,j} = \rho_{j,i}$.

:::{.callout-note}
## Properties of Correlation Matrices
Every correlation matrix has the following properties:

  1. It is a square matrix, and the dimension is determined by the number of observations within a subject.
  2. It is symmetric (the transpose is the same as the original matrix). That is, $\rho_{i,j} = \rho_{j,i}$.
  3. The diagonal entries are always 1; any value is perfectly correlated with itself.
  4. All off-diagonal elements must be between -1 and 1.
:::

A correlation matrix is very similar to a variance-covariance matrix; in fact, we can think of a correlation matrix as a standardized variance-covariance matrix.  

The correlation matrix in @eq-rm-correlation makes no assumptions about the structure of the off-diagonal elements (other than they are values between -1 and 1).  This is known as an unstructured form.

:::{#def-unstructured-correlation-structure}
## Unstructured Correlation Structure
An unstructured correlation structure suggests that the correlation between any two errors within a subject can take on any value.  We only require that it be a valid correlation matrix.
:::

If we think of each correlation as an additional parameter to estimate, then we have just specified an additional $\binom{m}{2}$ parameters to our model, where $m$ is the number of repeated observations on a subject.  We are essentially choosing not to place any structure on the correlation matrix and allow the data to completely determine the structure.  This can be useful if we have no intuition about the sources of variability; however, it requires a lot of data as we have added a large number of parameters to the model.

As in any model, there is tension between specifying a model which is flexible and one which is more tractable.  We often impose some simplifying structure on the correlation matrix.  While there are several possible structures, we discuss the most common.  On the other extreme from the unstructured correlation matrix discussed above is to assume the observations within a subject are independent of one another.

:::{#def-independence-correlation-structure}
## Independence Correlation Structure
An independence correlation structure suggests there is no correlation among any of the error terms within a subject.  If there are five observations within a block, this has the form

$$\Gamma = \begin{pmatrix} 
1 & 0 & 0 & 0 & 0 \\
\cdot & 1 & 0 & 0 & 0 \\
\cdot & \cdot & 1 & 0 & 0 \\
\cdot & \cdot & \cdot & 1 & 0 \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.$$
:::

While we already assume that observations between subjects are independent, the independence structure goes further and essentially says all observations are independent.  At first glance, this would seem to revert back to the classical regression model, which we have already established is inappropriate for repeated measures.  However, we will argue later that using such a structure does have some differences.

When we feel that observations from the same subject are associated primarily because they are from the same subject, and that the order of the observations within the subject is irrelevant, a compound symmetric correlation structure is appropriate.

:::{#def-compound-symmetric-correlation-structure}
## Compound Symmetric Correlation Structure
A compound symmetric correlation structure, also known as an _exchangeable_ correlation structure, suggests the correlation between any two errors within a subject is equal.  If there are five observations within a block, this has the form

$$\Gamma = \begin{pmatrix} 
1 & \rho & \rho & \rho & \rho \\
\cdot & 1 & \rho & \rho & \rho \\
\cdot & \cdot & 1 & \rho & \rho \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.$$
:::

The compound symmetric structure adds only one additional parameter to our model and actually models well many scenarios.  When we do believe that the order of the observations within a subject is important, and that observations occurring closer together (generally in time) are more highly correlated than observations further apart in time, an autoregressive structure is appropriate.

:::{#def-autoregressive-correlation-structure}
## Autoregressive Correlation Structure
An autoregressive correlation structure suggests the correlation between two observations diminishes as the observations get further apart (generally, further apart in time).  We generally only consider the autoregressive structure of degree 1 here; if there are five observations within a block, this has the form

$$\Gamma = \begin{pmatrix} 
1 & \rho & \rho^2 & \rho^3 & \rho^4 \\
\cdot & 1 & \rho & \rho^2 & \rho^3 \\
\cdot & \cdot & 1 & \rho & \rho^2 \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.$$
:::

The autoregressive structure is borrowed from the time-series literature.  It is primarily useful when we are taking observations somewhat close together in time.  Like the compound symmetric structure, it only adds a single parameter to the model.

Regardless of which of the structures we believe is beneath the data, we also assume stationarity.

:::{#def-stationarity}
## Stationarity
The assumption of stationarity states that the correlation structure does not depend on time, only the distance between the observations.
:::

Essentially, stationarity says that at no point does the structure evolve as the study continues; that is, we cannot include a parameter such as $\rho^{(\text{time})}$.  

Choosing an appropriate structure is often guided by discipline expertise regarding how the sources of variability combine and impact the relationship between the responses.  When multiple sources of variability have competing structures, we generally adopt the structure of the "dominant" source.  However, it turns out that the choice of the structure need not have a large impact on the analysis --- simply indicating in the analysis that there is a potential for correlation can be sufficient.  This is the idea behind the approach we describe.


## The Key to Success of Generalized Estimating Equations
In the previous section, we considered models for the correlation structure that result from the combination of the various sources of variability in the data generating process.  This structure will be used within the generalized estimating equation (GEE) approach.

:::{#def-gee}
## Generalized Estimating Equations (GEE)
Generalized estimating equations can be used to estimate the parameters of a model while accounting for the correlation among observations.  In addition to specifying a model for the overall average response, a "working" structure is specified for the correlation of observations from the same subject.  The working structure is updated during the estimation process and used to adjust the standard errors of the parameter estimates in the mean model.
:::

Recall that our inference on the parameters requires us to compute the variance-covariance matrix of the corresponding parameter estimates.  When a model is estimated using generalized estimating equations, the model we specify for the correlation structure is known as the "working" correlation matrix; this is then updated using the observed data when computing the variance-covariance matrix.  As a result, the variance-covariance matrix we use is not based solely on the specified model but is a blend of the model specified and the observed data; this is known as the robust sandwich estimator.

:::{#def-robust-sandwich-estimator}
## Robust Sandwich Estimator
The robust sandwich estimator of the variance-covariance matrix of the parameter estimates from the mean model balances the relationship between the parameter estimates specified by the model (and the "working" correlation matrix) with the relationship suggested by the observed data.  Specifically, it has the form

$$\widehat{\bs{\Sigma}} = \widehat{\bm{U}} \widehat{\bm{U}}^{-1/2} \bm{R} \widehat{\bm{U}}^{-1/2} \widehat{\bm{U}}$$

where $\bm{U}$ represents the model-based variance-covariance matrix if the structure specified by the working correlation matrix were completely correct, and $\bm{R}$ represents the correction factor estimated from the residuals (an empirical estimate).  
:::

:::{.callout-tip}
## Big Idea
The use of the robust sandwich variance-covariance estimator is what makes the GEE approach unique and so powerful.
:::

While the structure of $\bm{U}$ is beyond the scope of this text, we can think of it as what the computer does by default when we specify a model under the classical conditions.  Essentially, the use of the robust sandwich estimator in the GEE framework means our posited correlation structure need not be correct; it is okay if $\bm{U}$ is wrong.  With enough data, the inference will be the same regardless of the structure we choose.  What we are really specifying is that there is a potential for correlation among these observations.  Of course, the better the specified model for the correlation structure, the less adjustment that is needed and the more powerful the results.

:::{.callout-note}
It is the use of the robust-sandwich estimator that makes specifying the "independent" correlation structure different than assuming the classical regression model.  In classical regression, inference is based on assuming independence.  In a GEE framework, the correlation structure will be updated after we assume independence.
:::

:::{.callout-note}
While we are discussing the use of the robust-sandwich estimator as a way of adjusting for the correlation present, we note that this will also adjust for violations in constant variance as a result.
:::

Consider the study to investigate the impact of four methods of delivering a pancreatic enzyme supplement (@exm-rm-enzyme-expanded).  In the GEE approach, we focus on specifying a model for the overall mean response; that is, our modeling is very similar to what we would do if we ignored the correlation altogether:

$$(\text{Fecal Fat})_{i,j} = \beta_0 + \beta_1 (\text{Tablet})_{i,j} + \beta_2 (\text{Coated})_{i,j} + \beta_3 (\text{Uncoated})_{i,j} + \varepsilon_{i,j}.$$ {#eq-rm-mixed-gee}

Notice that our model does acknowledge that multiple observations were collected on each subject (we used both an $i$ and $j$ index).  The difference between the GEE approach and the classical approach is that we now specify a correlation structure for the errors.  As described in @exm-rm-enzyme-expanded, there are four observations within each participant.  Since we believe that diet is the primary reason for differences in fecal fat across individuals, the primary source of variability is the participant.  This leads us to posit the following compound symmetric working correlation structure:

$$\Gamma = \begin{pmatrix} 1 & \rho & \rho & \rho \\
\cdot & 1 & \rho & \rho \\
\cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & 1 \end{pmatrix}.$$

We would then be sure to use the robust-sandwich estimator when computing the standard errors of the parameter estimates in @eq-rm-mixed-gee.

```{r}
#| label: rm-gee-enzyme-glm
#| include: false


fit1.enzyme <- lm(`Fecal Fat` ~ Supplement, data = enzyme)
fit0.enzyme <- lm(`Fecal Fat` ~ 1, data = enzyme)

aov.enzyme <- anova(fit0.enzyme, fit1.enzyme)
```

```{r}
#| label: rm-gee-enzyme-gee
#| include: false

enzyme <- enzyme |>
  arrange(Subject)

gee.enzyme <- geeglm(`Fecal Fat` ~ Supplement, 
                     id = Subject,
                     data = enzyme,
                     corstr = 'exchangeable')

K <- matrix(
  c(0, 1, 0, 0,
    0, 0, 1, 0,
    0, 0, 0, 1),
  nrow = 3, ncol = 4, byrow = TRUE
)

aov.gee <- linearHypothesis(gee.enzyme, K)
```

When we fit a model using generalized estimating equations, we are fitting a semiparametric model --- specifying the model for the mean response and the correlation structure.  Inference on parameters is then carried out using large-sample theory.  Recall that we concluded there was no evidence (`r display_pval(aov.enzyme, 'Fecal Fat ~ Supplement')`) the average fecal fat was associated with the type of supplement when we ignored the correlation in the data.  However, if we account for the correlation estimating the parameters using generalized estimating equations with a compound-symmetric working correlation matrix, we have strong evidence (`r display_pval(aov.gee, 'SupplementTablet')`) the average fecal fat differs for at least one of the supplement types.  Correctly accounting for the correlation structure resulted in a more powerful analysis.

## Comparison of GEE and Mixed Effects Approaches
While both mixed effects models and estimation via generalized estimating equations account for the correlation structure, the two approaches differ in many ways.  The mixed effects modeling approach is fully parametric, while estimating via GEE is semiparametric.

More broadly, these represent two different approaches to repeated measures data: subject-specific and population-averaged.

:::{#def-subject-specific}
## Subject Specific Models
Also known as conditional modeling, the subject-specific approach models at the subject-level and addresses the correlation indirectly through the inclusion of random effects.
:::

:::{#def-population-averaged}
## Population Averaged Models
Also known as marginal modeling, the population-averaged approach posits a model for the mean response directly and addresses the correlation through directly modeling its structure.
:::

If you have more intuition about how the response should be characterized at the individual level, then the subject-specific (mixed-effects) approach will be more tractable.  If you have more intuition about how the response should be characterized on average across individuals, then the population-averaged (estimated with GEE's) approach will be more tractable. The GEE approach is fairly robust to misspecifications of the working correlation structure and constant variance conditions.  The mixed-effects model approach allows us to quantify the variability in an effect across subjects in the population.
