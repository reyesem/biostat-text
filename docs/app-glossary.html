<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Modeling for the Biological Sciences - Appendix A — Glossary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="mystyles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./app-glossary.html">Appendices</a></li><li class="breadcrumb-item"><a href="./app-glossary.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Biostatistics</a> 
        <div class="sidebar-tools-main">
    <a href="./ma482-text.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./01a-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit I: Review of Statistics and Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01b-statistical-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview of the Statistical Process</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01c-distributional-quartet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Distributional Quartet</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01d-essential-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Essential Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02a-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit II: General Linear Model</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02b-glm-model-framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">General Linear Model Framework</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02c-glm-assessing-conditions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Assessing the Conditions for the General Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02d-glm-unifying-framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The General Linear Model as a Unifying Framework</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03a-modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit III: General Modeling Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03b-modeling-related-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addressing Relationships Between Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03c-modeling-categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Incorporating Categorical Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03d-modeling-interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Allowing Effect Modification (Interaction Terms)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03e-modeling-linear-hypotheses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">General Linear Hypothesis Test</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03f-modeling-large-sample-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Sample Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03g-modeling-splines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Modeling Curvature (Splines)</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04a-rm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit IV: Models for Repeated Measures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04b-rm-terminology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">The Language of Repeated Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04c-rm-mixed-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Mixed Effects Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04d-rm-gee.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Generalized Estimating Equations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-glossary.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Appendix A — Glossary</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.</p>
<dl>
<dt>Alternate Characterization of the Classical Regression Model (<a href="02b-glm-model-framework.html#def-alternate-characterization">Definition&nbsp;<span>4.5</span></a>)</dt>
<dd>
Under the classical regression conditions on the error term (see <a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>), we can characterize the classical regression model as
</dd>
</dl>
<p><span class="math display">\[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i \stackrel{\text{Ind}}{\sim} N\left(\beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i, \sigma^2\right).\]</span></p>
<p>Here, the symbol <span class="math inline">\(\mid\)</span> is read “given” and means that the distribution of the response is specified after knowing the values of the predictors. That is, the distribution of the response depends on these variables.</p>
<dl>
<dt>Autoregressive Correlation Structure (<a href="04d-rm-gee.html#def-autoregressive-correlation-structure">Definition&nbsp;<span>15.4</span></a>)</dt>
<dd>
An autoregressive correlation structure suggests the correlation between two observations diminishes as the observations get further apart (generally, further apart in time). We generally only consider the autoregressive structure of degree 1 here; if there are five observations within a block, this has the form
</dd>
</dl>
<p><span class="math display">\[\Gamma = \begin{pmatrix}
1 &amp; \rho &amp; \rho^2 &amp; \rho^3 &amp; \rho^4 \\
\cdot &amp; 1 &amp; \rho &amp; \rho^2 &amp; \rho^3 \\
\cdot &amp; \cdot &amp; 1 &amp; \rho &amp; \rho^2 \\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; \rho \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 \end{pmatrix}.\]</span></p>
<dl>
<dt>Bernoulli Distribution (<a href="01d-essential-probability.html#def-bernoulli-distribution">Definition&nbsp;<span>3.6</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a discrete random variable taking the value 0 or 1. <span class="math inline">\(X\)</span> is said to have a Bernoulli distribution with density
</dd>
</dl>
<p><span class="math display">\[f(x) = \theta^x (1 - \theta)^{1 - x} \qquad x \in \{0, 1\},\]</span></p>
<p>where <span class="math inline">\(0 &lt; \theta &lt; 1\)</span> is the probability that <span class="math inline">\(X\)</span> takes the value 1.</p>
<ul>
<li><span class="math inline">\(E(X) = \theta\)</span></li>
<li><span class="math inline">\(Var(X) = \theta(1 - \theta)\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim Ber(\theta)\)</span>, which is read “X has a Bernoulli distribution with probability <span class="math inline">\(\theta\)</span>.”</p>
<dl>
<dt>Blocking (<a href="04b-rm-terminology.html#def-blocking">Definition&nbsp;<span>13.4</span></a>)</dt>
<dd>
Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.
</dd>
</dl>
<p>In both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.</p>
<dl>
<dt>Bootstrapping (<a href="03f-modeling-large-sample-theory.html#def-bootstrapping">Definition&nbsp;<span>11.8</span></a>)</dt>
<dd>
A process of constructing a sampling distribution of the parameter estimates through resampling. The observed data is resampled repeatedly, and the parameters of interest are estimated in each resample. The distribution of these estimates across the resamples is then used as an empirical model of the corresponding sampling distributions.
</dd>
<dt>Case Resampling Bootstrap (<a href="03f-modeling-large-sample-theory.html#def-case-resampling-bootstrap">Definition&nbsp;<span>11.10</span></a>)</dt>
<dd>
Suppose we observe a sample of size <span class="math inline">\(n\)</span> and use the data to compute the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> for the parameters in the model
</dd>
</dl>
<p><span class="math display">\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]</span></p>
<p>The case resampling bootstrap proceeds according to the following algorithm:</p>
<ol type="1">
<li>Take a random sample of size <span class="math inline">\(n\)</span> (with replacement) of the raw data (keeping all variables from the same observation together); denote the <span class="math inline">\(i\)</span>-th selected response and predictors <span class="math inline">\((\text{Response})_i^*\)</span> and <span class="math inline">\((\text{Predictor } j)_i^*\)</span>, respectively.</li>
<li>Obtain the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\alpha}}\)</span> by finding the values of <span class="math inline">\(\boldsymbol{\alpha}\)</span> that minimize</li>
</ol>
<p><span class="math display">\[\sum_{i=1}^{n} \left((\text{Response})_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i^*\right)^2.\]</span></p>
<ol start="3" type="1">
<li>Repeat steps 1-2 <span class="math inline">\(m\)</span> times.</li>
</ol>
<p>We often take <span class="math inline">\(m\)</span> to be large (at least 1000). After each pass through the algorithm, we retain the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\alpha}}\)</span> from the resample. The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.</p>
<dl>
<dt>Categorical Variable (<a href="01b-statistical-process.html#def-categorical-variable">Definition&nbsp;<span>1.5</span></a>)</dt>
<dd>
Also called a “qualitative variable,” a measurement on a subject which denotes a grouping or categorization.
</dd>
<dt>Central Limit Theorem (<a href="03f-modeling-large-sample-theory.html#def-clt">Definition&nbsp;<span>11.6</span></a>)</dt>
<dd>
Let <span class="math inline">\(Y_1, Y_2, \dotsc, Y_n\)</span> be independent and identically distributed random variables with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Then, as <span class="math inline">\(n\)</span> approaches infinity, the distribution of the ratio
</dd>
</dl>
<p><span class="math display">\[\frac{\sqrt{n}\left(\bar{Y} - \mu\right)}{\sigma}\]</span></p>
<p>approaches that of a Standard Normal random variable.</p>
<dl>
<dt>Chi-Square Distribution (<a href="01d-essential-probability.html#def-chi-square-distribution">Definition&nbsp;<span>3.8</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a Chi-Square distribution if the density is given by
</dd>
</dl>
<p><span class="math display">\[f(x) = \frac{1}{2^{\nu/2}\Gamma (\nu/2)}\;x^{\nu/2-1}e^{-x/2} \qquad x &gt; 0,\]</span></p>
<p>where <span class="math inline">\(\nu &gt; 0\)</span> is the degrees of freedom.</p>
<p>We write <span class="math inline">\(X \sim \chi^2_{\nu}\)</span>, which is read “X has a Chi-Square distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.”</p>
<dl>
<dt>Classical Regression (Conditions on Predictors) (<a href="02b-glm-model-framework.html#def-classical-regression-cont">Definition&nbsp;<span>4.4</span></a>)</dt>
<dd>
The classical regression model (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>) places the following conditions on the predictors:
</dd>
</dl>
<ol type="1">
<li>Each predictor is measured without error.</li>
<li>Each predictor has an additive linear effect on the response.</li>
</ol>
<dl>
<dt>Classical Regression Model (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>)</dt>
<dd>
In the “classical regression model,” we place the following four conditions on the distribution of the error <span class="math inline">\(\varepsilon_i\)</span>:
</dd>
</dl>
<ol type="1">
<li>The average error across all levels of the predictors is 0; mathematically, we write <span class="math inline">\(E\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = 0\)</span>.</li>
<li>The variance of the errors is constant across all levels of the predictors; mathematically, we write <span class="math inline">\(Var\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = \sigma^2\)</span> for some unknown constant <span class="math inline">\(\sigma^2 &gt; 0\)</span>. This is sometimes referred to as homoskedasticity.</li>
<li>The error terms are independent; in particular, the magnitude of the error for one observation does not influence the magnitude of the error for any other observation.</li>
<li>The distribution of the errors follows a Normal distribution with the above mean and variance.</li>
</ol>
<dl>
<dt>Cluster Samples (<a href="04b-rm-terminology.html#def-cluster-samples">Definition&nbsp;<span>13.14</span></a>)</dt>
<dd>
Stratified sampling divides a population into groups and samples from within each group; in contrast, cluster sampling divides the population into groups and randomly samples a few groups and takes measurements from within the group.
</dd>
<dt>Codebook (<a href="01b-statistical-process.html#def-codebook">Definition&nbsp;<span>1.16</span></a>)</dt>
<dd>
Also called a “data dictionary,” a codebook provides complete information regarding the variables contained within a dataset.
</dd>
<dt>Compound Symmetric Correlation Structure (<a href="04d-rm-gee.html#def-compound-symmetric-correlation-structure">Definition&nbsp;<span>15.3</span></a>)</dt>
<dd>
A compound symmetric correlation structure, also known as an <em>exchangeable</em> correlation structure, suggests the correlation between any two errors within a subject is equal. If there are five observations within a block, this has the form
</dd>
</dl>
<p><span class="math display">\[\Gamma = \begin{pmatrix}
1 &amp; \rho &amp; \rho &amp; \rho &amp; \rho \\
\cdot &amp; 1 &amp; \rho &amp; \rho &amp; \rho \\
\cdot &amp; \cdot &amp; 1 &amp; \rho &amp; \rho \\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; \rho \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 \end{pmatrix}.\]</span></p>
<dl>
<dt>Confidence Interval (<a href="01b-statistical-process.html#def-confidence-interval">Definition&nbsp;<span>1.11</span></a>)</dt>
<dd>
An interval (range of values) estimate of a parameter that incorporates the variability in the statistic. The process of constructing <span class="math inline">\(k\)</span>% confidence intervals results in them containing the parameter of interest in <span class="math inline">\(k\)</span>% of <em>repeated</em> studies. The value of <span class="math inline">\(k\)</span> is called the <em>confidence level</em>.
</dd>
<dt>Confidence Interval for Parameters Under Classical Model (<a href="02b-glm-model-framework.html#def-classical-ci">Definition&nbsp;<span>4.10</span></a>)</dt>
<dd>
Under the classical regression conditions (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>), a <span class="math inline">\(100c\)</span>% confidence interval for the parameter <span class="math inline">\(\beta_j\)</span> is given by
</dd>
</dl>
<p><span class="math display">\[\widehat{\beta}_j \pm t_{n-p-1, 0.5(1+c)} \sqrt{Var\left(\widehat{\beta}_j\right)}.\]</span></p>
<p>where <span class="math inline">\(t_{n-p-1, 0.5(1+c)}\)</span> is the <span class="math inline">\(0.5(1+c)\)</span> quantile from the <span class="math inline">\(t_{n-p-1}\)</span> distribution, known as the critical value for the confidence interval.</p>
<dl>
<dt>Confounding (<a href="01b-statistical-process.html#def-confounding">Definition&nbsp;<span>1.15</span></a>)</dt>
<dd>
When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.
</dd>
<dt>Correlation Structure (<a href="04b-rm-terminology.html#def-correlation-structure">Definition&nbsp;<span>13.6</span></a>)</dt>
<dd>
The correlation structure quantifies the strength and direction of the relationship between the errors in the observed responses.
</dd>
<dt>Cross Sectional Study (<a href="04b-rm-terminology.html#def-cross-sectional-study">Definition&nbsp;<span>13.12</span></a>)</dt>
<dd>
A cross sectional study considers data from a single snapshot in time.
</dd>
<dt>Cross-Over Study (<a href="04b-rm-terminology.html#def-cross-over-study">Definition&nbsp;<span>13.9</span></a>)</dt>
<dd>
A cross-over study exposes each participant to multiple treatments. Whenever possible, the order of the treatments is randomly determined. This is equivalent to a randomized complete block design in which the blocks are the participants. When the treatments are believed to have a lingering effect, a wash-out period between treatments is used to minimize the impact of previous treatments on the treatment the participant is currently being exposed to.
</dd>
<dt>Cumulative Distribution Function (CDF) (<a href="01d-essential-probability.html#def-cdf">Definition&nbsp;<span>3.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable; the cumulative distribution function (CDF) is defined as
</dd>
</dl>
<p><span class="math display">\[F(u) = Pr(X \leq u).\]</span></p>
<p>For a continuous random variable, we have that</p>
<p><span class="math display">\[F(u) = \int_{-\infty}^{u} f(x) dx\]</span></p>
<p>implying that the density function is the derivative of the CDF. For a discrete random variable</p>
<p><span class="math display">\[F(u) = \sum_{x \leq u} f(x).\]</span></p>
<dl>
<dt>Density Function (<a href="01d-essential-probability.html#def-density-function">Definition&nbsp;<span>3.2</span></a>)</dt>
<dd>
A density function <span class="math inline">\(f\)</span> relates the potential values of a random variable <span class="math inline">\(X\)</span> with the probability those values occur. For a <em>continuous</em> random variable, the probability the random variable <span class="math inline">\(X\)</span> falls within an interval <span class="math inline">\((a, b)\)</span> is given by
</dd>
</dl>
<p><span class="math display">\[Pr(a \leq X \leq b) = \int_{a}^{b} f(x) dx.\]</span></p>
<p>For a <em>discrete</em> random variable, the probability the random variable <span class="math inline">\(X\)</span> is equal to the value <span class="math inline">\(u\)</span> is given by</p>
<p><span class="math display">\[Pr(X = u) = f(u).\]</span></p>
<dl>
<dt>Distribution (<a href="01b-statistical-process.html#def-distribution">Definition&nbsp;<span>1.7</span></a>)</dt>
<dd>
The pattern of variability corresponding to a set of values.
</dd>
<dt>Estimate of the Variance of the Errors (<a href="02b-glm-model-framework.html#def-estimate-sigma2">Definition&nbsp;<span>4.9</span></a>)</dt>
<dd>
The unknown variance in the linear model, which captures the variability in the response for any set of predictors (also called the residual variance), is estimated by
</dd>
</dl>
<p><span class="math display">\[\widehat{\sigma}^2 = \frac{1}{n-p-1} \sum\limits_{i=1}^{n} \left((\text{Response})_i - \widehat{\beta}_0 - \sum\limits_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_{i}\right)^2.\]</span></p>
<dl>
<dt>F-Distribution (<a href="01d-essential-probability.html#def-f-distribution">Definition&nbsp;<span>3.9</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have an F-distribution if the density is given by
</dd>
</dl>
<p><span class="math display">\[f(x) = \frac{\Gamma((r + s)/2)}{(\Gamma(r/2) \Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \qquad x &gt; 0,\]</span></p>
<p>where <span class="math inline">\(r,s &gt; 0\)</span> are the numerator and denominator degrees of freedom, respectively.</p>
<p>We write <span class="math inline">\(X \sim F_{r, s}\)</span>, which is read “X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.”</p>
<dl>
<dt>Fixed Effect (<a href="04b-rm-terminology.html#def-fixed-effect">Definition&nbsp;<span>13.7</span></a>)</dt>
<dd>
Fixed effects are terms in the model for which we are interested in both the specific grouping levels, and we are interested in characterizing the relationship between these levels and the response.
</dd>
<dt>General Linear Hypothesis (<a href="03e-modeling-linear-hypotheses.html#def-general-linear-hypothesis">Definition&nbsp;<span>10.1</span></a>)</dt>
<dd>
The general linear hypothesis framework refers to testing hypotheses of the form
</dd>
</dl>
<p><span class="math display">\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m} \qquad \text{vs.} \qquad H_1: \mathbf{K}\boldsymbol{\beta} \neq \mathbf{m}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span class="math inline">\((p+1)\)</span>-length vector of the parameters (includes the intercept),</li>
<li><span class="math inline">\(\mathbf{K}\)</span> is an <span class="math inline">\(r\)</span>-by-<span class="math inline">\((p+1)\)</span> matrix that specifies the linear combinations defining the hypothesis of interest, and</li>
<li><span class="math inline">\(\mathbf{m}\)</span> is a vector of length <span class="math inline">\(r\)</span> specifying the null values, the value of each linear combination under the null hypothesis (often a vector of 0’s).</li>
</ul>
<dl>
<dt>General Linear Model (<a href="02b-glm-model-framework.html#def-general-linear-model">Definition&nbsp;<span>4.1</span></a>)</dt>
<dd>
The general linear model views the response (outcome) as a linear combination of several predictors:
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  (\text{Response})_i
    &amp;= \beta_0 + \beta_1 (\text{Predictor 1})_{i} + \beta_2 (\text{Predictor 2})_{i} + \dotsb +
      \beta_p (\text{Predictor } p)_{i} + \varepsilon_i \\
    &amp;= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i} + \varepsilon_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of subjects in the sample, <span class="math inline">\(p &lt; n\)</span> is the number of predictors in the model, and <span class="math inline">\(\varepsilon_i\)</span> is a random variable that captures the error in the response.</p>
<dl>
<dt>Generalized Estimating Equations (GEE) (<a href="04d-rm-gee.html#def-gee">Definition&nbsp;<span>15.6</span></a>)</dt>
<dd>
Generalized estimating equations can be used to estimate the parameters of a model while accounting for the correlation among observations. In addition to specifying a model for the overall average response, a “working” structure is specified for the correlation of observations from the same subject. The working structure is updated during the estimation process and used to adjust the standard errors of the parameter estimates in the mean model.
</dd>
<dt>Hierarchical Model (<a href="04c-rm-mixed-models.html#def-hierarchical-model">Definition&nbsp;<span>14.1</span></a>)</dt>
<dd>
A hierarchical model breaks the data generating process into smaller stages and posits a model for each stage. The stages are determined by defining a hierarchy of units and thereby capturing the sources of variability.
</dd>
<dt>Independence Correlation Structure (<a href="04d-rm-gee.html#def-independence-correlation-structure">Definition&nbsp;<span>15.2</span></a>)</dt>
<dd>
An independence correlation structure suggests there is no correlation among any of the error terms within a subject. If there are five observations within a block, this has the form
</dd>
</dl>
<p><span class="math display">\[\Gamma = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\cdot &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
\cdot &amp; \cdot &amp; 1 &amp; 0 &amp; 0 \\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 0 \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 \end{pmatrix}.\]</span></p>
<dl>
<dt>Indicator Variables (<a href="03c-modeling-categorical-predictors.html#def-indicator-variables">Definition&nbsp;<span>8.1</span></a>)</dt>
<dd>
Also called “dummy variables,” these are a set of binary variables that capture the grouping defined by a categorical variable for regression modeling.
</dd>
<dt>Individual-Level Model (<a href="04c-rm-mixed-models.html#def-individual-model">Definition&nbsp;<span>14.2</span></a>)</dt>
<dd>
The individual-level model characterizes the response for the <span class="math inline">\(i\)</span>-th subject (or block) only.
</dd>
<dt>Interaction (<a href="03d-modeling-interactions.html#def-interaction">Definition&nbsp;<span>9.2</span></a>)</dt>
<dd>
An interaction term allows the effect of a predictor on the response to depend on the value of a second predictor (capturing an effect modification).
</dd>
</dl>
<ul>
<li>The interaction term is created by adding the product of the two predictors under consideration to the model.</li>
</ul>
<dl>
<dt>Intercept (<a href="02b-glm-model-framework.html#def-intercept">Definition&nbsp;<span>4.6</span></a>)</dt>
<dd>
The population intercept, denoted <span class="math inline">\(\beta_0\)</span>, is the <em>mean</em> response when all predictors take the value zero.
</dd>
<dt>Large Sample Model for the Sampling Distribution of the Least Squares Estimates (<a href="03f-modeling-large-sample-theory.html#def-ls-sampling-distribution-large-samples">Definition&nbsp;<span>11.7</span></a>)</dt>
<dd>
Suppose the classical regression conditions hold, with the exception of the errors following a Normal distribution. As the sample size gets large, we have that the distribution of the ratio
</dd>
</dl>
<p><span class="math display">\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim N(0, 1)\]</span></p>
<p>for all <span class="math inline">\(j = 0, 1, \dotsc, p\)</span>. Further, under the null hypothesis</p>
<p><span class="math display">\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m}\]</span></p>
<p>we have</p>
<p><span class="math display">\[\left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim \chi^2_r.\]</span></p>
<dl>
<dt>Large Sample Theory (<a href="03f-modeling-large-sample-theory.html#def-large-sample-theory">Definition&nbsp;<span>11.5</span></a>)</dt>
<dd>
The phrase “large sample theory” (or “asymptotics”) is used to describe a scenario when the model for the sampling distribution (or null distribution) of an estimate (or standardized statistic) can be approximated as the sample size becomes infinitely large. That is, as the sample size approaches infinity, the sampling distribution (or null distribution) can be easily modeled using a known probability distribution.
</dd>
<dt>Least Squares Estimation (<a href="02b-glm-model-framework.html#def-least-squares">Definition&nbsp;<span>4.2</span></a>)</dt>
<dd>
The method of least squares may be used to estimate the coefficients (parameters) of a linear model. In particular, we choose the values of the coefficients that minimize
</dd>
</dl>
<p><span class="math display">\[\sum\limits_{i=1}^{n} \left((\text{Response})_i - \beta_0 - \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i}\right)^2.\]</span></p>
<p>The resulting “least squares” estimates are denoted <span class="math inline">\(\widehat{\beta}_0, \widehat{\beta}_1, \dotsc, \widehat{\beta}_p\)</span>.</p>
<dl>
<dt>Linear Model (<a href="03g-modeling-splines.html#def-linear-model">Definition&nbsp;<span>12.1</span></a>)</dt>
<dd>
A model is said to be linear if it can be expressed as a linear combination of the <em>parameters</em>. That is, the linearity does not refer to the form of the predictors but the form of the parameters.
</dd>
<dt>Linear Spline (<a href="03g-modeling-splines.html#def-linear-spline">Definition&nbsp;<span>12.3</span></a>)</dt>
<dd>
A linear spline is a continuous piecewise linear function.
</dd>
<dt>Longitudinal Study (<a href="04b-rm-terminology.html#def-longitudinal-study">Definition&nbsp;<span>13.11</span></a>)</dt>
<dd>
A longitudinal study repeatedly measures the response on each subject at various points in time.
</dd>
<dt>Mean and Variance of a Random Variable (<a href="01d-essential-probability.html#def-rv-mean-variance">Definition&nbsp;<span>3.4</span></a>)</dt>
<dd>
Suppose <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(f\)</span>. If <span class="math inline">\(X\)</span> is a continuous random variable, then the mean and variance are given by
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  E(X) &amp;= \int x f(x) dx \\
  Var(X) &amp;= \int \left(x - E(X)\right)^2 f(x) dx.
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable, then the mean and variance are given by</p>
<p><span class="math display">\[
\begin{aligned}
  E(X) &amp;= \sum x f(x) \\
  Var(X) &amp;= \sum \left(x - E(X)\right)^2 f(x).
\end{aligned}
\]</span></p>
<dl>
<dt>Mixed-Effects Model (<a href="04c-rm-mixed-models.html#def-mixed-effects-model">Definition&nbsp;<span>14.4</span></a>)</dt>
<dd>
A mixed-effects model denotes a hierarchical model for which some effects are fixed (not allowed to vary across subjects) and others are random (allowed to vary across subjects).
</dd>
<dt>Model for the Null Distribution with the General Linear Hypothesis (<a href="03e-modeling-linear-hypotheses.html#def-general-linear-hypothesis-null">Definition&nbsp;<span>10.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> be the <span class="math inline">\((p+1)\)</span> vector of estimates for the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, and let the estimates have variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Assuming the null hypothesis
</dd>
</dl>
<p><span class="math display">\[H_0: \mathbf{K} \boldsymbol{\beta} = \mathbf{m}\]</span></p>
<p>is true, under the conditions of the classical regression model (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>)</p>
<p><span class="math display">\[(1/r) \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim F_{r, n-p-1}.\]</span></p>
<dl>
<dt>Multicollinearity (<a href="03b-modeling-related-predictors.html#def-multicollinearity">Definition&nbsp;<span>7.1</span></a>)</dt>
<dd>
When two predictors are highly correlated with one another, we say that there is multicollinearity in the model.
</dd>
<dt>Nonparametric Model (<a href="03f-modeling-large-sample-theory.html#def-nonparametric-model">Definition&nbsp;<span>11.2</span></a>)</dt>
<dd>
A nonparametric model is unable to characterize the response using a finite set of parameters; for our purposes, this generally means the model makes no assumptions about the structure of the underlying distribution of the response given the predictors. Only minimal assumptions (such as independence between observations) are imposed.
</dd>
<dt>Normal (Gaussian) Distribution (<a href="01d-essential-probability.html#def-normal-distribution">Definition&nbsp;<span>3.5</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a Normal (or Gaussian) distribution if the density is given by
</dd>
</dl>
<p><span class="math display">\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} \qquad -\infty &lt; x &lt; \infty,\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is any real number and <span class="math inline">\(\sigma^2 &gt; 0\)</span>.</p>
<ul>
<li><span class="math inline">\(E(X) = \mu\)</span></li>
<li><span class="math inline">\(Var(X) = \sigma^2\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim N\left(\mu, \sigma^2\right)\)</span>, which is read “X has a Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.” This short-hand implies the density above.</p>
<dl>
<dt>Numeric Variable (<a href="01b-statistical-process.html#def-numeric-variable">Definition&nbsp;<span>1.6</span></a>)</dt>
<dd>
Also called a “quantitative variable,” a measurement on a subject which takes on a numeric value <em>and</em> for which ordinary arithmetic makes sense.
</dd>
<dt>Observational Study (<a href="01b-statistical-process.html#def-observational-study">Definition&nbsp;<span>1.14</span></a>)</dt>
<dd>
A study in which each participant “self-selects” into one of groups being compared in the study. The phrase “self-selects” is used very loosely here and can include studies in which the groups are defined by an inherent characteristic, the groups are determined according to a non-random mechanism, and each participant chooses the group to which they belong.
</dd>
<dt>P-Value (<a href="01b-statistical-process.html#def-pvalue">Definition&nbsp;<span>1.12</span></a>)</dt>
<dd>
The probability, assuming the null hypothesis is true, that we would observe a statistic, from sampling variability alone, as extreme or more so as that observed in our sample. This quantifies the strength of evidence against the null hypothesis. Smaller values indicate stronger evidence.
</dd>
<dt>P-Value for Testing if Parameter Belongs in Model Under Classical Model (<a href="02b-glm-model-framework.html#def-classical-p">Definition&nbsp;<span>4.11</span></a>)</dt>
<dd>
Under the classical regression conditions (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>), the p-value for testing the hypotheses
</dd>
</dl>
<p><span class="math display">\[H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0\]</span></p>
<p>is given by</p>
<p><span class="math display">\[Pr\left(\lvert T\rvert &gt; \lvert\frac{\widehat{\beta}_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}}\rvert\right)\]</span></p>
<p>where <span class="math inline">\(T \sim t_{n-p-1}\)</span>.</p>
<dl>
<dt>Parameter (<a href="01b-statistical-process.html#def-parameter">Definition&nbsp;<span>1.9</span></a>)</dt>
<dd>
Numeric quantity which summarizes the distribution of a variable within the <em>population</em> of interest. Generally denoted by Greek letters in statistical formulas.
</dd>
<dt>Parametric Model (<a href="03f-modeling-large-sample-theory.html#def-parametric-model">Definition&nbsp;<span>11.1</span></a>)</dt>
<dd>
A parametric model characterizes the distribution of the response using a finite set of parameters; for our purposes, this generally means the model <em>fully</em> characterize the distribution of the response given the predictors.
</dd>
<dt>Population (<a href="01b-statistical-process.html#def-population">Definition&nbsp;<span>1.1</span></a>)</dt>
<dd>
The collection of subjects we would like to say something about.
</dd>
<dt>Population Averaged Models (<a href="04d-rm-gee.html#def-population-averaged">Definition&nbsp;<span>15.9</span></a>)</dt>
<dd>
Also known as marginal modeling, the population-averaged approach posits a model for the mean response directly and addresses the correlation through directly modeling its structure.
</dd>
<dt>Population-Level Model (<a href="04c-rm-mixed-models.html#def-population-model">Definition&nbsp;<span>14.3</span></a>)</dt>
<dd>
The population-level model characterizes how the <em>parameters</em> of the individual-level model vary across subjects (or blocks) in the population.
</dd>
<dt>Random Effect (<a href="04b-rm-terminology.html#def-random-effect">Definition&nbsp;<span>13.8</span></a>)</dt>
<dd>
Random effects are terms in the model that capture the correlation induced due to an inherent characteristic that varies across the population. We are <em>not</em> interested in the specific grouping levels, and we either are not interested in the relationship with the response.
</dd>
<dt>Random Variable (<a href="01d-essential-probability.html#def-random-variable">Definition&nbsp;<span>3.1</span></a>)</dt>
<dd>
A random variable represents a measurement that will be collected and for which the value cannot be predicted with certainty; they are generally represented with a capital letter. Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements.
</dd>
<dt>Randomization (<a href="04b-rm-terminology.html#def-randomization">Definition&nbsp;<span>13.2</span></a>)</dt>
<dd>
Randomization can refer to random <em>selection</em> or random <em>allocation</em>.
</dd>
</dl>
<p>Random selection refers to the use of a random mechanism to select units from the population. Random selection minimizes bias.</p>
<p>Random allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment. Random allocation eliminates confounding and permits causal interpretations.</p>
<dl>
<dt>Randomized Clinical Trial (<a href="01b-statistical-process.html#def-randomized-clinical-trial">Definition&nbsp;<span>1.13</span></a>)</dt>
<dd>
Also called a “controlled experiment,” a study in which each participant is randomly assigned to one of the groups being compared in the study.
</dd>
<dt>Randomized Complete Block Design (<a href="04b-rm-terminology.html#def-rcbd">Definition&nbsp;<span>13.10</span></a>)</dt>
<dd>
A randomized complete block design is an example of a controlled experiment utilizing blocking. Each treatment is randomized to observations within blocks in such a way that every treatment is present within the block and the same number of observations are assigned to each treatment within each block.
</dd>
<dt>Reduction of Noise (<a href="04b-rm-terminology.html#def-noise-reduction">Definition&nbsp;<span>13.3</span></a>)</dt>
<dd>
Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or through blocking. These actions reduce the number of differences between the units under study.
</dd>
<dt>Reference Group (<a href="03c-modeling-categorical-predictors.html#def-reference-group">Definition&nbsp;<span>8.2</span></a>)</dt>
<dd>
The group defined by having all indicator variables for a particular categorical variable set to zero.
</dd>
<dt>Repeated Measures (<a href="04b-rm-terminology.html#def-repeated-measures">Definition&nbsp;<span>13.5</span></a>)</dt>
<dd>
The phrase “repeated measures” refers to data for which the observed responses can be grouped based on some nuisance variable (typically the participant), and this grouping captures some inherent characteristic such that observations within a group tend to be more alike than observations across groups.
</dd>
<dt>Replication (<a href="04b-rm-terminology.html#def-replication">Definition&nbsp;<span>13.1</span></a>)</dt>
<dd>
Replication results from taking measurements on different units (or subjects) for which you expect the results to be similar. That is, any variability across the units is due to natural variability within the population.
</dd>
<dt>Residual (<a href="02c-glm-assessing-conditions.html#def-residual">Definition&nbsp;<span>5.1</span></a>)</dt>
<dd>
A residual for the <span class="math inline">\(i\)</span>-th observation is the difference between an observed value and the predicted response:
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  (\text{Residual})_i
    &amp;= (\text{Observed Response})_i - (\text{Predicted Response})_i \\
    &amp;= (\text{Response})_i - \left(\widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i\right).
\end{aligned}
\]</span></p>
<dl>
<dt>Residual Bootstrap (<a href="03f-modeling-large-sample-theory.html#def-residual-bootstrap">Definition&nbsp;<span>11.9</span></a>)</dt>
<dd>
Suppose we observe a sample of size <span class="math inline">\(n\)</span> and use the data to compute the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> for the parameters in the model
</dd>
</dl>
<p><span class="math display">\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]</span></p>
<p>The residual bootstrap proceeds according to the following algorithm:</p>
<ol type="1">
<li>Compute the residuals</li>
</ol>
<p><span class="math display">\[(\text{Residuals})_i = (\text{Response})_i - (\text{Predicted Response})_i\]</span></p>
<ol start="2" type="1">
<li>Take a random sample of size <span class="math inline">\(n\)</span> (with replacement) of the residuals; call these values <span class="math inline">\(e_1^*, \dotsc, e_n^*\)</span>.</li>
<li>Form “new” responses <span class="math inline">\(y_1^*, \dotsc, y_n^*\)</span> according to</li>
</ol>
<p><span class="math display">\[y_i^* = \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i + e_i^*.\]</span></p>
<ol start="4" type="1">
<li>Obtain the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\alpha}}\)</span> by finding the values of <span class="math inline">\(\boldsymbol{\alpha}\)</span> that minimize</li>
</ol>
<p><span class="math display">\[\sum_{i=1}^{n} \left(y_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i\right)^2.\]</span></p>
<ol start="5" type="1">
<li>Repeat steps 2-4 <span class="math inline">\(m\)</span> times.</li>
</ol>
<p>We often take <span class="math inline">\(m\)</span> to be large (at least 1000). After each pass through the algorithm, we retain the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\alpha}}\)</span> from the resample. The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.</p>
<dl>
<dt>Response Variable (<a href="01b-statistical-process.html#def-response">Definition&nbsp;<span>1.8</span></a>)</dt>
<dd>
Also called the “outcome,” this is the primary variable of interest in the research question; it is the variable we either want to explain or predict.
</dd>
<dt>Restricted Cubic Spline (<a href="03g-modeling-splines.html#def-restricted-cubic-spline">Definition&nbsp;<span>12.4</span></a>)</dt>
<dd>
A restricted cubic spline is a continuous function comprised of piecewise cubic polynomials for which the tails of the spline have been restricted to be linear.
</dd>
<dt>Robust Sandwich Estimator (<a href="04d-rm-gee.html#def-robust-sandwich-estimator">Definition&nbsp;<span>15.7</span></a>)</dt>
<dd>
The robust sandwich estimator of the variance-covariance matrix of the parameter estimates from the mean model balances the relationship between the parameter estimates specified by the model (and the “working” correlation matrix) with the relationship suggested by the observed data. Specifically, it has the form
</dd>
</dl>
<p><span class="math display">\[\widehat{\boldsymbol{\Sigma}} = \widehat{\mathbf{U}} \widehat{\mathbf{U}}^{-1/2} \mathbf{R} \widehat{\mathbf{U}}^{-1/2} \widehat{\mathbf{U}}\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}\)</span> represents the model-based variance-covariance matrix if the structure specified by the working correlation matrix were completely correct, and <span class="math inline">\(\mathbf{R}\)</span> represents the correction factor estimated from the residuals (an empirical estimate).</p>
<dl>
<dt>Sample (<a href="01b-statistical-process.html#def-sample">Definition&nbsp;<span>1.2</span></a>)</dt>
<dd>
The collection of subjects for which we actually obtain measurements (data).
</dd>
<dt>Sampling Distribution of the Least Squares Estimates (<a href="02b-glm-model-framework.html#def-ls-sampling-distribution">Definition&nbsp;<span>4.8</span></a>)</dt>
<dd>
Under the classical regression conditions (<a href="02b-glm-model-framework.html#def-classical-regression">Definition&nbsp;<span>4.3</span></a>), we have that
</dd>
</dl>
<p><span class="math display">\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim t_{n - p - 1}.\]</span></p>
<p>The denominator <span class="math inline">\(\sqrt{Var\left(\widehat{\beta}_j\right)}\)</span> is known as the <em>standard error</em> of the estimate <span class="math inline">\(\widehat{\beta}_j\)</span>. This formula holds for all <span class="math inline">\(j = 0, 1, \dotsc, p\)</span>.</p>
<dl>
<dt>Semiparametric Linear Model (<a href="03f-modeling-large-sample-theory.html#def-semiparametric-linear-model">Definition&nbsp;<span>11.4</span></a>)</dt>
<dd>
Suppose we no longer require that the error terms follow a Normal distribution; however, we do continue to impose the remaining conditions of the classical regression model. Then, our model could be written as
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  E\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &amp;= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i \\
  Var\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &amp;= \sigma^2
\end{aligned}
\]</span></p>
<p>where the responses are independent of one another given the predictors.</p>
<dl>
<dt>Semiparametric Model (<a href="03f-modeling-large-sample-theory.html#def-semiparametric-model">Definition&nbsp;<span>11.3</span></a>)</dt>
<dd>
A semiparametric model specifies some components of the underlying distribution of the response using a finite set of parameters but does not fully characterize the distribution. This generally means that we may specify the mean and/or variance of the response given the predictors, but we do not characterize the distributional family of the response.
</dd>
<dt>Slope (<a href="02b-glm-model-framework.html#def-slope">Definition&nbsp;<span>4.7</span></a>)</dt>
<dd>
The coefficient for the <span class="math inline">\(j\)</span>-th predictor, denoted <span class="math inline">\(\beta_j\)</span>, is the change in the mean response associated with a one unit increase in Predictor <span class="math inline">\(j\)</span>, <em>holding all other predictors fixed</em>.
</dd>
<dt>Spaghetti Plot (<a href="04b-rm-terminology.html#def-spaghetti-plot">Definition&nbsp;<span>13.15</span></a>)</dt>
<dd>
A spaghetti plot is a scatterplot that displays the trends within a subject, highlighting the correlation structure by connecting points from the same subject.
</dd>
<dt>Spline (<a href="03g-modeling-splines.html#def-spline">Definition&nbsp;<span>12.2</span></a>)</dt>
<dd>
A spline is a continuous piecewise polynomial used to model curvature. The points that define the piecewise components are called <em>knot points</em>; the functional form is allowed to change at the knot points.
</dd>
<dt>Stationarity (<a href="04d-rm-gee.html#def-stationarity">Definition&nbsp;<span>15.5</span></a>)</dt>
<dd>
The assumption of stationarity states that the correlation structure does not depend on time, only the distance between the observations.
</dd>
<dt>Statistic (<a href="01b-statistical-process.html#def-statistic">Definition&nbsp;<span>1.10</span></a>)</dt>
<dd>
Numeric quantity which summarizes the distribution of a variable within the observed <em>sample</em>.
</dd>
<dt>Statistical Inference (<a href="01b-statistical-process.html#def-inference">Definition&nbsp;<span>1.3</span></a>)</dt>
<dd>
The process of using a sample to characterize some aspect of the underlying population.
</dd>
<dt>Subgroup Analysis (<a href="03d-modeling-interactions.html#def-subgroup-analysis">Definition&nbsp;<span>9.1</span></a>)</dt>
<dd>
Refers to repeating a specified analysis (e.g., regression model) within various levels of a categorical predictor.
</dd>
</dl>
<ul>
<li>This will appropriately estimate the effect modification.</li>
<li>This results in a loss of information because <em>all parameters</em> are forced to vary across the subgroups.</li>
</ul>
<dl>
<dt>Subject Specific Models (<a href="04d-rm-gee.html#def-subject-specific">Definition&nbsp;<span>15.8</span></a>)</dt>
<dd>
Also known as conditional modeling, the subject-specific approach models at the subject-level and addresses the correlation indirectly through the inclusion of random effects.
</dd>
<dt>Subsampling (<a href="04b-rm-terminology.html#def-subsampling">Definition&nbsp;<span>13.13</span></a>)</dt>
<dd>
Subsampling occurs when several measurements are taken on each subject under the same treatment, possibly at unique locations.
</dd>
<dt>Unstructured Correlation Structure (<a href="04d-rm-gee.html#def-unstructured-correlation-structure">Definition&nbsp;<span>15.1</span></a>)</dt>
<dd>
An unstructured correlation structure suggests that the correlation between any two errors within a subject can take on any value. We only require that it be a valid correlation matrix.
</dd>
<dt>Variable (<a href="01b-statistical-process.html#def-variable">Definition&nbsp;<span>1.4</span></a>)</dt>
<dd>
A measurement, or category, describing some aspect of the subject.
</dd>
<dt>Variance-Covariance Matrix (<a href="03e-modeling-linear-hypotheses.html#def-variance-covariance-matrix">Definition&nbsp;<span>10.2</span></a>)</dt>
<dd>
Let <span class="math inline">\(\boldsymbol{\beta}\)</span> represent the <span class="math inline">\((p+1)\)</span>-length vector of the parameters and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> represent the <span class="math inline">\((p+1)\)</span> vector of the parameter <em>estimates</em>. The variance-covariance matrix of the parameter estimates is the <span class="math inline">\((p+1)\)</span>-by-<span class="math inline">\((p+1)\)</span> matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> where
</dd>
</dl>
<ul>
<li>the <span class="math inline">\(j\)</span>-th diagonal element contains <span class="math inline">\(Var\left(\widehat{\beta}_j\right)\)</span>, and</li>
<li>the <span class="math inline">\((i,j)\)</span>-th element contains the covariance between <span class="math inline">\(\widehat{\beta}_i\)</span> and <span class="math inline">\(\widehat{\beta}_j\)</span>.</li>
</ul>
<dl>
<dt>t-Distribution (<a href="01d-essential-probability.html#def-t-distribution">Definition&nbsp;<span>3.7</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a t-distribution if the density is given by
</dd>
</dl>
<p><span class="math display">\[f(x) = \frac{\Gamma \left(\frac{\nu+1}{2} \right)} {\sqrt{\nu\pi}\,\Gamma \left(\frac{\nu}{2} \right)} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}} \qquad x &gt; 0\]</span></p>
<p>where <span class="math inline">\(\nu &gt; 0\)</span> is the degrees of freedom.</p>
<p>We write <span class="math inline">\(X \sim t_{\nu}\)</span>, which is read “X has a t-distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.”</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./references.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">References</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>