# Addressing Relationships Between Predictors {#sec-modeling-related-predictors}

{{< include _setupcode.qmd >}}

@sec-glm-model-framework introduced the framework of the general linear model, including the interpretation of the parameters (@def-slope and @def-intercept).  While the mathematical structure of the general linear model may be fascinating to some, we are particularly interested in the model's utility to address scientific questions.  Therefore, for our purposes, the interpretation of the parameters is of utmost importance.  And, immediately from the interpretation of the slope coefficients, the linear model framework allows to isolate the effect of one variable while holding all other variables constant.  This has implications on the conclusions we can draw from the model.


## Adjusting for Confounders
Scientific studies can be roughly categorized as being an observational study (@def-observational-study) or a controlled experiment (also called a randomized clinical trial, @def-randomized-clinical-trial).  When our scientific question centers on the relationship between a response and a predictor, and the values of the predictor have not been randomly assigned to subjects, the (observational) study is subject to confounding (@def-confounding).  It is the potential for confounding that leads to the often cited "correlation does not imply causation."  It does not take much to see that this is extremely limiting.  We can imagine randomizing subjects to different levels of a categorical predictor, but randomizing subjects to a quantitative predictor would require very large sample sizes.  For example, imagine randomly allocating subjects to the amount of water they consume in their diet each day --- think of all the amounts of water you might want to consider.  Further, it would rule out ever being able to causally link a response with a quantitative predictor which represents an inherit characteristic.  For example, randomly allocating a participant to a specific height would require adding or removing bone mass to alter their height --- which we hope goes without saying is unethical and unacceptable (not to mention just disturbing).

Multivariable models, however, naturally address confounding because of the interpretation of the coefficients: _holding all other predictors fixed_ (we said this would be a crucial phrase).  This interpretation suggests the coefficient attached to a predictor is quantifying the effect of the predictor on the response, isolated from all other predictors in the model.  By isolating the predictor's effect, we are able to see its impact on the response beyond the impact of any other predictors.  We often say that the model is "adjusted" for the other predictors.

:::{.callout-note}
## Adjusted Models
An "adjusted" model just means that we constructed a multivariable model. The relationship between the response and the predictor of interest is "adjusted" by the other predictors that appear in the model.
:::

Think again about the importance of random allocation in allowing for causal interpretations in controlled experiments.  The random allocation of subjects to groups means that the groups are similar _with respect to all other variables_ --- the only difference between the groups is the treatment received.  The "holding all other predictors fixed" phrase accomplishes something similar.  
We know that the deterministic portion of the general linear model characterizes the mean response.  That is, the deterministic portion tells us the average value of the response we should expect among the _group_ of individuals with a particular value of the predictors.  Therefore, when we increase a predictor by one unit _holding all other predictors fixed_, we are creating two groups where the only difference is that increase of one unit.  The two groups are similar _with respect to all other predictors in the model_.

This seems almost too good to be true.  By simply expanding our model to incorporate other predictors, we have addressed the issue of confounding, and we have gotten back a causal interpretation of the impact of the predictor on the response.  But, it is important to note the differences between a controlled experiment and the interpretation provided by a multivariable model:

  - Controlled experiment: the groups are similar _with respect to all other variables_.
  - Multivariable model: the groups are similar _with respect to all other predictors_.

The difference in the language is subtle but important.  We can only adjust for predictors that we observe and put in the model.  That is, a multivariable model does not automatically solve all confounding issues; it ensures that any potential confounding cannot be the result of the other predictors in the model.  This allows us to make causal conclusions if we can assume that all potential confounders are present as other predictors in the model.

:::{.callout-tip}
## Big Idea
A multivariable model allows us to isolate the effect of one variable from the other predictors; this allows us to state that those other predictors are not contributing to any potential confounding between the variable of interest and the response.
:::


## Multicollinearity
A confounder masks or exaggerates the effect of one predictor on the response.  In order for confounding to exist, the confounder must be related to both the response and the predictor.  A distinctly unique, but often confused topic, is that of multicollinearity.

:::{#def-multicollinearity}
## Multicollinearity
When two predictors are highly correlated with one another, we say that there is multicollinearity in the model.
:::

To understand the impact, we consider a very simple hypothetical example.  Suppose we are interested in modeling the number of steps taken over the course of a typical day (as recorded by popular fitness trackers) using the participant's height and stride length.  Our model would have the form

$$(\text{Number of Steps})_i = \beta_0 + \beta_1(\text{Height})_i + \beta_2(\text{Stride Length})_i + \varepsilon_i.$$

Suppose that we test the following two sets of hypotheses:

$$
\begin{aligned}
  H_0:& \beta_1 = 0 \qquad \text{vs.} \qquad H_1: \beta_1 \neq 0 \\
  H_0:& \beta_2 = 0 \qquad \text{vs.} \qquad H_1: \beta_2 \neq 0.
\end{aligned}
$$

Further, suppose that we have a large p-value for each of these tests.  

How would we interpret the large p-value for the first test?  It would tell us that there is no evidence of a relationship between the average number of steps taken and the participant's height, _holding their stride length fixed_.  The idea of holding all other predictors fixed plays an important part here.  The large p-value for this first hypothesis tells us that there is no evidence the participant's height is helpful for predicting the number of steps taken _after accounting for their stride length_.  Similarly, the large p-value for the second hypothesis tells us that there is no evidence the participant's stride length is helpful for predicting the number of steps taken _after accounting for their height_.  However, we know that a person's stride is very correlated with their height --- those who are taller have longer legs and tend to have a longer stride.  That is, there is not much information that a person's stride length will tell us that their height does not already convey; that explains the results we are seeing here.  It is not that the height is not associated with the number of steps taken; it is that it is not helpful _above and beyond_ knowing the participant's stride length.  This is multicollinearity.

Multicollinearity can make inference on a single predictor misleading.  The p-value is not incorrect; we just need to remember that it is testing whether the term belongs after accounting for other predictors in the model.  That is, the regression model is trying to isolate the effect above and beyond that of other predictors in the model.  

:::{.callout-note}
A tell-tale sign of multicollinearity between two predictors is that alone, each is significantly associated with the response; but, when both are placed in the model, neither appears significant.  
:::

Multicollinearity is not necessarily a problem.  If our primary aim in constructing the regression model is prediction, then we are not concerned with the inference on each parameter individually.  The estimates of the parameters are valid; as a result, we can leave both predictors in the model.  However, if our goal is inference on the parameters to further characterize the relationship, then the standard errors are misleading (leading to unreliable p-values and confidence intervals).  The solution is to remove the predictor that is less likely to be on the "causal pathway" which requires discipline expertise.

:::{.callout-tip}
## Big Idea
When two predictors capture the same information, placing both in the model can lead to misleading p-values and confidence intervals for each predictor.  However, if the primary aim is prediction, this is not generally a concern.
:::
