% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Statistical Modeling for the Biological Sciences},
  pdfauthor={Eric M Reyes},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Statistical Modeling for the Biological Sciences}
\author{Eric M Reyes}
\date{Updated: 29 February 2024}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, interior hidden, enhanced, frame hidden, breakable, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

The biological sciences often yield data which present unique challenges
to analysis. This text introduces these challenges and the statistical
methods employed to overcome them. We begin with an introduction to the
use of statistical regression models and then explore how such models
can be altered to account for various features in the data. This could
include non-linear or categorical response variables, censored survival
(or reliability) data, or repeated measurements on the same subject. We
touch on additional topics, such as study design and power, drawing
causal conclusions from observational data, missing data, and general
modeling techniques throughout.

This text is applied, focusing primarily on knowing when various
modeling strategies are appropriate and how to interpret their results.
This course surveys many different analysis strategies under a
statistical modeling framework; we leave a thorough treatment of each
topic to other authors. Our primary aim is to enable readers to evaluate
the strength of evidence presented in the literature within their own
field of study.

As with any text in statistics, we seek to develop your statistical
literacy and statistical reasoning.

\part{Unit I: Review of Statistics and Probability}

We assume that you are familiar with performing statistical inference at
the introductory level; this includes graphical and numerical summaries,
inference (confidence intervals and hypothesis testing) for a mean
response, simple linear regression to characterize the relationship
between two quantitative variables, and analysis of variance for
comparing the mean response across groups. We also assume this
introduction to statistical inference includes major concepts like the
importance of study design in interpreting results, modeling the
sampling distribution of a statistic (or standardized test statistic)
using a classical approach (probability) or a modern empirical approach
(bootstrapping). When viewed as a list of topics like this, the
introductory course can feel overwhelming. In this first unit, we
provide a brief review of these topics through the lens of a unifying
framework for inference. Our goal is to provide a ``story'' that will be
further developed in the remainder of the text.

We also provide a brief introduction to the essential elements of
probability. Probability is the field within mathematics that studies
and models random processes. In contrast, Statistics is a discipline
separate from mathematics that uses data to make inference on a
population. Like many other disciplines (e.g., Engineering and the
Sciences), while Statistics is a separate discipline, the theory
underlying the discipline relies heavily on mathematics; for Statistics,
probability plays a pivotal role. We personally favor introducing
statistical concepts with minimal reference to probability, instead
choosing to build on students' intuition of probability. In line with
that philosophy, we will strive to introduce statistical approaches
within the biological sciences with minimal probability. However, we do
need a little more machinery to address these topics than is necessary
for an introductory course. As a result, this unit includes elements of
probability essential to our future development of statistical methods.
We choose to place it here so that it is easily referenced from multiple
units in the future and to emphasize that probability is separate from
statistics.

\hypertarget{sec-statistical-process}{%
\chapter{Overview of the Statistical
Process}\label{sec-statistical-process}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Research is about telling a story, and good data presentation and
statistical inference can help tell that story in a compelling way. This
chapter cannot replace an introductory course on statistical analysis.
We strive to give practical advice for data storage, presentation, and
analysis while presenting a framework for inference; it is within this
context that we review terminology fundamental to our study of
statistical models in the biological sciences.

\hypertarget{overview-of-drawing-inference}{%
\section{Overview of Drawing
Inference}\label{overview-of-drawing-inference}}

Every research question posed is trying to characterize a
\textbf{population}.

\begin{definition}[Population]\protect\hypertarget{def-population}{}\label{def-population}

The collection of subjects we would like to say something about.

\end{definition}

It is often impossible (or impractical) to observe the entire
population. Instead, we make observations on a subset of the population;
this smaller group is known as the \textbf{sample}.

\begin{definition}[Sample]\protect\hypertarget{def-sample}{}\label{def-sample}

The collection of subjects for which we actually obtain measurements
(data).

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

We acknowledge the weight of the term ``subject'' when discussing human
participants. Medical research has not been immune to unjust practices
exploiting marginalized groups within society. While the term persists
in the description of research practices in general, we opt for the term
``participants'' when describing those individuals who actually
participate in a study.

While the semantics may seem a small component, this small shift adds a
human element to the analysis. It is important to remember that each
observation within the data has a story, and when those stories
represent the lives of others, they deserve our full respect.

\end{tcolorbox}

For each subject within the sample, we obtain a collection of
measurements, which form our data. This could be the result, for
example, of a survey, examination of medical records, or a prospective
study which follows subjects for a lengthy period of time. The goal of
statistical modeling is to use the sample (the group we actually
observe) to say something about the population of interest (the group we
wish we had observed); this process is known as \textbf{statistical
inference} and is illustrated in
Figure~\ref{fig-statistical-process-statistical-process}.

\begin{definition}[Statistical
Inference]\protect\hypertarget{def-inference}{}\label{def-inference}

The process of using a sample to characterize some aspect of the
underlying population.

\end{definition}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/Statistical-Process-Statistical-Process.jpg}

}

\caption{\label{fig-statistical-process-statistical-process}Illustration
of the statistical process.}

\end{figure}

\hypertarget{data-storage}{%
\section{Data Storage}\label{data-storage}}

Each measurement, or piece of information, you record for a subject is a
different \textbf{variable}.

\begin{definition}[Variable]\protect\hypertarget{def-variable}{}\label{def-variable}

A measurement, or category, describing some aspect of the subject.

\end{definition}

In order to conduct analysis, it is best to adhere to ``tidy data
principles'' (Wickham 2014) when storing data. In brief:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each column contains a unique variable.
\item
  Each record (or row in the data set) corresponds to a different
  observation of the variable(s). If each subject is only measured once
  (a single survey for each subject, for example), each record will
  correspond to a different subject. If, on the other hand, each subject
  is measured multiple times (the same survey is given prior to an
  appointment and at a specified follow-up period, for example), there
  may be multiple records which correspond to the same subject, but each
  record corresponds to a unique observation.
\item
  If you have multiple data sets, there should be a variable in the
  table that allows the various tables to be linked (subject
  identifier). For larger more complex studies, for example, you may
  have one table that has the demographic information of subjects and a
  separate table which contains the lab results for the subjects.
\item
  The first row in the data set should have the names of each variable.
\end{enumerate}

The above description eliminates a common method of data storage ---
placing different groups in different spreadsheets. All observations
should be stored together. The first few records of a hypothetical data
set are illustrated in Table~\ref{tbl-statistical-process-tidy-data}.

\hypertarget{tbl-statistical-process-tidy-data}{}
\begin{table}
\caption{\label{tbl-statistical-process-tidy-data}Example of storying data according to ``tidy data'' principles. Data is
from a hypothetical study. }\tabularnewline

\centering
\begin{tabular}[t]{rlrrrl}
\toprule
Subject ID & Education & Age (yrs) & Parity & Number of Miscarriages & Treatment Group\\
\midrule
\cellcolor{gray!6}{1089} & \cellcolor{gray!6}{0-5yrs} & \cellcolor{gray!6}{28} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{Active Treatment}\\
1160 & 0-5yrs & 36 & 1 & 0 & Active Treatment\\
\cellcolor{gray!6}{1025} & \cellcolor{gray!6}{0-5yrs} & \cellcolor{gray!6}{34} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{Active Treatment}\\
1035 & 0-5yrs & 32 & 4 & 1 & Active Treatment\\
\cellcolor{gray!6}{1112} & \cellcolor{gray!6}{6-11yrs} & \cellcolor{gray!6}{32} & \cellcolor{gray!6}{3} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{Active Treatment}\\
\addlinespace
1030 & 6-11yrs & 33 & 4 & 1 & Active Treatment\\
\cellcolor{gray!6}{1159} & \cellcolor{gray!6}{0-5yrs} & \cellcolor{gray!6}{26} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{2} & \cellcolor{gray!6}{Placebo}\\
1207 & 0-5yrs & 42 & 1 & 0 & Placebo\\
\cellcolor{gray!6}{1179} & \cellcolor{gray!6}{0-5yrs} & \cellcolor{gray!6}{39} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{Placebo}\\
1014 & 0-5yrs & 34 & 4 & 0 & Placebo\\
\addlinespace
\cellcolor{gray!6}{1195} & \cellcolor{gray!6}{6-11yrs} & \cellcolor{gray!6}{35} & \cellcolor{gray!6}{3} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{Placebo}\\
1170 & 6-11yrs & 36 & 4 & 1 & Placebo\\
\bottomrule
\end{tabular}
\end{table}

Once your data has been placed in a spreadsheet, it should be kept
separate from the analysis. Any changes to the data should be done using
your analysis file so that those changes are clearly documented
alongside the analysis. While it may be easy, it is poor practice to
include graphics and numeric summaries in the same spreadsheet as the
data. If you want your data to be \emph{portable} (easily opened by any
spreadsheet or analysis software package), save your data as a comma
separated file (CSV).

\hypertarget{tabular-data-presentation}{%
\section{Tabular Data Presentation}\label{tabular-data-presentation}}

If you have several variables you want to summarize, this is probably
best done using a table. For example, you may want to summarize the
demographics of the subjects in your study across each treatment group.
How a variable is summarized depends on its type. \textbf{Qualitative}
(or \textbf{categorical}) variables define a grouping or categorization
of a subject (e.g., race, treatment group, etc.). When summarizing
qualitative data, we generally report the number of subjects in each
group and the corresponding percentage of the sample.

\begin{definition}[Categorical
Variable]\protect\hypertarget{def-categorical-variable}{}\label{def-categorical-variable}

Also called a ``qualitative variable,'' a measurement on a subject which
denotes a grouping or categorization.

\end{definition}

\textbf{Quantitative} (or \textbf{numeric}) variables are those
measurements for which arithmetic makes sense (e.g., heart rate, age).
These variables are generally summarized by reporting both a measure of
location and spread; this could be mean and standard deviation or median
and interquartile range.

\begin{definition}[Numeric
Variable]\protect\hypertarget{def-numeric-variable}{}\label{def-numeric-variable}

Also called a ``quantitative variable,'' a measurement on a subject
which takes on a numeric value \emph{and} for which ordinary arithmetic
makes sense.

\end{definition}

If you are not comparing groups of subjects, it is reasonable to report
results for the entire sample. If the goal of your research is to
compare groups (such as rural vs.~urban residents), we typically
summarize data within each group and present the comparisons side by
side. Table~\ref{tbl-statistical-process-data-summary} summarizes the
data from our hypothetical study, allowing the reader to compare the
treatment and placebo groups. Notice that while we might think of the
number of miscarriages as being a numeric variable, when there are only
a small number of possibilities, we might treat that variable as
categorical for the purposes of summarizing it.

\hypertarget{tbl-statistical-process-data-summary}{}
\begin{table}
\caption{\label{tbl-statistical-process-data-summary}Summary of patient characteristics from our hypothetical study. }\tabularnewline

\centering
\begin{tabular}{l|c|c}
\hline
**Characteristic** & **Active Treatment**, N = 165 & **Placebo**, N = 83\\
\hline
Education &  & \\
\hline
0-5yrs & 8 (4.8\%) & 4 (4.8\%)\\
\hline
6-11yrs & 80 (48\%) & 40 (48\%)\\
\hline
12+ yrs & 77 (47\%) & 39 (47\%)\\
\hline
Age & 30.51 (2.67) & 31.53 (5.28)\\
\hline
Parity & 2.08 (1.24) & 2.11 (1.28)\\
\hline
Number of Miscarriages &  & \\
\hline
0 & 113 (68\%) & 28 (34\%)\\
\hline
1 & 40 (24\%) & 31 (37\%)\\
\hline
2 & 12 (7.3\%) & 24 (29\%)\\
\hline
\end{tabular}
\end{table}

When reporting numerical summaries within the body of your report, it is
good to keep the same format as you adopt in the table; for example,
summarizing a qualitative variables with N (\%).

Statistics is generally concerned with explaining the variation in a
variable, and that is characterized by its \textbf{distribution}. When
we summarize a variable, whether numerically or graphically, we are
actually summarizing this distribution.

\begin{definition}[Distribution]\protect\hypertarget{def-distribution}{}\label{def-distribution}

The pattern of variability corresponding to a set of values.

\end{definition}

\hypertarget{graphical-data-presentation}{%
\section{Graphical Data
Presentation}\label{graphical-data-presentation}}

As the saying goes, a picture is worth 1000 words. Each graphic you
construct, however, should add value to the story you are telling. We
primarily reserve graphics for conveying a message about our primary
\textbf{response}.

\begin{definition}[Response
Variable]\protect\hypertarget{def-response}{}\label{def-response}

Also called the ``outcome,'' this is the primary variable of interest in
the research question; it is the variable we either want to explain or
predict.

\end{definition}

As with tabular data presentation, our approach to graphical
presentation depends on the type of variable being summarized. For
example, while a scatter-plot is well suited for examining the
relationship between two quantitative variables, side-by-side box-plots
are better suited for examining the relationship between a quantitative
response and a categorical predictor.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Following best practices in the research community, we recommend the use
of a \emph{bar chart} instead of a \emph{pie chart} when examining a
categorical response. Bar charts are often less cluttered and more
clearly communicate the same information.

\end{tcolorbox}

Figure~\ref{fig-statistical-process-graphics} illustrates two graphics
(one for a qualitative and one for a quantitative response); again, in
practice, your graphics should be driven by your research question.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-statistical-process-graphics-1.pdf}

}

\caption{\label{fig-statistical-process-graphics}Two graphical
presentations of data from a hypothetical study, adhering to good
graphical practices.}

\end{figure}

Notice that the left panel of the graphic makes use of bar charts to
compare a qualitative variable (number of miscarriages) across a second
qualitative variable (treatment group). The use of color here is
important because it brings out additional features that do not appear
on the x- or y-axis. The right panel of the graphic makes use of
box-plots (with jitter-plots overlaid) to compare a quantitative
variable (age of the patient) across the qualitative variable (treatment
group). One idea worth discussing here is that a graphical summary of a
quantitative variable should always portray both \emph{location} and
\emph{spread}. Notice that in the right panel in
Figure~\ref{fig-statistical-process-graphics}, we see that the ages of
patients receiving placebo are comparable (in location) to that of those
receiving the active treatment; however, the variability in the ages of
patients receiving placebo is much larger compared to those receiving
the active treatment. Compare this to
Figure~\ref{fig-statistical-process-poor-graphics}, which only
summarizes location with no sense of spread; while this is a popular
default graphic in some software, it does not adequately allow a reader
to determine the size of the effect relative to the variability in the
data.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-statistical-process-poor-graphics-1.pdf}

}

\caption{\label{fig-statistical-process-poor-graphics}Inappropriate
graphical presentation from a hypothetical study as it ignores a sense
of variability in the data.}

\end{figure}

\hypertarget{basic-terminology-for-statistical-tests}{%
\section{Basic Terminology for Statistical
Tests}\label{basic-terminology-for-statistical-tests}}

In some cases, summarizing the data numerically and graphically is
sufficient for telling a compelling story. Often, however, the summaries
are accompanied by a statistical analysis. Regardless of the simplicity
(or complexity) of the statistical procedure, there are a few
fundamental ideas which are common to all methods.

A \textbf{statistic} (summary of data) is a point estimate of a
\textbf{parameter} (corresponding value in the population of interest).
For example, the value 2.08 in
Table~\ref{tbl-statistical-process-data-summary} is the average number
of children among those women in the study who received the active
treatment; but, it \emph{estimates} the average number of children among
all women in the population who receive the active treatment.

\begin{definition}[Parameter]\protect\hypertarget{def-parameter}{}\label{def-parameter}

Numeric quantity which summarizes the distribution of a variable within
the \emph{population} of interest. Generally denoted by Greek letters in
statistical formulas.

\end{definition}

\begin{definition}[Statistic]\protect\hypertarget{def-statistic}{}\label{def-statistic}

Numeric quantity which summarizes the distribution of a variable within
the observed \emph{sample}.

\end{definition}

Instead of estimating a parameter with this single value, we can
estimate the parameter with a \textbf{confidence interval} (a 95\%
confidence interval is standard practice).

\begin{definition}[Confidence
Interval]\protect\hypertarget{def-confidence-interval}{}\label{def-confidence-interval}

An interval (range of values) estimate of a parameter that incorporates
the variability in the statistic. The process of constructing \(k\)\%
confidence intervals results in them containing the parameter of
interest in \(k\)\% of \emph{repeated} studies. The value of \(k\) is
called the \emph{confidence level}.

\end{definition}

It is important to recognize that the entire interval is our estimate.
In text, we generally report the point estimate with the 95\% confidence
interval in parentheses. For example,

\begin{quote}
The probability of a miscarriage is 0.32 (95\% CI: {[}0.25, 0.39{]}) for
women given the active treatment.
\end{quote}

There are several common misinterpretations of a confidence interval;
generally, these do not enter the literature because we avoid
interpreting the interval directly and simply state it and discuss its
implications (as above). For completeness, however, it is best to think
of a confidence interval as giving all the reasonable values of the
parameter based on the data observed.

While confidence intervals estimate an effect, a \textbf{p-value}
quantifies the amount of evidence in the data against the lack of an
effect.

\begin{definition}[P-Value]\protect\hypertarget{def-pvalue}{}\label{def-pvalue}

The probability, assuming the null hypothesis is true, that we would
observe a statistic, from sampling variability alone, as extreme or more
so as that observed in our sample. This quantifies the strength of
evidence against the null hypothesis. Smaller values indicate stronger
evidence.

\end{definition}

We generally report a p-value to 3 decimal places (with values less than
0.001 being written as ``\textless{} 0.001''). It is best to state
p-values alongside the conclusion. For example,

\begin{quote}
There is strong evidence (p \textless{} 0.001) that the active treatment
reduces the risk of a miscarriage.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-caution-color-frame, breakable, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, opacitybacktitle=0.6]

There are two very important things to keep in mind when examining a
p-value:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A small p-value does not imply the effect is clinically
  relevant/important. It simply indicates that we are able to
  statistically discern an effect/difference is present.
\item
  A large p-value does not imply there is no effect/difference. It
  simply indicates that we cannot statistically discern the presence of
  an effect/difference.
\end{enumerate}

\end{tcolorbox}

For these reasons, a p-value should always be accompanied by either a
confidence interval (preferred when possible) or a point estimate of the
effect to allow readers to determine if the impact is clinically
relevant.

When interpreting statistical results, the design of the study plays a
role. In particular, we can only conclude a causal relationship when the
data is from a \textbf{randomized clinical trial}. When your data is
from an \textbf{observational study}, any group comparisons are subject
to \textbf{confounding}.

\begin{definition}[Randomized Clinical
Trial]\protect\hypertarget{def-randomized-clinical-trial}{}\label{def-randomized-clinical-trial}

Also called a ``controlled experiment,'' a study in which each
participant is randomly assigned to one of the groups being compared in
the study.

\end{definition}

\begin{definition}[Observational
Study]\protect\hypertarget{def-observational-study}{}\label{def-observational-study}

A study in which each participant ``self-selects'' into one of groups
being compared in the study. The phrase ``self-selects'' is used very
loosely here and can include studies in which the groups are defined by
an inherent characteristic, the groups are determined according to a
non-random mechanism, and each participant chooses the group to which
they belong.

\end{definition}

\begin{definition}[Confounding]\protect\hypertarget{def-confounding}{}\label{def-confounding}

When the effect of a variable on the response is mis-represented due to
the presence of a third, potentially unobserved, variable known as a
confounder.

\end{definition}

\begin{example}[Dental Health and Cardiovascular
Disease]\protect\hypertarget{exm-dental-health}{}\label{exm-dental-health}

It has been suggested that brushing your teeth twice a day for at least
two minutes may lower the risk of cardiovascular diseases\footnote{\url{https://www.heart.org/en/news/2018/11/07/bad-tooth-brushing-habits-tied-to-higher-heart-risk}}.

However, most of these results are from large surveys, which are
observational studies. It is quite plausible that those who take
excellent care of their teeth tend to be health-conscious individuals,
and health-conscious individuals are more likely to have healthy diets
and exercise regularly, both of which decrease the risk of
cardiovascular diseases.

\end{example}

In Example~\ref{exm-dental-health}, being health-conscious is a
confounder because it is associated with \emph{both} the factor under
study (brushing behavior) \emph{and} the outcome of interest
(cardiovascular disease); see
Figure~\ref{fig-statistical-process-confounding}. In order to establish
a causal link between brushing and the risk of cardiovascular disease,
we could conduct a clinical trial in which we randomize patients to a
brushing routine and then track their long-term cardiovascular health;
in this design, the link between the confounder and the treatment group
is broken, allowing us to make a causal conclusion. While clinical
trials allow for causal conclusions, they are not always feasible or
practical; observational studies allow us to add to the body of
knowledge in such situations. There are some methods for addressing
confounding in observational studies through statistical analysis, but
such methods often require a large sample and more advanced methodology.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/Statistical-Process-Confounding.jpg}

}

\caption{\label{fig-statistical-process-confounding}Illustration of
confounding in observational studies.}

\end{figure}

\hypertarget{a-note-on-codebooks}{%
\section{A Note on Codebooks}\label{a-note-on-codebooks}}

A dataset on its own is meaningless if you cannot understand what the
values represent. \emph{Before} you access a dataset, you should always
review any available \textbf{codebook}.

\begin{definition}[Codebook]\protect\hypertarget{def-codebook}{}\label{def-codebook}

Also called a ``data dictionary,'' a codebook provides complete
information regarding the variables contained within a dataset.

\end{definition}

Some codebooks are excellent, with detailed descriptions of how the
variables were collected and appropriate units. Other codebooks give
only an indication of what each variable represents. Whenever you are
working with previously collected data, reviewing a codebook is the
first step; and, you should be prepared to revisit the codebook often
throughout an analysis. When you are collecting your own dataset,
constructing a codebook is essential for others to make use of your
data.

\hypertarget{sec-distributional-quartet}{%
\chapter{Distributional Quartet}\label{sec-distributional-quartet}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Any good statistical analysis moves between four key distributions ---
what we refer to as the \emph{Distributional Quartet}. While not always
explicitly discussed, these distributions are always present in an
analysis. Understanding their role is important to implementing and
interpreting an analysis.

We begin by considering the following example from Rosner (2006).

\begin{example}[Blood Pressure when Lying
Down]\protect\hypertarget{exm-distributional-quartet-bp}{}\label{exm-distributional-quartet-bp}

Blood pressure is one metric for the health of your heart. A blood
pressure reading includes two numbers --- the systolic blood pressure
(the ``top number,'' measures the amount of pressure in your arteries
when your heart contracts) and the diastolic blood pressure (the
``bottom number,'' measures the amount of pressure in your arteries when
your heart is between beats).

An individual does not have a single blood pressure reading; our blood
pressure fluctuates as a result of activity as well as our position. In
a study examining the impact of position on blood pressure, 32
participants had their blood pressure measured while lying down with
their arms at their sides.

\end{example}

Stated simply, the discipline of statistics is about using data to say
something about a process that characterizes a population. Our analysis,
therefore, begins with the \textbf{Distribution of the Population}.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-important-color-frame, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Distribution of the Population}, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, opacitybacktitle=0.6]

The pattern of variability in values of a variable across individuals of
the population. The shape of this distribution is governed by unknown
parameters. While we generally do not know the shape of this
distribution, we may occasionally posit a model for it.

\end{tcolorbox}

We are interested in using the data from this study to characterize the
systolic blood pressure of individuals when in this recumbent position,
with their arm at their side. Of course, we are unable to assess the
blood pressure of all individuals in the world. Therefore, we do not
know what the distribution of systolic blood pressure measurements is
for this population. However, we \emph{might} posit a model for this
distribution. Such models, which must account for the variability among
the population, are studied in probability theory, which we consider in
the next section. For now, it suffices to imagine the distribution of
the population graphically; it is characterized by the unknown
parameters (Figure~\ref{fig-distributional-quartet-population}).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-distributional-quartet-population-1.pdf}

}

\caption{\label{fig-distributional-quartet-population}Hypothetical model
for the distribution of systolic blood pressure within the population.
The unknown population mean is denoted on the graphic.}

\end{figure}

The data we actually observe comes from the sample, and we will use this
smaller group to say something about the underlying population. It is
the distribution of sample which we are summarizing each time we
construct a graphic.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-important-color-frame, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Distribution of the Sample}, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, opacitybacktitle=0.6]

The pattern of variability in values of a variable across individuals of
the sample. This is typically summarized graphically and numerically.

\end{tcolorbox}

Figure~\ref{fig-distributional-quartet-sample} summarizes the sample
using a histogram. If our sample is collected well, then it should be
representative of the population, meaning that the distribution of the
sample should reflect the characteristics of the (unobserved)
distribution of the population. The location, spread, and shape should
all reflect what we might see within the population.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-distributional-quartet-sample-1.pdf}

}

\caption{\label{fig-distributional-quartet-sample}Distribution of
systolic blood pressure within the observed sample of 32 participants.}

\end{figure}

Examining the sample is critical to understanding the story in the data.
We must remember, however, that the statistics we compute using our
sample are dependent upon the data we observed. If we were to repeat the
sampling process (collect new data to answer the same question), our
statistics would change. Good inference requires us to acknowledge this
sampling variability and incorporate it when making statements about the
population.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-important-color-frame, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Sampling Distribution}, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, opacitybacktitle=0.6]

The pattern of variability in values of a \emph{statistic} (or
standardized statistic) across repeated samples of the same size from
the population. This must be modeled in practice.

\end{tcolorbox}

As we are generally unable to perform replicate studies, we model the
sampling distribution using the data available (future chapters will
discuss the various methods available for modeling this distribution).
The model for the sampling distribution allows us to determine values of
the parameter for which the data is consistent. That is, it allows us to
compute a confidence interval to estimate a parameter of interest. For
example, a 95\% CI for the mean systolic blood pressure (mm Hg, when
recumbent with arm at their side), based on our data available, is
(113.1, 121.9); this is illustrated in
Figure~\ref{fig-distributional-quartet-sampling-distribution} alongside
the model for the sampling distribution of the sample mean systolic
blood pressure from which the CI was computed.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-distributional-quartet-sampling-distribution-1.pdf}

}

\caption{\label{fig-distributional-quartet-sampling-distribution}Empirical
model for the sampling distribution of mean systolic blood pressure for
a sample of 32 participants. A 95\% confidence interval is also
illustrated.}

\end{figure}

While statisticians generally have a preference toward estimation (and
therefore reporting confidence intervals), scientists often have
specific research questions they would like to address. When this is the
case, the scientist may want to quantify the strength of evidence in the
sample against some specified hypothesis. In order to determine how rare
our data is, we must know what we should expect under the specified
hypothesis.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-important-color-frame, breakable, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Null Distribution}, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, opacitybacktitle=0.6]

The sampling distribution of a statistic (or standardized ``test''
statistic) \emph{when the null hypothesis is true}. This must be modeled
in practice.

\end{tcolorbox}

The null distribution effectively tells us what values of the statistic
we would expect to see if the null hypothesis were true. If our observed
statistic seems plausible according to the null distribution, then it
follows that the null hypothesis is reasonable. If, on the other hand,
our observed statistic is unexpected according to the null distribution,
then we have evidence that the null hypothesis is false (and therefore
that the alternative is true). That is, we are able to statistically
discern the difference between our data and what we would have expected
to see under the null hypothesis. Note that these are our only two
potential conclusions. The strength of the evidence is quantified by the
p-value. For example, suppose we are interested in using our data to
test the following set of hypotheses:

\[H_0: \mu = 120 \qquad \text{vs.} \qquad H_1: \mu \neq 120,\]

where \(\mu\) represents the average systolic blood pressure of an
individual when lying down.

That is, we are interested in determining if there is evidence that, on
average, the systolic blood pressure (recumbent with arm at side)
differs from 120 mm Hg. According to the data, it is reasonable (p =
0.247) that the average systolic blood pressure is 120 mm Hg. The
computation of the p-value is illustrated in
Figure~\ref{fig-distributional-quartet-null-distribution} alongside the
model for the null distribution for this particular hypothesis.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-distributional-quartet-null-distribution-1.pdf}

}

\caption{\label{fig-distributional-quartet-null-distribution}Empirical
model for the null distribution of mean systolic blood pressure within
the sample of 32 participants when the null hypothesis is that the mean
systolic blood pressure is 120 mm Hg. The p-value is also illustrated.}

\end{figure}

Our focus in this short review is not this specific analysis. The
emphasis here is on the \emph{process} --- the use of the four
distributions used in a statistical analysis. They allow us to take the
sample and make inference on the underlying population. As we move
forward in the course, we will study more sophisticated models. However,
behind the scenes, the procedures are always bouncing between these four
distributions in order to allow us to make inference.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Note that we construct a \emph{model} for the sampling distribution of a
statistic or a \emph{model} for the null distribution of a
(standardized) statistic. All models are simplifications of complex
processes; and, the validity of each model requires certain conditions
be true about the data generating process.

Generally, we must make assumptions about the data generating process.
Therefore, the reliability of these models, and consequently our
analysis, depends on whether these assumptions are reasonable. We must
always keep in mind that our analysis is subject to the assumptions we
make.

\end{tcolorbox}

\hypertarget{sec-essential-probability}{%
\chapter{Essential Probability}\label{sec-essential-probability}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The discipline of Statistics uses data to make inference on a
population. In turn, statistical theory is built on probability --- the
discipline of mathematics that studies and models random processes.
While we do not need to be experts in probability to be practitioners of
statistical methodology, a foundation in models from probability is
helpful for seeing common threads in statistical modeling. This chapter
provides a brief introduction to the most relevant aspects of
probability theory necessary for engaging with the remainder of the
text.

\hypertarget{density-functions-as-models}{%
\section{Density Functions as
Models}\label{density-functions-as-models}}

Any process for which the outcome cannot be predicted with certainty is
a random process. Typically, probability is taught from a mathematical
perspective, with a goal of constructing a coherent and complete
framework for characterizing such random processes. Here, our goal is to
introduce key probability concepts by relating them to their
data-centric analogues. That is, we want to think of probability in
light of how we will use it in statistical analysis.

Each time we collect data, we can think of each observation as the
result of a random process. These observations are recorded as variables
in our dataset. In probability, a random variable is used to represent a
measurement that results from a random process. Just as we have both
\emph{quantitative} and \emph{qualitative} variables, there are
\emph{continuous} and \emph{discrete} random variables.

\begin{definition}[Random
Variable]\protect\hypertarget{def-random-variable}{}\label{def-random-variable}

A random variable represents a measurement that will be collected and
for which the value cannot be predicted with certainty; they are
generally represented with a capital letter. Continuous random variables
represent quantitative measurements while discrete random variables
represent qualitative measurements.

\end{definition}

Consider measuring a single variable on a sample of \(n\) participants.
Then, we might represent the measurements we will obtain as
\(X_1, X_2, \dots, X_n\).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

There are many ways to interpret probability. In classical
(``frequentist'') statistics, we think of probability as the likelihood
of an event over repeated experimentation. Therefore, probability does
not describe events that have already occurred; we can only describe the
likelihood of future events.

\end{tcolorbox}

Each of our random variables \(X_1, X_2, \dotsc, X_n\) will be
observations from some underlying population. As we described in
previous chapters, the distribution of the population is unknown.
However, we might posit a model for this distribution. This is our
primary use of probability theory in statistics --- to model
distributions. The most common way to represent a probability model is
through its density function.

\begin{definition}[Density
Function]\protect\hypertarget{def-density-function}{}\label{def-density-function}

A density function \(f\) relates the potential values of a random
variable \(X\) with the probability those values occur. For a
\emph{continuous} random variable, the probability the random variable
\(X\) falls within an interval \((a, b)\) is given by

\[Pr(a \leq X \leq b) = \int_{a}^{b} f(x) dx.\]

For a \emph{discrete} random variable, the probability the random
variable \(X\) is equal to the value \(u\) is given by

\[Pr(X = u) = f(u).\]

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

In a probability course, there is often a distinction made between
probability density functions (continuous random variables) and
probability mass functions (discrete random variables). We do not make
this distinction and instead rely on the context to determine whether we
are dealing with a continuous or discrete random variable.

\end{tcolorbox}

With few exceptions, we will be working with continuous random
variables. As a result, the density function is a smooth function over
some region, and the actual value of the function is not interpretable;
instead, we obtain probabilities by computing the area under the curve.
Again, drawing connections to data analysis, we can think of a density
function as a mathematical formula representing a smooth histogram. The
area under the curve for any region gives the proportion of the
population which has a value in that region. That is, we get the
probability that a random variable will be in an interval by integrating
the density function over that interval.
Figure~\ref{fig-essential-probability-density} illustrates this idea; we
have a hypothetical dataset that has been summarized using a histogram;
we overlay a density function (with the corresponding mathematical model
that describes this density function). The figure shows how the sample
(summarized in the histogram) is approximating the population (the
density function).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-essential-probability-density-1.pdf}

}

\caption{\label{fig-essential-probability-density}Illustration of a
density function representing the distribution of the population and a
histogram from a representative sample.}

\end{figure}

Especially for visualization, the density function is the most common
way of characterizing a probability model. However, computing the
probability using the density is problematic due to the integration
required. Many software programs address this by working with the
cumulative distribution function (CDF).

\begin{definition}[Cumulative Distribution Function
(CDF)]\protect\hypertarget{def-cdf}{}\label{def-cdf}

Let \(X\) be a random variable; the cumulative distribution function
(CDF) is defined as

\[F(u) = Pr(X \leq u).\]

For a continuous random variable, we have that

\[F(u) = \int_{-\infty}^{u} f(x) dx\]

implying that the density function is the derivative of the CDF. For a
discrete random variable

\[F(u) = \sum_{x \leq u} f(x).\]

\end{definition}

Working with the CDF improves computation because it avoids the need to
integrate each time; instead, the integral is computed once (and stored
internally in the computer) and we use the result to compute
probabilities directly.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Density functions are the mathematical models for distributions; they
link values of the variable with the likelihood of occurrence. However,
for computational reasons, we often work with the cumulative
distribution function which provides the probability a random variable
is less than or equal to a value.

\end{tcolorbox}

\hypertarget{summarizing-distributions-parameters}{%
\section{Summarizing Distributions
(Parameters)}\label{summarizing-distributions-parameters}}

Most scientific questions are focused on the location or spread of a
distribution. For example, we are interested in estimating the average
yield of a crop, or the variance in the amount of sleep among college
students. Introductory statistics introduces summaries of location and
spread within the sample (e.g., sample mean for location and sample
variance for spread). Analogous summaries exist for density functions.

In particular, the mean of a random variable (denoted by \(E(X)\)) and
the variance of a random variable (denoted by \(Var(X)\)) are measures
of the location and spread, respectively, of the distribution
represented by its corresponding density function. When the density
function is a model for the population, these represent the parameters
of the population --- the same parameters we estimate and make inference
on using our data analysis. For completeness, we present the
computational formulas for the mean and variance of a random variable,
but we do not make use of these formulas moving forward. Instead, we
simply note that these formulas are similar to their sample
counterparts.

\begin{definition}[Mean and Variance of a Random
Variable]\protect\hypertarget{def-rv-mean-variance}{}\label{def-rv-mean-variance}

Suppose \(X\) is a random variable with density function \(f\). If \(X\)
is a continuous random variable, then the mean and variance are given by

\[
\begin{aligned}
  E(X) &= \int x f(x) dx \\
  Var(X) &= \int \left(x - E(X)\right)^2 f(x) dx.
\end{aligned}
\]

If \(X\) is a discrete random variable, then the mean and variance are
given by

\[
\begin{aligned}
  E(X) &= \sum x f(x) \\
  Var(X) &= \sum \left(x - E(X)\right)^2 f(x).
\end{aligned}
\]

\end{definition}

As we have stated, the distribution of the population is generally
unknown. If we were able to fully specify the density function for the
population, then there would be no need for statistical analysis.
Instead, the model is generally posited up to some unknown values
(parameters). For example, a researcher might posit that within the
population, the time until a medical device fails could be modeled using
the density

\[f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}} \qquad x > 0.\]

Here, the researcher has really posited a form of the model, but not the
exact model as \(\mu\) is unknown. The value \(\mu\) represents the
average response (which could be confirmed using the formulas in the
above definition). In such cases, making inference on the parameters
allows us to characterize the distribution of the population.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

When a probability model is specified for a population, it is generally
specified up to some unknown parameter(s). Making inference on the
unknown parameter(s) therefore characterizes the distribution ---
characterizes the manner in which the response varies across individuals
in the population.

\end{tcolorbox}

\hypertarget{specific-models-for-populations}{%
\section{Specific Models for
Populations}\label{specific-models-for-populations}}

While we could posit any non-negative function as a model for a density
function, there are some models that are very common. The most common
model for the population of a continuous random variable is the Normal
distribution.

\begin{definition}[Normal (Gaussian)
Distribution]\protect\hypertarget{def-normal-distribution}{}\label{def-normal-distribution}

Let \(X\) be a continuous random variable. \(X\) is said to have a
Normal (or Gaussian) distribution if the density is given by

\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} \qquad -\infty < x < \infty,\]

where \(\mu\) is any real number and \(\sigma^2 > 0\).

\begin{itemize}
\tightlist
\item
  \(E(X) = \mu\)
\item
  \(Var(X) = \sigma^2\)
\end{itemize}

We write \(X \sim N\left(\mu, \sigma^2\right)\), which is read ``X has a
Normal distribution with mean \(\mu\) and variance \(\sigma^2\).'' This
short-hand implies the density above.

\end{definition}

This model is a bell-shaped distribution centered at the mean \(\mu\).
While this is a common model, it should not be assumed by default. In
future chapters, we will consider methods for assessing whether assuming
a Normal distribution is reasonable.

When a response is binary (assumes one of two values), it is a Bernoulli
distribution. In order to make use of this distribution, we typically
define one of the two possible outcomes as a ``success'' and the other
as a ``failure.'' For example,

\[X = \begin{cases} 1 & \text{if a success is observed} \\ 0 & \text{if a success is not observed.} \end{cases}\]

\begin{definition}[Bernoulli
Distribution]\protect\hypertarget{def-bernoulli-distribution}{}\label{def-bernoulli-distribution}

Let \(X\) be a discrete random variable taking the value 0 or 1. \(X\)
is said to have a Bernoulli distribution with density

\[f(x) = \theta^x (1 - \theta)^{1 - x} \qquad x \in \{0, 1\},\]

where \(0 < \theta < 1\) is the probability that \(X\) takes the value
1.

\begin{itemize}
\tightlist
\item
  \(E(X) = \theta\)
\item
  \(Var(X) = \theta(1 - \theta)\)
\end{itemize}

We write \(X \sim Ber(\theta)\), which is read ``X has a Bernoulli
distribution with probability \(\theta\).''

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

A generalization of the Bernoulli distribution is the Binomial
distribution. So, we sometimes hear people refer to a Bernoulli
distribution as ``a Binomial distribution with a single event.''

\end{tcolorbox}

\hypertarget{models-for-sampling-distributions-and-null-distributions}{%
\section{Models for Sampling Distributions and Null
Distributions}\label{models-for-sampling-distributions-and-null-distributions}}

A statistical analysis does not exist in a vacuum. Instead, based on the
context of the study, we make assumptions about the process which
generated the data. The conditions we are willing to assume govern how
we model the sampling distribution or null distribution. Occasionally,
we can lean on statistical theory to say how the sampling distribution
or null distribution will behave. That is, under certain conditions,
statistical theory tells us what the appropriate model is. In these
situations, there are some common models.

The t-distribution is a bell-shaped distribution, similar to the Normal
distribution but with wider tails. It has a single parameter, known as
the degrees of freedom. Note that unlike many other distributions, this
parameter (the degrees of freedom) is not associated with the location
of the distribution. Instead, the parameter governs the spread (but is
not the variance).

\begin{definition}[t-Distribution]\protect\hypertarget{def-t-distribution}{}\label{def-t-distribution}

Let \(X\) be a continuous random variable. \(X\) is said to have a
t-distribution if the density is given by

\[f(x) = \frac{\Gamma \left(\frac{\nu+1}{2} \right)} {\sqrt{\nu\pi}\,\Gamma \left(\frac{\nu}{2} \right)} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}} \qquad x > 0\]

where \(\nu > 0\) is the degrees of freedom.

We write \(X \sim t_{\nu}\), which is read ``X has a t-distribution with
\(\nu\) degrees of freedom.''

\end{definition}

The Chi-Square distribution is a skewed distribution (looks like a giant
slide). It has a single parameter, known as the degrees of freedom. The
degrees of freedom for this distribution characterize both the location
and spread simultaneously.

\begin{definition}[Chi-Square
Distribution]\protect\hypertarget{def-chi-square-distribution}{}\label{def-chi-square-distribution}

Let \(X\) be a continuous random variable. \(X\) is said to have a
Chi-Square distribution if the density is given by

\[f(x) = \frac{1}{2^{\nu/2}\Gamma (\nu/2)}\;x^{\nu/2-1}e^{-x/2} \qquad x > 0,\]

where \(\nu > 0\) is the degrees of freedom.

We write \(X \sim \chi^2_{\nu}\), which is read ``X has a Chi-Square
distribution with \(\nu\) degrees of freedom.''

\end{definition}

The F-distribution is a skewed distribution. It has two parameters,
known as the numerator and denominator degrees of freedom. While neither
variable is the mean or variance, together these two parameters
characterize both the location and the spread.

\begin{definition}[F-Distribution]\protect\hypertarget{def-f-distribution}{}\label{def-f-distribution}

Let \(X\) be a continuous random variable. \(X\) is said to have an
F-distribution if the density is given by

\[f(x) = \frac{\Gamma((r + s)/2)}{(\Gamma(r/2) \Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \qquad x > 0,\]

where \(r,s > 0\) are the numerator and denominator degrees of freedom,
respectively.

We write \(X \sim F_{r, s}\), which is read ``X has an F-distribution
with r numerator degrees of freedom and s denominator degrees of
freedom.''

\end{definition}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-essential-probability-comparisons-1.pdf}

}

\caption{\label{fig-essential-probability-comparisons}Comparison of
various common distributions.}

\end{figure}

The formulas above are ugly, but we will not be working with them
directly. Instead, statistical software has these distributions
embedded. The key idea here is that when we know the model for a
sampling distribution, we are able to rely on that model in order to
obtain confidence intervals. And, when we have a model for the null
distribution, we are able to rely on that model to obtain p-values.
These models are behind default implementations of statistical methods
in software.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Some probability models occur so frequently that we give them names for
easy reference. Some models are common for modeling the population, in
which case they are defined in terms of unknown parameters to be
estimated. Some models are common for modeling sampling distributions or
null distributions, in which case their form will be explicitly
determined according to statistical theory.

\end{tcolorbox}

\part{Unit II: General Linear Model}

The general linear model, also known as multiple linear regression,
provides a framework appropriate for modeling a continuous outcome
(response) as a function of several predictors. We introduce the
framework and illustrate how it unifies the methods typically discussed
in an introductory statistics course.

\hypertarget{sec-glm-model-framework}{%
\chapter{General Linear Model Framework}\label{sec-glm-model-framework}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The most interesting scientific questions involve characterizing the
relationship between a response and some predictor. And, we know that
these relationships do not exist in a vacuum. The response we observe is
typically the result of a complex data generating process involving
several potential predictors (or features/characteristics) of the
subjects in the population. In order to incorporate these additional
features, we need \emph{multivariable} models.

The development of a model should not be divorced from its intended use,
and in general, there are three uses for multivariable models. That is,
the majority of scientific questions can be categorized into one of
three groups: prediction, isolating an effect, or studying the interplay
between variables.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

There are primarily three uses for a multivariable model

\begin{itemize}
\tightlist
\item
  \textbf{Prediction}: modeling a relationship for the purpose of
  estimating a future occurrence given new data.
\item
  \textbf{Isolating an Effect}: describing the relationship between a
  response and predictor after accounting for the influence of other
  predictors measured.
\item
  \textbf{Studying the Interplay}: examining how the relationship of two
  variables is impacted by the value of a third variable.
\end{itemize}

\end{tcolorbox}

While we introduce these elements in the context of the general linear
model, note that these uses carry over into other regression models we
will examine.

Consider a gardener studying two common organic fertilizers. She could
have the following questions in mind:

\begin{enumerate}
\def\labelenumi{\Alph{enumi}.}
\tightlist
\item
  What do I anticipate the yield of tomatoes to be next summer when
  using cow manure?
\item
  Does bat guano tend to result in higher tomato yields compared with
  cow manure after accounting for any impact on yield that results from
  the amount of water the plants receive?
\item
  Does the efficacy of bat guano (compared with cow manure) depend on
  the amount of sunlight the plants receive?
\end{enumerate}

The first question is an example of prediction; given the fertilizer
applied (as well as potentially other characteristics of the garden),
what does she expect the results to be in the future? The second
question examines the impact (or effect) of the fertilizer \emph{above
and beyond} any impact of watering; she is interested in
\emph{isolating} the effect of fertilizer from the effect of watering.
In the last question, she is not only interested in the effect of the
fertilizer on the yield, but she wants to acknowledge that this impact
could depend on a third variable (sunlight); for example, bat guano may
be superior under low light settings but inferior under lots of
sunlight. This is an example of the interplay between the fertilizer and
the sunlight.

In each of these objectives, there are multiple things at play,
requiring modeling techniques that account for multiple predictors
simultaneously. The general linear model views the response as a being
the result of a linear combination of several variables; our measurement
of this linear combination is then subject to error. Specifically, the
framework generalizes the simple linear regression model studied in
introductory statistics to characterize the average response as a
function of several variables simultaneously.

\begin{definition}[General Linear
Model]\protect\hypertarget{def-general-linear-model}{}\label{def-general-linear-model}

The general linear model views the response (outcome) as a linear
combination of several predictors:

\[
\begin{aligned}
  (\text{Response})_i 
    &= \beta_0 + \beta_1 (\text{Predictor 1})_{i} + \beta_2 (\text{Predictor 2})_{i} + \dotsb + 
      \beta_p (\text{Predictor } p)_{i} + \varepsilon_i \\
    &= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i} + \varepsilon_i
\end{aligned}
\]

where \(n\) is the number of subjects in the sample, \(p < n\) is the
number of predictors in the model, and \(\varepsilon_i\) is a random
variable that captures the error in the response.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Many texts use \(y_i\) to denote the response of the \(i\)-th
observation and \(x_{j,i}\) to denote the value of the \(j\)-th
predictor for the \(i\)-th subject, resulting in the general linear
model having the form

\[y_i = \beta_0 + \sum\limits_{j=1}^{p} \beta_j x_{j, i} + \varepsilon_i.\]

This notation is helpful when discussing the underlying mathematics, but
we prefer being more explicit in identifying the response and predictors
when discussing the model itself.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Some disciplines refer to the response/outcome as the ``dependent
variable'' and the predictors as ``independent variables,'' but we find
this language a bit dated.

We will use the terms ``predictor'' and ``covariate'' interchangeably,
while some disciplines distinguish between categorical predictors as
factors and continuous predictors as covariates (variables that
``co-vary'' with the factor of interest).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While not a theoretical requirement, we will only consider the case
where \(p < n\), which is common in many disciplines. One discipline in
which this is often not valid is genetics. Special methods are required
in such ``high dimensional'' settings that are beyond the scope of this
text.

\end{tcolorbox}

The general linear model has two distinct components --- a deterministic
component (the linear combination of the predictors) and a stochastic
component (the error term). We can think of the error term as the ``junk
drawer'' for the model, capturing anything not explained by the
deterministic portion of the model. The error could include systematic
error in measuring the response, biological error contributing to the
fact that two subjects with the same values of the predictors have
different responses, etc.\\
:::\{.callout-caution\} We stress that
Definition~\ref{def-general-linear-model} is a \emph{model}. Like all
models, it is a simple representation of a complex process. It is
something we posit characterizes the underlying data generating process.
:::

The key feature of this model is that it relates the response to several
predictors \emph{simultaneously}. However, this model is currently
comprised of unknown parameters (the coefficients
\(\beta_1, \beta_2, \dotsc, \beta_p\)). For it to be useful in practice,
we need estimates of these parameters.

\hypertarget{parameter-estimation}{%
\section{Parameter Estimation}\label{parameter-estimation}}

The coefficients in front of each predictor act as parameters in the
model, as they are unknown and characterize the distribution of the
response in some way. Our goal is to construct estimates of these
unknown quantities. The most common method of estimation is the method
of least squares.

\begin{definition}[Least Squares
Estimation]\protect\hypertarget{def-least-squares}{}\label{def-least-squares}

The method of least squares may be used to estimate the coefficients
(parameters) of a linear model. In particular, we choose the values of
the coefficients that minimize

\[\sum\limits_{i=1}^{n} \left((\text{Response})_i - \beta_0 - \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i}\right)^2.\]

The resulting ``least squares'' estimates are denoted
\(\widehat{\beta}_0, \widehat{\beta}_1, \dotsc, \widehat{\beta}_p\).

\end{definition}

It is important to remember that the method of least squares results in
\emph{estimates} of the parameters. We are not ``solving'' for the
parameters; the parameters will always remain unknown quantities. We are
using data to estimate the parameters. There is really nothing
statistical about least squares. It is simply an optimization problem
--- choosing coefficients to minimize some criteria. Of course, we do
not determine these estimates by hand; instead, we rely on statistical
software.

We cannot stress enough that the act of obtaining these estimates is
simply an optimization exercise. While a computer can provide these
estimates, we cannot yet even interpret these estimates without further
assumptions on the model. This is where it becomes a \emph{statistical}
problem --- specifying the conditions required for the purpose of making
inference on the unknown parameters.

\hypertarget{conditions-on-the-model}{%
\section{Conditions on the Model}\label{conditions-on-the-model}}

The act of estimation alone is really a mathematical problem. Being able
to describe the properties of those estimates, quantify the variability
in those estimates, and use those estimates to make inference on the
population parameters is where we enter statistics. Whenever a random
variable is present in a model, inference requires us to make
assumptions about its underlying distribution. As analysts, we balance
making inference easy mathematically by making more assumptions (adding
more structure to the model) and making the model more flexible (not
making the model too restrictive).

Most software, by default, places four conditions on the distribution of
the error term in the model. We refer to this collection of conditions
as the ``classical regression model.''

\begin{definition}[Classical Regression
Model]\protect\hypertarget{def-classical-regression}{}\label{def-classical-regression}

In the ``classical regression model,'' we place the following four
conditions on the distribution of the error \(\varepsilon_i\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The average error across all levels of the predictors is 0;
  mathematically, we write
  \(E\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = 0\).
\item
  The variance of the errors is constant across all levels of the
  predictors; mathematically, we write
  \(Var\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = \sigma^2\)
  for some unknown constant \(\sigma^2 > 0\). This is sometimes referred
  to as homoskedasticity.
\item
  The error terms are independent; in particular, the magnitude of the
  error for one observation does not influence the magnitude of the
  error for any other observation.
\item
  The distribution of the errors follows a Normal distribution with the
  above mean and variance.
\end{enumerate}

\end{definition}

It would be a mistake to consider the above conditions only from a
probabilistic perspective; wrestling with what these mean in practice is
critical to understanding the model.

The first condition says the structure of the model is correct; that is,
no variables were omitted and the functional form of the response is
really determined by a linear combination of the predictors. Violations
of this assumption are very serious and indicate a different model
structure is needed. Essentially, if we believe this condition is not
met, it means we should revisit the science and rationale behind the
proposed model because it is likely invalid.

The second condition considers the precision with which the response is
measured. The condition asserts that this precision is consistent across
all possible values for the predictors. For example, consider the
academic performance of two classes; this condition prohibits cases in
which the grades for one class have a wider range than the grades for
the other.

The third condition eliminates data for which measurements are related
beyond sharing common values of the predictors in the model. For
example, suppose we are modeling the height of a tree as a function of
its age. All trees of a similar age may be ``related'' in the sense that
we expect them to have similar heights; the model allows this. However,
it does not allow for trees being ``related'' in the sense that trees in
a similar region will share a similar height due to differences in
resources among regions; this is prohibited because ``region'' is not
captured by the model. In the biological sciences, this condition is
often called into question when we take repeated measurements on
subjects or when observations are measured close together in time. This
type of data will be addressed later in the text
(Chapter~\ref{sec-rm-terminology}).

The last condition is a strong one; it states that we are able to fully
characterize the distribution of the error terms. While the other
conditions describe certain characteristics of the distribution, this
says we know the exact form of the distribution. Historically, this
condition was imposed to ensure the error terms were well behaved (and
because the probability theory worked out nicely).

Statistics courses (especially the introductory course) focus on these
four conditions on the error. However, the classical framework typically
also imposes additional conditions on the predictors.

\begin{definition}[Classical Regression (Conditions on
Predictors)]\protect\hypertarget{def-classical-regression-cont}{}\label{def-classical-regression-cont}

The classical regression model
(Definition~\ref{def-classical-regression}) places the following
conditions on the predictors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each predictor is measured without error.
\item
  Each predictor has an additive linear effect on the response.
\end{enumerate}

\end{definition}

The first condition states that there cannot be any noise present in the
measurement of the \emph{predictors}. For example, imagine modeling the
length (or height) of infants as a function of their age. When the
doctor asks for the age of the child, we are assuming that this age can
be computed/measured without error. This seems reasonable when the
predictor is age. However, consider using the temperature of the infant
as a predictor in the model; if the thermometer is only accurate to
within 2 tenths of a degree, than we may believe that the body
temperature is measured with error. Addressing measurement error in
models is beyond the scope of this text, and it is in general a
difficult problem. Typically, even if a predictor is potentially
measured with error, we are able to assume the error is negligible
compared to the amount of error in the response. Throughout the text, we
will assume all predictors are measured without error.

The second condition on the predictors is very closely related to the
condition on the errors that the mean of the errors is 0. If we are
empirically building a model and find evidence that the model has been
mis-specified, it is generally a result of the predictors not having a
linear relationship with the response.

\hypertarget{alternate-characterization-of-the-model}{%
\section{Alternate Characterization of the
Model}\label{alternate-characterization-of-the-model}}

Recall that a distribution is just the pattern of variability among the
values of a variable; that is, a distribution describes how values
differ from one another. Chapter~\ref{sec-essential-probability}
presented probability tools that can be used to model these
distributions. We saw that it is possible to specify these models up to
some unknown parameters; for example, we may write
\(X \sim N\left(\mu, \sigma^2\right)\) in order to say the density of
the random variable \(X\) can be modeled using the following
mathematical formula:

\[\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2}.\]

We often think about these parameters as being a single value, but
nothing prohibits that value from being described by a function of
variables. That is, we could let \(\mu = g(\text{Predictors})\) for some
function \(g\). In fact, the conditions on the error term specified in
the previous section lead us to an alternate characterization of the
general linear model.

\begin{definition}[Alternate Characterization of the Classical
Regression
Model]\protect\hypertarget{def-alternate-characterization}{}\label{def-alternate-characterization}

Under the classical regression conditions on the error term (see
Definition~\ref{def-classical-regression}), we can characterize the
classical regression model as

\[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i \stackrel{\text{Ind}}{\sim} N\left(\beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i, \sigma^2\right).\]

Here, the symbol \(\mid\) is read ``given'' and means that the
distribution of the response is specified after knowing the values of
the predictors. That is, the distribution of the response depends on
these variables.

\end{definition}

The alternate characterization of the regression model in
Definition~\ref{def-alternate-characterization} is particularly useful
in statistical theory, but that is not why we mention it here. We
mention this form because it sheds light on the true nature of
regression models (beyond just the classical regression model) ---
regression models characterize the distribution of the response.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Regression models allow the parameters characterizing the distribution
of the population to depend on the predictors through some function.

\end{tcolorbox}

Closely examining Definition~\ref{def-alternate-characterization}, we
see that the deterministic portion of the general linear model is
actually characterizing the \emph{mean} of the response (for specified
values of the predictors). In fact, this realization is actually the
direct result of the first (``mean 0'') condition we placed on the error
terms. This is what allows us to begin interpreting the parameters in
the model.

\hypertarget{interpretation-of-parameters}{%
\section{Interpretation of
Parameters}\label{interpretation-of-parameters}}

When we assume that the error in the response, on average, is 0 for all
values of the predictor, we are really saying that the deterministic
portion of the model defines the mean response. We see this in the
alternate characterization of the regression model above where \(\mu\)
in the Gaussian (Normal) model is replaced by

\[\beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i.\]

Notice what happens if we plug zero in for \emph{every} predictor:

\[\beta_0 + \sum_{j=1}^{p} \beta_j (0) = \beta_0.\]

Since this deterministic portion specifies the average response, then we
see that the average response is \(\beta_0\) when all predictors have
the value zero.

\begin{definition}[Intercept]\protect\hypertarget{def-intercept}{}\label{def-intercept}

The population intercept, denoted \(\beta_0\), is the \emph{mean}
response when all predictors take the value zero.

\end{definition}

We should point out that while this is the correct interpretation, it
may not always make sense in context. For example, if we are modeling
the heart rate of patients as a function of their body temperature and
weight; the model would have the form

\[(\text{Heart Rate})_i = \beta_0 + \beta_1 (\text{Body Temperature})_i + \beta_2 (\text{Weight})_i + \varepsilon_i.\]

Based on Definition~\ref{def-intercept}, we would interpret the
intercept in this model as the average heart rate for individuals with a
body temperature of zero degrees and a weight of zero pounds; as this
group of individuals does not exist, the interpretation does not make
sense in this context.

We now turn to considering an interpretation for the slope. Consider two
groups of individuals:

\begin{itemize}
\tightlist
\item
  Group 1 has the value \(a\) for the first predictor and value \(x_j\)
  for Predictor \(j\) (for \(j = 2, \dotsc, p\)).
\item
  Group 2 has the value \(a + 1\) for the first predictor and value
  \(x_j\) for Predictor \(j\) for \(j = 2, \dotsc, p\).
\end{itemize}

That is, the only way the two groups differ is that Group 2 has
increased the value of the first predictor by 1. From our model, we have
that the average response for Group 1 is

\[\beta_0 + \beta_1 a + \sum_{j=2}^{p} \beta_j x_j.\]

The average response for Group 2 is

\[\beta_0 + \beta_1 (a + 1) \sum_{j=2}^{p} \beta_j x_j.\]

Consider taking the difference in these two \emph{mean} responses (Group
2 minus Group 1):

\[\beta_0 + \beta_1 (a + 1) \sum_{j=2}^{p} \beta_j x_j - \left(\beta_0 + \beta_1 a + \sum_{j=2}^{p} \beta_j x_j\right) = \beta_1.\]

That is, the slope is the difference in the \emph{mean} response between
the two groups.

\begin{definition}[Slope]\protect\hypertarget{def-slope}{}\label{def-slope}

The coefficient for the \(j\)-th predictor, denoted \(\beta_j\), is the
change in the mean response associated with a one unit increase in
Predictor \(j\), \emph{holding all other predictors fixed}.

\end{definition}

The last part of Definition~\ref{def-slope} is a critical part of the
interpretation, and it is critical to the full utility of regression
models. Again, while holding all other predictors fixed may not be
practically feasible (for example, could we really increase an
individual's height without also increasing their weight), it allows us
to investigate the impact of a predictor separate from other variables.

Interpretation of the parameters is a large step beyond simply
estimating the parameters. However, we still have not developed the
tools to do much beyond estimation. We now turn our attention to
inference.

\hypertarget{inference-about-the-mean-parameters}{%
\section{Inference About the Mean
Parameters}\label{inference-about-the-mean-parameters}}

As suggested in Chapter~\ref{sec-distributional-quartet}, the key to
making formal inference on the parameters of a population is to develop
a model for the sampling distribution (or null distribution) of the
corresponding statistics. Under the classical regression conditions of
Definition~\ref{def-classical-regression}, we are able to form an exact
model for the sampling distribution of the least squares estimates.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While beyond the scope of this course, it can be shown that the least
squares estimates of the parameters are linear combinations of the
observed responses. This, combined with the modeling assumptions, allows
us to construct a model for the sampling distribution of the estimates.

\end{tcolorbox}

\begin{definition}[Sampling Distribution of the Least Squares
Estimates]\protect\hypertarget{def-ls-sampling-distribution}{}\label{def-ls-sampling-distribution}

Under the classical regression conditions
(Definition~\ref{def-classical-regression}), we have that

\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim t_{n - p - 1}.\]

The denominator \(\sqrt{Var\left(\widehat{\beta}_j\right)}\) is known as
the \emph{standard error} of the estimate \(\widehat{\beta}_j\). This
formula holds for all \(j = 0, 1, \dotsc, p\).

\end{definition}

Definition~\ref{def-ls-sampling-distribution} states the standardized
difference between our estimate and the parameter follows a
t-distribution, where the degrees of freedom depend on the sample size
and the number of parameters in the model. The specific model is not as
important as knowing that under the classical regression conditions, an
exact model is known. Nearly every software package that implements
regression does so under the classical regression conditions, and the
inference is based on the above model for the sampling distribution.

The detail-oriented reader will note that we did not include a formula
for the standard error of an estimate. The formula is beyond the scope
of this course, but it is a function of the values of the predictor as
well as the variability in the error term. You see, the moment we
specified the second condition (``constant variance''), we introduced
another parameter: \(\sigma^2\). The parameter \(\sigma^2\) does not
govern the mean response; so, it tends to be of less direct interest for
our purposes. Instead, it characterizes the variability in the response
(for a given set of predictors), and it plays a role in inference (as we
see in the above model for the sampling distribution of the least
squares estimates of the parameters in the mean model). It will
therefore play a role in computing confidence intervals and p-values.
Since it is unknown, it must also be estimated.

\begin{definition}[Estimate of the Variance of the
Errors]\protect\hypertarget{def-estimate-sigma2}{}\label{def-estimate-sigma2}

The unknown variance in the linear model, which captures the variability
in the response for any set of predictors (also called the residual
variance), is estimated by

\[\widehat{\sigma}^2 = \frac{1}{n-p-1} \sum\limits_{i=1}^{n} \left((\text{Response})_i - \widehat{\beta}_0 - \sum\limits_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_{i}\right)^2.\]

\end{definition}

Note that the estimate of the variance depends upon the least squares
estimates. Of more interest is that the scaling factor \((n - p - 1)\)
is the same as the degrees of freedom for the sampling distribution;
that is not an accident.

A model for the sampling distribution is the holy grail of statistical
inference. It can be updated to determine the model for the null
distribution. And, once you have a model for the sampling distribution
in hand, you can wield it to construct a confidence interval (and null
distributions to yield p-values).

\begin{definition}[Confidence Interval for Parameters Under Classical
Model]\protect\hypertarget{def-classical-ci}{}\label{def-classical-ci}

Under the classical regression conditions
(Definition~\ref{def-classical-regression}), a \(100c\)\% confidence
interval for the parameter \(\beta_j\) is given by

\[\widehat{\beta}_j \pm t_{n-p-1, 0.5(1+c)} \sqrt{Var\left(\widehat{\beta}_j\right)}.\]

where \(t_{n-p-1, 0.5(1+c)}\) is the \(0.5(1+c)\) quantile from the
\(t_{n-p-1}\) distribution, known as the critical value for the
confidence interval.

\end{definition}

Like many confidence intervals, the idea is that we are grabbing the
middle portion of the model for the sampling distribution. The
confidence interval represents the values of the parameter for which the
data is consistent --- the reasonable values of the parameter based on
the observed data. Also note that this confidence interval is specified
for each parameter individually.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

For large values of \(n\) relative to \(p\), the critical value for a
95\% confidence interval is approximately 1.96. Hence, a rough
confidence interval is therefore 2 standard errors in either direction
of the point estimate.

\end{tcolorbox}

\begin{definition}[P-Value for Testing if Parameter Belongs in Model
Under Classical
Model]\protect\hypertarget{def-classical-p}{}\label{def-classical-p}

Under the classical regression conditions
(Definition~\ref{def-classical-regression}), the p-value for testing the
hypotheses

\[H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0\]

is given by

\[Pr\left(\lvert T\rvert > \lvert\frac{\widehat{\beta}_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}}\rvert\right)\]

where \(T \sim t_{n-p-1}\).

\end{definition}

Definition~\ref{def-classical-p} highlights that the null distribution
for the standardized ratio is developed by taking the model for the
sampling distribution and enforcing the null hypothesis
\(\left(\beta_j = 0\right)\). Using this null distribution, our p-value
then summarizes how likely it is we would obtain a value of the
standardized statistic at least as large of that observed by chance
alone when the null hypothesis is true.

The interpretation of the confidence interval and p-value follows the
interpretation of the confidence intervals and p-values computed in an
introductory course (and reviewed in
Chapter~\ref{sec-statistical-process}. This section just establishes
that the conditions we placed on the error term yield explicit formulas
for their computation (even if these formulas are implemented in the
background of the software).

The framework introduced here provides the basics for making inference
using a statistical model. As we consider more flexible modeling
strategies, these key concepts do not leave us. We need a model for the
sampling distribution or null distribution in order to make inference.
And, the model for that distribution depends on the conditions we are
willing to make.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

A model for the sampling distribution (and/or null distribution) is
needed for making inference, and that model depends on the conditions we
are willing to impose on the model for the data generating process.

\end{tcolorbox}

\hypertarget{sec-glm-assessing-conditions}{%
\chapter{Assessing the Conditions for the General Linear
Model}\label{sec-glm-assessing-conditions}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

In the previous chapter, we presented a model for the sampling
distribution (Definition~\ref{def-ls-sampling-distribution}) of the
least squares estimates. This model allows us to make inference on the
unknown parameters that govern the mean response of the general linear
model (Definition~\ref{def-general-linear-model}). However, our model
for the sampling distribution presented assumes all the conditions for
the classical regression model
(Definition~\ref{def-classical-regression}) hold. We should not blindly
make assumptions. Instead, we should ensure our data is consistent with
any conditions we impose.

The majority of the conditions in the classical regression model are
placed on the error term, a random variable that we never observe in
practice. This means that the conditions cannot be assessed using the
errors directly. Instead, modeling conditions are assessed graphically
using residuals.

\begin{definition}[Residual]\protect\hypertarget{def-residual}{}\label{def-residual}

A residual for the \(i\)-th observation is the difference between an
observed value and the predicted response:

\[
\begin{aligned}
  (\text{Residual})_i 
    &= (\text{Observed Response})_i - (\text{Predicted Response})_i \\
    &= (\text{Response})_i - \left(\widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i\right).
\end{aligned}
\]

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

If a condition holds on the distribution of the errors, we expect the
residuals to adopt specific behavior. Therefore, while the conditions
are placed on the error, they are \emph{assessed} using the residuals.
The conditions, however, are \emph{not} about the residuals.

\end{tcolorbox}

It is important to assess the conditions we place on the model as they
determine the model for the sampling distribution (and null
distribution). That is, if the conditions we have assumed are incorrect,
our p-values and confidence intervals will be invalid.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

The assumptions we are willing to make about a data generating process
(in particular, the random component of our regression model) determine
the form of the model for the sampling distributions (null
distributions) of the resulting estimates (standardized statistics).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

You will notice we often jump between ``conditions'' and ``assumptions''
as if they are interchangeable. They are often used synonymously in the
literature. However, as a distinction, \emph{conditions} are the
mathematical properties that must be met in order to justify the
statistical theory; in practice, we make \emph{assumptions} about which
conditions we believe are reasonable.

We can never prove a condition holds; therefore, we must always make
assumptions.

\end{tcolorbox}

As a general rule, if our data is consistent with the conditions we have
assumed, then the error term is just noise; that is, it should not have
any signal left. We would therefore expect the residuals to lack any
patterns; if we find patterns in the residuals, it suggests there is
some structure our model has ignored. While there are many methods
available for detecting patterns in the residuals, we prefer a graphical
approach. Since we cannot verify a condition holds, we will always be
making assumptions. By taking a graphical approach to assessment, we are
embracing the subjective nature of such investigations; other approaches
can give the appearance that there is more certainty in the conclusion
than actually exists.

Table~\ref{tbl-glm-assessing-conditions-residual-plots} aligns the
conditions we place on the model with the graphic used for assessment.

\hypertarget{tbl-glm-assessing-conditions-residual-plots}{}
\begin{table}
\caption{\label{tbl-glm-assessing-conditions-residual-plots}Method of graphical assessment for the conditions of the classical
regression model. }\tabularnewline

\centering
\begin{tabular}[t]{ll}
\toprule
Condition & Graphical Assessment\\
\midrule
\cellcolor{gray!6}{Error is 0, on average, for all predictors} & \cellcolor{gray!6}{Residual vs. Predicted Values}\\
Errors are independent & Time-series Plot of Residuals\\
\cellcolor{gray!6}{Homoskedasticity} & \cellcolor{gray!6}{Residual vs. Predicted Values}\\
Errors are Normally distributed & Probability Plot of Residuals\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{}\\
\addlinespace
No Measurement Error & None (discipline expertise)\\
\cellcolor{gray!6}{Predictor enters linearly} & \cellcolor{gray!6}{Residual vs. Predictor}\\
\bottomrule
\end{tabular}
\end{table}

The conditions on the error are assessed in the same way they are in
simple linear regression, covered in an introductory course. So, we only
briefly review them here.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the Plot of the Residuals vs.~Predicted Values}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

This plot is used to assess both the ``mean 0'' and ``constant
variance'' conditions. We are looking for trends in the \emph{location}
and in the \emph{spread}, respectively, to assess these conditions.

\end{tcolorbox}

If we observe a trend in the \emph{location} of the residuals as we move
left-to-right on the graphic, we have evidence that the ``mean 0''
condition is violated. If we observe a trend in the \emph{spread} of the
residuals as we move left-to-right across the graphic, we have evidence
that the ``constant variance'' condition is violated. As we move
forward, we will examine techniques to relax the condition of constant
variance. While there is no ``fix'' for violations of the ``mean 0''
condition, we will study scenarios for which the response and predictors
are not linearly related. What is amazing about this graphic is that we
have reduced a multi-dimensional problem to two dimensions.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the Time-Series Plot of the Residuals}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

This graph plots the residuals against the order in which the data was
collected. Any trends in the residuals (in location \emph{or} spread)
indicates evidence the errors collected close together in time may be
associated in some way, violating the assumption of independence.

\end{tcolorbox}

The creation of a time-series plot is only reasonable/useful if we know
the order in which the data was collected. A cross-sectional analysis
(single snapshot in time), for example has no natural ordering. It is
important to note that a time-series plot only allows us to examine
dependence as a result of time. If the errors are correlated due to some
other factor (for example, geographical location or family groups), the
violation may not be detected. When we are able to identify the
dependence structure, we can incorporate it into the model, as discussed
in later units (Chapter~\ref{sec-rm-terminology}). Regardless of whether
the graphic can be constructed, a thorough assessment of independence
always requires discipline expertise and a critical review of the data
collection plan.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the Probability Plot of the Residuals}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Also called a ``QQ Plot,'' a probability plot examines the relationship
between the observed residuals and the expected (or ``theoretical'')
values we would expect if the errors followed a Normal distribution. Any
departures from a straight line indicate evidence the errors do not
follow a Normal distribution.

\end{tcolorbox}

The assumption of Normality is the strongest of the four conditions;
that is, this condition goes beyond characterizing an aspect of the
error distribution to specifying the functional family to which the
distribution belongs. As we will see, this condition is the easiest to
relax.

Recall that in addition to conditions on the stochastic portion of the
model, we have considered conditions on the predictors as well
(Definition~\ref{def-classical-regression-cont}).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Assessing Measurement Error in the Predictors}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Determining whether a predictor is subject to measurement error relies
on discipline expertise.

\end{tcolorbox}

If the predictors are measured with (non-negligible) error, our
estimates are biased and therefore unreliable. Methods for addressing
predictors that are subject to measurement error are beyond the scope of
the text.

Requiring that the predictors enter the model linearly is a refinement
of the ``mean 0'' condition; that is, one way that we often misspecify
the deterministic portion of the model is to attempt to model curvature
in the data using a line. It should not be a surprise then that
assessing the linearity of the predictors is similar to assessing the
``mean 0'' condition.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Assessing Linearity of the Predictors}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

If the relationship between the response and predictor is adequately
explained by a linear relationship, then there should not be any
structure in the \emph{location} of the residuals when examined against
the predictor.

\end{tcolorbox}

That is, when assessing the ``mean 0'' condition, we examine the
residuals against the predicted values; when assessing the linearity
condition, we examine the residuals against each \emph{quantitative}
predictor.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

We will discuss the incorporation of categorical predictors in the next
chapter; however, we note that the linearity assumption only applies to
quantitative predictors.

\end{tcolorbox}

The data will not always be consistent with the conditions we would like
to place on the model. Proceeding as if the conditions are reasonable
when they are not can lead to invalid inference and incorrect
conclusions. Discarding the results misses out on potential insights the
data offers. Fortunately, the modeling framework is flexible enough to
be relaxed to address violations of the conditions, which we examine
toward the end of this unit in the text.

\hypertarget{sec-glm-unifying-framework}{%
\chapter{The General Linear Model as a Unifying
Framework}\label{sec-glm-unifying-framework}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The general linear model (Definition~\ref{def-general-linear-model}) is
much more flexible and powerful than we might initially imagine. The
full flexibility of this modeling framework is explored in the next
unit. It should be clear that simple linear regression is a special case
of the general linear model for which we only have a single predictor.
In the remainder of this chapter, we outline how the methods typically
studied in an introductory course also relate to the general linear
model framework.

\hypertarget{one-sample-inference}{%
\section{One Sample Inference}\label{one-sample-inference}}

Perhaps the most cited analysis technique from an introductory
statistics course is the ``1-sample t-test.'' In brief, this test
considers the hypotheses

\[H_0: \mu = \mu_0 \qquad \text{vs.} \qquad H_1: \mu \neq \mu_0\]

where \(\mu\) is the average response in the population. These
hypotheses are making inference on the mean response of a single
population (or ``one sample''). A classical introductory statistics
course will introduce the test statistic

\[T^* = \frac{\sqrt{n} \left(\bar{y} - \mu_0\right)}{s}\]

where \(\bar{y}\) and \(s\) represent the sample mean and sample
standard deviation, respectively, of the response. The one-sample t-test
proceeds to model the (null) distribution of \(T^*\) as a t-distribution
with \(n - 1\) degrees of freedom. This test assumes that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The response for one observation is independent of the response for
  all other observations.
\item
  The responses are identically distributed.
\item
  The responses follow a Normal distribution.
\end{enumerate}

We can recover the same analysis using the general linear model.
Consider the model

\[(\text{Response})_i = \mu + \varepsilon_i.\]

This is sometimes referred to as the ``intercept-only'' model. It can be
shown that the least-squares estimate of \(\mu\) is the sample mean of
the response. Similarly, our estimate of \(\sigma^2\) is the sample
variance. Further, we have that

\[Var\left(\widehat{\mu}\right) = \frac{s^2}{n},\]

meaning the standardized statistic in
Definition~\ref{def-ls-sampling-distribution} is equivalent to the ratio
taught in the introductory statistics course.

Having the same standardized statistic is one thing, but the analysis is
only equivalent if the conditions imposed also agree. If you consider
the classical regression conditions
(Definition~\ref{def-classical-regression}), then given that there is no
predictor in the model, the conditions would translate to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The response for one observation is independent of the response for
  all other observations.
\item
  The responses are identically distributed.
\item
  The responses follow a Normal distribution.
\end{enumerate}

You may wonder where the ``mean 0'' condition went. Assuming the error
is 0 on average is equivalent to assuming the deterministic portion of
the model is correctly specified. In the case when the deterministic
model does not have a predictor, we are essentially assuming that
\(\mu\) is the average (the sample is representative of the underlying
population). In particular, since we made no simplifying assumptions
about the structure of the relationship between the response and
predictor (since there is no predictor), those simplifying assumptions
cannot be incorrect.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

A one-sample t-test is equivalent to running an intercept-only model in
the general linear model framework under the classical conditions.

\end{tcolorbox}

\hypertarget{paired-t-test}{%
\section{Paired t-Test}\label{paired-t-test}}

Most analyses covered in an introductory class assume independence
between all observations. The most notable exception is the ``paired
t-test.'' In this scenario, we collect a total of \(2n\) observations,
but for a total of \(n\) independent pairs. For example, \(n\)
participants complete a pre and post test. Or, we measure a response on
both the left and right eye for \(n\) participants. In each of these
examples, there are two measurements for each of the \(n\) participants
that we believe are related (general ability with exams means some
students will naturally score higher; genetics result in some
individuals having better vision; etc.). The typical way of addressing
this problem in the introductory course is to take the difference in the
response within each pair, and then conduct a one-sample t-test. That
is, we consider

\[T^* = \frac{\sqrt{n}\left(\bar{y}_d - \mu_{d,0}\right)}{s_d},\]

where \(\bar{y}_d\) is the sample mean of the differences in the
response, \(s_d\) is the sample standard deviation of the differences,
and \(n\) is the number of paired observations.

Since this is a one sample test, building on our discussion earlier in
this chapter, we have that an equivalent approach is to consider the
model

\[(\text{Difference in Responses})_i = \mu + \varepsilon_i\]

with the classical conditions imposed.

The paired t-test is actually a special case of a ``repeated measures
ANOVA,'' which can also be viewed as a special case of the general
linear model. We discuss repeated measures and appropriate approaches
for analysis in a future unit.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

A paired t-test is equivalent to running an intercept-only model, with
the pairwise differences as the response, in the general linear model
framework under the classical conditions.

\end{tcolorbox}

\hypertarget{group-comparisons}{%
\section{Group Comparisons}\label{group-comparisons}}

Suppose we are interested in comparing the average response across two
or more groups. If there are only two groups, our hypotheses take the
form

\[H_0: \mu_1 = \mu_2 \qquad \text{vs.} \qquad H_1: \mu_1 \neq \mu_2,\]

where \(\mu_1\) and \(\mu_2\) represent the average response for each of
the two groups. The two-sample t-test is typically discussed to address
this question where

\[T^* = \frac{\left(\bar{y}_2 - \bar{y}_1\right)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}},\]

where \(\bar{y}_j\) and \(s^2_j\) are the sample mean and sample
variance of the response within group \(j\) (\(j = 1, 2\)),
respectively. A t-distribution is used to model the distribution of this
standardized statistic under the following conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The response from one observation is independent of the response of
  all other observations (implying independence both within and between
  groups).
\item
  The responses \emph{within} a group are identically distributed.
\item
  The responses within each group follow a Normal distribution.
\end{enumerate}

When there are three or more groups, our hypotheses take the form

\[H_0: \mu_1 = \mu_2 = \dotsb = \mu_k \qquad \text{vs.} \qquad H_1: \text{at least 1 } \mu_j \text{ differs},\]

where \(\mu_j\) represents the average response for group \(j\),
\(j = 1, 2, \dotsc, k\). Analysis of variance (ANOVA) is typically used
to address this question. Typically, ANOVA imposes the following
conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The response from one observation is independent of the response of
  all other observations (implying independence both within and between
  groups).
\item
  The variability in the response within a group is the same for each
  group.
\item
  The responses within each group follow a Normal distribution.
\end{enumerate}

It would seem that the two-group comparison is a special case of ANOVA;
however, as stated above, they would yield slightly different inference
because the conditions imposed differ. Specifically, ANOVA assumes the
variance of the response in a group is the same across groups; however,
the two-sample t-test allows the variance of the response to differ
across groups.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

There is a version of the two-sample t-test which uses the ``pooled''
sample variance; in that case, the variance of the response is assumed
to be the same within each group and the two-sample t-test is a special
case of ANOVA.

\end{tcolorbox}

It would therefore seem that there is no way these methods relate to the
general linear model framework. However, the general linear model
framework does encompass both approaches. An ANOVA can be accomplished
by inserting a single categorical predictor into the model capturing the
grouping structure (see
Chapter~\ref{sec-modeling-categorical-predictors}). Since the classical
regression assumptions correspond to ANOVA, we recover the same
inference. The two-group comparison requires that we additionally relax
the constant variance condition, which we discuss in
\textbf{?@sec-nlm-heteroskedasticity} (while discussed in the context of
non-linear models, the same techniques apply to the general linear model
as well).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

ANOVA is equivalent to running the general linear model with a single
categorical predictor under the classical conditions.

\end{tcolorbox}

\part{Unit III: General Modeling Techniques}

In the previous unit, we introduced the general linear model. In this
unit, we use the general linear model as the backdrop for introducing
several modeling strategies that allow us to address common questions in
scientific research. While these strategies are introduced in the
context of the linear model, they can be applied with all the models
discussed in the text.

\hypertarget{sec-modeling-related-predictors}{%
\chapter{Addressing Relationships Between
Predictors}\label{sec-modeling-related-predictors}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Chapter~\ref{sec-glm-model-framework} introduced the framework of the
general linear model, including the interpretation of the parameters
(Definition~\ref{def-slope} and Definition~\ref{def-intercept}). While
the mathematical structure of the general linear model may be
fascinating to some, we are particularly interested in the model's
utility to address scientific questions. Therefore, for our purposes,
the interpretation of the parameters is of utmost importance. And,
immediately from the interpretation of the slope coefficients, the
linear model framework allows to isolate the effect of one variable
while holding all other variables constant. This has implications on the
conclusions we can draw from the model.

\hypertarget{adjusting-for-confounders}{%
\section{Adjusting for Confounders}\label{adjusting-for-confounders}}

Scientific studies can be roughly categorized as being an observational
study (Definition~\ref{def-observational-study}) or a controlled
experiment (also called a randomized clinical trial,
Definition~\ref{def-randomized-clinical-trial}). When our scientific
question centers on the relationship between a response and a predictor,
and the values of the predictor have not been randomly assigned to
subjects, the (observational) study is subject to confounding
(Definition~\ref{def-confounding}). It is the potential for confounding
that leads to the often cited ``correlation does not imply causation.''
It does not take much to see that this is extremely limiting. We can
imagine randomizing subjects to different levels of a categorical
predictor, but randomizing subjects to a quantitative predictor would
require very large sample sizes. For example, imagine randomly
allocating subjects to the amount of water they consume in their diet
each day --- think of all the amounts of water you might want to
consider. Further, it would rule out ever being able to causally link a
response with a quantitative predictor which represents an inherit
characteristic. For example, randomly allocating a participant to a
specific height would require adding or removing bone mass to alter
their height --- which we hope goes without saying is unethical and
unacceptable (not to mention just disturbing).

Multivariable models, however, naturally address confounding because of
the interpretation of the coefficients: \emph{holding all other
predictors fixed} (we said this would be a crucial phrase). This
interpretation suggests the coefficient attached to a predictor is
quantifying the effect of the predictor on the response, isolated from
all other predictors in the model. By isolating the predictor's effect,
we are able to see its impact on the response beyond the impact of any
other predictors. We often say that the model is ``adjusted'' for the
other predictors.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Adjusted Models}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

An ``adjusted'' model just means that we constructed a multivariable
model. The relationship between the response and the predictor of
interest is ``adjusted'' by the other predictors that appear in the
model.

\end{tcolorbox}

Think again about the importance of random allocation in allowing for
causal interpretations in controlled experiments. The random allocation
of subjects to groups means that the groups are similar \emph{with
respect to all other variables} --- the only difference between the
groups is the treatment received. The ``holding all other predictors
fixed'' phrase accomplishes something similar.\\
We know that the deterministic portion of the general linear model
characterizes the mean response. That is, the deterministic portion
tells us the average value of the response we should expect among the
\emph{group} of individuals with a particular value of the predictors.
Therefore, when we increase a predictor by one unit \emph{holding all
other predictors fixed}, we are creating two groups where the only
difference is that increase of one unit. The two groups are similar
\emph{with respect to all other predictors in the model}.

This seems almost too good to be true. By simply expanding our model to
incorporate other predictors, we have addressed the issue of
confounding, and we have gotten back a causal interpretation of the
impact of the predictor on the response. But, it is important to note
the differences between a controlled experiment and the interpretation
provided by a multivariable model:

\begin{itemize}
\tightlist
\item
  Controlled experiment: the groups are similar \emph{with respect to
  all other variables}.
\item
  Multivariable model: the groups are similar \emph{with respect to all
  other predictors}.
\end{itemize}

The difference in the language is subtle but important. We can only
adjust for predictors that we observe and put in the model. That is, a
multivariable model does not automatically solve all confounding issues;
it ensures that any potential confounding cannot be the result of the
other predictors in the model. This allows us to make causal conclusions
if we can assume that all potential confounders are present as other
predictors in the model.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

A multivariable model allows us to isolate the effect of one variable
from the other predictors; this allows us to state that those other
predictors are not contributing to any potential confounding between the
variable of interest and the response.

\end{tcolorbox}

\hypertarget{multicollinearity}{%
\section{Multicollinearity}\label{multicollinearity}}

A confounder masks or exaggerates the effect of one predictor on the
response. In order for confounding to exist, the confounder must be
related to both the response and the predictor. A distinctly unique, but
often confused topic, is that of multicollinearity.

\begin{definition}[Multicollinearity]\protect\hypertarget{def-multicollinearity}{}\label{def-multicollinearity}

When two predictors are highly correlated with one another, we say that
there is multicollinearity in the model.

\end{definition}

To understand the impact, we consider a very simple hypothetical
example. Suppose we are interested in modeling the number of steps taken
over the course of a typical day (as recorded by popular fitness
trackers) using the participant's height and stride length. Our model
would have the form

\[(\text{Number of Steps})_i = \beta_0 + \beta_1(\text{Height})_i + \beta_2(\text{Stride Length})_i + \varepsilon_i.\]

Suppose that we test the following two sets of hypotheses:

\[
\begin{aligned}
  H_0:& \beta_1 = 0 \qquad \text{vs.} \qquad H_1: \beta_1 \neq 0 \\
  H_0:& \beta_2 = 0 \qquad \text{vs.} \qquad H_1: \beta_2 \neq 0.
\end{aligned}
\]

Further, suppose that we have a large p-value for each of these tests.

How would we interpret the large p-value for the first test? It would
tell us that there is no evidence of a relationship between the average
number of steps taken and the participant's height, \emph{holding their
stride length fixed}. The idea of holding all other predictors fixed
plays an important part here. The large p-value for this first
hypothesis tells us that there is no evidence the participant's height
is helpful for predicting the number of steps taken \emph{after
accounting for their stride length}. Similarly, the large p-value for
the second hypothesis tells us that there is no evidence the
participant's stride length is helpful for predicting the number of
steps taken \emph{after accounting for their height}. However, we know
that a person's stride is very correlated with their height --- those
who are taller have longer legs and tend to have a longer stride. That
is, there is not much information that a person's stride length will
tell us that their height does not already convey; that explains the
results we are seeing here. It is not that the height is not associated
with the number of steps taken; it is that it is not helpful \emph{above
and beyond} knowing the participant's stride length. This is
multicollinearity.

Multicollinearity can make inference on a single predictor misleading.
The p-value is not incorrect; we just need to remember that it is
testing whether the term belongs after accounting for other predictors
in the model. That is, the regression model is trying to isolate the
effect above and beyond that of other predictors in the model.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

A tell-tale sign of multicollinearity between two predictors is that
alone, each is significantly associated with the response; but, when
both are placed in the model, neither appears significant.

\end{tcolorbox}

Multicollinearity is not necessarily a problem. If our primary aim in
constructing the regression model is prediction, then we are not
concerned with the inference on each parameter individually. The
estimates of the parameters are valid; as a result, we can leave both
predictors in the model. However, if our goal is inference on the
parameters to further characterize the relationship, then the standard
errors are misleading (leading to unreliable p-values and confidence
intervals). The solution is to remove the predictor that is less likely
to be on the ``causal pathway'' which requires discipline expertise.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

When two predictors capture the same information, placing both in the
model can lead to misleading p-values and confidence intervals for each
predictor. However, if the primary aim is prediction, this is not
generally a concern.

\end{tcolorbox}

\hypertarget{sec-modeling-categorical-predictors}{%
\chapter{Incorporating Categorical
Predictors}\label{sec-modeling-categorical-predictors}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The general linear model framework
(Definition~\ref{def-general-linear-model}) is quite flexible; in
particular, it allows us to consider not only quantitative predictors,
but also categorical (qualitative) predictors. Our strategy is to define
a new set of quantitative variables which capture the group membership
appropriately.

:::\{\#exm-perceived-stress\} \#\# Perceived Stress The \emph{Perceived
Stress Scale (PSS)} is a widely used psychological instrument for
measuring the perception of stress. Subjects answer ten short questions
regarding the degree to which situations in their life are viewed as
stressful, and the responses are codified into a score between 0 and 40
(higher values indicate higher stress). Suppose we were interested in
modeling the PSS score among college students as a function of their
class standing (Freshman, Sophomore, Junior, Senior) and the number of
hours of sleep the student reports getting on a typical night. The first
few records in our data might hypothetically look like that illustrated
in Table~\ref{tbl-modeling-categorical-predictors-stress-data}.

\hypertarget{tbl-modeling-categorical-predictors-stress-data}{}
\begin{table}
\caption{\label{tbl-modeling-categorical-predictors-stress-data}Hypothetical data on stress in college students. }\tabularnewline

\centering
\begin{tabular}[t]{rrrl}
\toprule
Subject ID & PSS & Hours Sleep & Class Standing\\
\midrule
\cellcolor{gray!6}{1415} & \cellcolor{gray!6}{14} & \cellcolor{gray!6}{7.5} & \cellcolor{gray!6}{Freshman}\\
1463 & 25 & 8.5 & Senior\\
\cellcolor{gray!6}{1179} & \cellcolor{gray!6}{26} & \cellcolor{gray!6}{7.0} & \cellcolor{gray!6}{Junior}\\
1526 & 27 & 8.5 & Senior\\
\cellcolor{gray!6}{1195} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{8.0} & \cellcolor{gray!6}{Sophomore}\\
\addlinespace
1938 & 27 & 5.0 & Freshman\\
\cellcolor{gray!6}{1818} & \cellcolor{gray!6}{28} & \cellcolor{gray!6}{5.5} & \cellcolor{gray!6}{Junior}\\
1118 & 9 & 4.0 & Freshman\\
\cellcolor{gray!6}{1299} & \cellcolor{gray!6}{29} & \cellcolor{gray!6}{9.0} & \cellcolor{gray!6}{Freshman}\\
1229 & 35 & 7.0 & Sophomore\\
\bottomrule
\end{tabular}
\end{table}

It does not take long to recognize that forming a model like

\[(\text{PSS Score})_i = \beta_0 + \beta_1 (\text{Hours Sleep})_i + \beta_2 (\text{Class Standing})_i + \varepsilon_i\]

does not work. Since class standing is a categorical variable, plugging
in does not make sense; that is, what does it mean to multiply
\(\beta_2\) by ``Junior''? We need a way of somehow bringing the
categorical predictor into the linear model. Before stating our
approach, let's first consider two common naive approaches:

\begin{itemize}
\tightlist
\item
  Replace each level of the categorical predictor with a number: convert
  Freshman to 1, Sophomore to 2, Junior to 3, and Senior to 4; enter
  this numeric variable into a regression model.
\item
  Construct different data sets for each level of the categorical
  predictor: four data sets in this case with one for Freshman, one for
  Sophomore, one for Junior, and one for Senior; conduct a different
  analysis on each set of data.
\end{itemize}

The first approach solves the ``number times word'' problem. We could
certainly fit such a model. However, this approach is limiting. It
assumes a linear trend across the levels of the categorical predictor.
Are we sure that the stress either increases or decreases as the class
standing increases? Do we want to allow the stress to be highest during
the sophomore year? More problematic are categorical predictors which
have no natural ordering (e.g., eye color); how do we determine the
mapping from text to numbers in that case?

The second approach sounds reasonable at first glance. It would yield
four different models:

\[
\begin{aligned}
  \text{Model 1}:& (\text{PSS Score})_i = \gamma_{\text{FR}} + 
    \alpha_{\text{FR}} (\text{Hours Sleep})_i + \varepsilon_{1,i} \\
  \text{Model 2}:& (\text{PSS Score})_i = \gamma_{\text{SO}} + 
    \alpha_{\text{SO}} (\text{Hours Sleep})_i + \varepsilon_{2,i} \\
  \text{Model 3}:& (\text{PSS Score})_i = \gamma_{\text{JR}} + 
    \alpha_{\text{JR}} (\text{Hours Sleep})_i + \varepsilon_{3,i} \\
  \text{Model 4}:& (\text{PSS Score})_i = \gamma_{\text{SR}} + 
    \alpha_{\text{SR}} (\text{Hours Sleep})_i + \varepsilon_{4,i} \\.
\end{aligned}
\]

In these models, we have different parameters for each group. The
problem is that we no longer have a single estimate for the impact of
the number of hours of sleep; we have a different estimate for each
group. Further, we would have a different estimate of the residual
variance for each model, which would not align with the condition of
assuming the variance is constant for all values of the predictors. This
approach diminishes the power of the study, and it does not make it easy
to address some questions of interest (such as, do freshman and
sophomores differ in their PSS score?).

Neither of these approaches seems to fully capture our goal. Instead, we
create multiple new variables that capture the qualitative grouping.
Consider defining new variables as follows

\[
\begin{aligned}
  (\text{Sophomore})_i &= \begin{cases}
    1 & \text{if i-th subject is a sophomore} \\
    0 & \text{otherwise}
    \end{cases} \\
  (\text{Junior})_i &= \begin{cases}
    1 & \text{if i-th subject is a junior} \\
    0 & \text{otherwise}
    \end{cases} \\
  (\text{Senior})_i &= \begin{cases}
    1 & \text{if i-th subject is a senior} \\
    0 & \text{otherwise}
    \end{cases}.
\end{aligned}
\]

Augmenting our original data set with these new predictors would result
in the data set illustrated in
Table~\ref{tbl-modeling-categorical-predictors-stress-data-aug}.

\hypertarget{tbl-modeling-categorical-predictors-stress-data-aug}{}
\begin{table}
\caption{\label{tbl-modeling-categorical-predictors-stress-data-aug}Hypothetical data on stress in college students augmented to include
additional variables capturing the class standing. }\tabularnewline

\centering
\begin{tabular}[t]{rrrlrrr}
\toprule
Subject ID & PSS & Hours Sleep & Class Standing & Sophomore & Junior & Senior\\
\midrule
\cellcolor{gray!6}{1415} & \cellcolor{gray!6}{14} & \cellcolor{gray!6}{7.5} & \cellcolor{gray!6}{Freshman} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
1463 & 25 & 8.5 & Senior & 0 & 0 & 1\\
\cellcolor{gray!6}{1179} & \cellcolor{gray!6}{26} & \cellcolor{gray!6}{7.0} & \cellcolor{gray!6}{Junior} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0}\\
1526 & 27 & 8.5 & Senior & 0 & 0 & 1\\
\cellcolor{gray!6}{1195} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{8.0} & \cellcolor{gray!6}{Sophomore} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
\addlinespace
1938 & 27 & 5.0 & Freshman & 0 & 0 & 0\\
\cellcolor{gray!6}{1818} & \cellcolor{gray!6}{28} & \cellcolor{gray!6}{5.5} & \cellcolor{gray!6}{Junior} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0}\\
1118 & 9 & 4.0 & Freshman & 0 & 0 & 0\\
\cellcolor{gray!6}{1299} & \cellcolor{gray!6}{29} & \cellcolor{gray!6}{9.0} & \cellcolor{gray!6}{Freshman} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
1229 & 35 & 7.0 & Sophomore & 1 & 0 & 0\\
\bottomrule
\end{tabular}
\end{table}

Using these additional variables, consider the model

\[
\begin{aligned}
  (\text{PSS Score})_i &= \beta_0 + \beta_1 (\text{Hours Sleep})_i + \beta_2 (\text{Sophomore})_i \\
    &\qquad + \beta_3 (\text{Junior})_i + \beta_4 (\text{Senior})_i + \varepsilon_i.
\end{aligned}
\]

This model embeds the grouping structure while only having one parameter
for the effect of sleep on the PSS score and one parameter for the
residual variance. You might at first think ``what happened to
freshman?'' To see what is really happening with this model, think about
what the structure provides us. Suppose we are considering freshman
students who get \(x\) hours of sleep; for this group of students, the
value of the variables \texttt{Sophomore}, \texttt{Junior}, and
\texttt{Senior} are all zero. Plugging into the deterministic portion of
the model, we find that the average PSS score for this group is

\[\beta_0 + \beta_1 x + \beta_2 (0) + \beta_3 (0) + \beta_4 (0) = \beta_0 + \beta_1 x.\]

The fact that each of the variables \texttt{Sophomore}, \texttt{Junior},
and \texttt{Senior} take either the value 0 or 1 makes the arithmetic
work out nicely. We can easily write down the average PSS score for each
group for a specific number of hours of sleep:

\[
\begin{aligned}
  E\left[\text{PSS Score} \mid \text{Hours Sleep, Freshman}\right]
    &= \beta_0 + \beta_1 (\text{Hours Sleep}) \\
  E\left[\text{PSS Score} \mid \text{Hours Sleep, Sophomore}\right]
    &= \left(\beta_0 + \beta_2\right) + \beta_1 (\text{Hours Sleep})\\
  E\left[\text{PSS Score} \mid \text{Hours Sleep, Junior}\right]
    &= \left(\beta_0 + \beta_3\right) + \beta_1 (\text{Hours Sleep}) \\
  E\left[\text{PSS Score} \mid \text{Hours Sleep, Senior}\right]
    &= \left(\beta_0 + \beta_4\right) + \beta_1 (\text{Hours Sleep}). \\
\end{aligned}
\]

So, freshman did not disappear from the model; they were there all along
in the intercept. This strategy relies on capturing the grouping
structure through a series of binary (0 or 1) variables, known as
indicator variables.

\begin{definition}[Indicator
Variables]\protect\hypertarget{def-indicator-variables}{}\label{def-indicator-variables}

Also called ``dummy variables,'' these are a set of binary variables
that capture the grouping defined by a categorical variable for
regression modeling.

\end{definition}

Indicator variables are like light switches that click on or off in
order to specify that a particular subject (or population of subjects)
with the corresponding characteristic is being considered. The way that
we have defined these variables ensures that no two light switches are
on at the same time; each subject is a member of exactly one group (a
student must have a class standing and cannot have two class standings
simultaneously; that is, a student cannot be classified as both a
freshman and a sophomore).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

A categorical predictor with \(k\) groups/levels requires \(k-1\)
indicator variables to fully capture the grouping structure.

\end{tcolorbox}

One group (known as the reference group) will always be captured by the
intercept term; the choice of this group is arbitrary and is often
chosen by the software package (perhaps alphabetically, for example).
Note that if the only difference between two models is the choice of the
reference group, the models result in equivalent inference (though the
interpretation of the parameters differs).

\begin{definition}[Reference
Group]\protect\hypertarget{def-reference-group}{}\label{def-reference-group}

The group defined by having all indicator variables for a particular
categorical variable set to zero.

\end{definition}

Recall that provided the ``mean 0'' condition on the error holds, we
have an interpretation of the coefficients in the model
(Definition~\ref{def-slope}). This yields a nice interpretation of the
coefficients for indicator variables.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Interpretation of Coefficient for Indicator}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Let \(\beta\) be the parameter corresponding to an indicator variable in
a linear model; then, \(\beta\) is the difference in the \emph{average}
response between the group defined by that indicator taking the value 1
and the reference group \emph{holding all other predictors fixed}.

\end{tcolorbox}

This does create a situation we have not yet encountered. Suppose we are
interested in determining if the PSS score is associated with class
standing after accounting for the hours of sleep the student gets on a
typical night. The hypothesis is no longer of the form

\[H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0\]

for some \(j\). Instead, there are several predictors in the model which
capture class standing. We need to instead consider testing multiple
parameters \emph{simultanesously}.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

The statistical significance of a categorical predictor is assessed by
testing if \emph{all} corresponding indicator variables are
simultaneously 0.

\end{tcolorbox}

In our case, we would be testing a hypothesis of the form

\[H_0: \beta_2 = \beta_3 = \beta_4 = 0 \qquad \text{vs.} \qquad H_1: \text{at least one of these } \beta_j \text{ not equal to 0}.\]

We will address hypotheses of this form in
Chapter~\ref{sec-modeling-linear-hypotheses}.

We end this section by stating that while our discussion centered on the
inclusion of categorical predictors in the linear model, this is a
general modeling technique. Regardless of the type of regression model,
categorical predictors can be included through the use of indicator
variables.

\hypertarget{sec-modeling-interactions}{%
\chapter{Allowing Effect Modification (Interaction
Terms)}\label{sec-modeling-interactions}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Chapter~\ref{sec-glm-model-framework} outlined three broad uses for
creating a multivariable model.
Chapter~\ref{sec-modeling-related-predictors} highlighted the benefit of
\emph{isolating} an effect. In this chapter, we discuss how to examine
the \emph{interplay} between two predictors. That is, we allow the
effect of one predictor to be modified (or dependent upon) the value of
another predictor. This is done through ``interaction'' terms in the
model

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-warning-color-frame, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, opacitybacktitle=0.6]

``Interactions'' are \emph{not} about the relationship between
predictors. Our goal is still to examine the relationship between the
response and the predictor. An interaction allows that relationship to
depend upon another predictor.

\end{tcolorbox}

Consider a linear model which views the response as a function of two
quantitative predictors and a categorical variable with only two groups;
that is,

\begin{equation}\protect\hypertarget{eq-interactions-base}{}{(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor 1})_i + \beta_2 (\text{Predictor 2})_i + \beta_3 (\text{Group B})_i + \varepsilon_i}\label{eq-interactions-base}\end{equation}

where

\[
(\text{Group B})_i = \begin{cases}
  1 & \text{if i-th participant belongs to Group B} \\
  0 & \text{if i-th participant belongs to Group A.}
\end{cases}
\]

This model says there is a single effect of the second predictor on the
response; that is, the effect of Predictor 2 is the same for
participants in Group A as it is for those in Group B. What if we
believe the effect is of Predictor 2 differs in the two groups? A naive
approach would be to consider two separate models:

\[
\begin{aligned}
  \text{Model 1 (Group A Only)}:& \quad (\text{Response})_i = 
    \alpha_0 + \alpha_1 (\text{Predictor 1})_i + \alpha_2 (\text{Predictor 2})_i + \varepsilon_{1,i} \\
  \text{Model 2 (Group B Only)}:& \quad (\text{Response})_i =
    \gamma_0 + \gamma_1 (\text{Predictor 1})_i + \gamma_2 (\text{Predictor 2})_i + \varepsilon_{2,i}.
\end{aligned}
\]

This creates some of the same problems we saw with creating multiple
models in order to address categorical predictors (see
Chapter~\ref{sec-modeling-categorical-predictors}). In particular,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We obtain two estimates of residual variance, but neither estimate is
  using the full data.
\item
  While we accomplished our goal of letting the effect of Predictor 2
  differ in each group, we also allowed the effect of Predictor 1 to
  differ in each group. If we believe the effect of Predictor 1 is the
  same across the two groups, allowing it to be estimated separately in
  each group is inefficient.
\item
  It makes it difficult to clearly conduct a hypothesis test to
  determine if there is evidence the effects are actually different;
  that is, it is harder to quantify the evidence that
  \(\alpha_2 \neq \gamma_2\).
\end{enumerate}

We seek a modeling structure that allows the effect of the a predictor
(Predictor 2 in the model above) to change for each value of the second
predictor (the group structure in this case). That is, we would like to
capture the possibility illustrated in
Figure~\ref{fig-modeling-interactions-interactions}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-modeling-interactions-interactions-1.pdf}

}

\caption{\label{fig-modeling-interactions-interactions}Illustration of
the interaction of a quantitative predictor and a qualitative predictor
when predicting a quantitative response.}

\end{figure}

Consider the following model:

\begin{equation}\protect\hypertarget{eq-interactions-interactions}{}{
\begin{aligned}
  (\text{Response})_i 
    &= \beta_0 + \beta_1 (\text{Predictor 1})_i + \beta_2 (\text{Predictor 2})_i \\
    &\qquad + \beta_3 (\text{Group B})_i + \beta_4 (\text{Predictor 2})_i (\text{Group B})_i + \varepsilon_i
\end{aligned}
}\label{eq-interactions-interactions}\end{equation}

To motivate the development of this model, let's return to
Equation~\ref{eq-interactions-base}. Remember, our goal is to allow the
effect of Predictor 2, captured by \(\beta_2\), to depend on the group
to which the participant belongs. Imagine replacing \(\beta_2\) with

\[\beta_2 = \eta_0 + \eta_1 (\text{Group B})_i.\]

Notice that this says the \emph{effect} \(\beta_2\) can depend on
whether the participant is in Group B; if the participant is in Group A,
then the indicator is 0 and \(\beta_2 = \eta_0\). If, on the other hand,
the participant is in Group B, then the indicator is 1 and
\(\beta_2 = \eta_0 + \eta_1\). Substituting this in, we have

\[
\begin{aligned}
  (\text{Response})_i 
    &= \beta_0 + \beta_1 (\text{Predictor 1})_i + \left(\eta_0 + \eta_1 (\text{Group B})_i\right) (\text{Predictor 2})_i \\
    &\qquad + \beta_3 (\text{Group B})_i + \varepsilon_i \\
    &= \beta_0 + \beta_1 (\text{Predictor 1})_i + \eta_0 (\text{Predictor 2})_i \\
    &\qquad + \eta_1 (\text{Predictor 2})_i (\text{Group B})_i + \beta_3 (\text{Group B})_i + \varepsilon_i.
\end{aligned}
\]

Recognizing that the choice of Greek letters \(\eta_0\) and \(\eta_1\)
are arbitrary, we have the same model as in
Equation~\ref{eq-interactions-interactions}.

Equation~\ref{eq-interactions-interactions} adds another variable to the
model that is the product of the second predictor and the group
indicator. Let's examine the structure that is provided here. Suppose we
are interested in examining the mean response for subjects in Group A;
the value of the group indicator is 0 for these subjects, leading to

\[E\left[\text{Response} \mid \text{Predictors, Group A}\right] = \beta_0 + \beta_1  (\text{Predictor 1}) + \beta_2 (\text{Predictor 2}).\]

For subjects in Group B, the value of group indicator is 1, leading to
the mean response

\[E\left[\text{Response} \mid \text{Predictors, Group B}\right] = \left(\beta_0 + \beta_3\right) + \beta_1  (\text{Predictor 1}) + \left(\beta_2 + \beta_4\right) (\text{Predictor 2}).\]

This allows both the intercept and the slope associated with Predictor 2
to differ between the two groups. That is, it allows not only a ``bump''
for being in Group B to the mean response, but it also allows the
\emph{effect} of the second predictor to differ for the two groups.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

In order to capture complex modeling structures, we embed those
structures in a large model as opposed to fitting several smaller models
in different subgroups of the population.

\end{tcolorbox}

We prefer the strategy in Equation~\ref{eq-interactions-interactions} to
performing a subgroup analysis.

\begin{definition}[Subgroup
Analysis]\protect\hypertarget{def-subgroup-analysis}{}\label{def-subgroup-analysis}

Refers to repeating a specified analysis (e.g., regression model) within
various levels of a categorical predictor.

\begin{itemize}
\tightlist
\item
  This will appropriately estimate the effect modification.
\item
  This results in a loss of information because \emph{all parameters}
  are forced to vary across the subgroups.
\end{itemize}

\end{definition}

The new predictor added to our model in
Equation~\ref{eq-interactions-interactions} (the product term) is known
as an interaction term.

\begin{definition}[Interaction]\protect\hypertarget{def-interaction}{}\label{def-interaction}

An interaction term allows the effect of a predictor on the response to
depend on the value of a second predictor (capturing an effect
modification).

\begin{itemize}
\tightlist
\item
  The interaction term is created by adding the product of the two
  predictors under consideration to the model.
\end{itemize}

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While we have illustrated the use of interaction terms using a
quantitative and categorical predictor, interactions can be used with
any type of predictors. For example, we could have considered the
product of Predictors 1 and 2 in Equation~\ref{eq-interactions-base} if
we had wanted to.

\end{tcolorbox}

While we have illustrated the use of interaction terms in a linear
model, this is a general modeling technique that can be extended to
other forms of regression models.

\hypertarget{sec-modeling-linear-hypotheses}{%
\chapter{General Linear Hypothesis
Test}\label{sec-modeling-linear-hypotheses}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

We previously discussed a model for the sampling distribution of the
parameter estimates (Definition~\ref{def-ls-sampling-distribution}) that
allows for making inference on individual parameters; that is, we can
test hypotheses of the form

\[H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0.\]

However, we do not yet have a way of testing more complex hypotheses
that occur regularly in scientific research. For example, testing
whether the response is associated a categorical predictor involves
determining if there is evidence that any of the coefficients associated
with the indicator variables differs from zero. Such simultaneous tests
fall under the General Linear Hypothesis framework.

While we write hypotheses as statements about parameters, we should keep
in mind that they are really comparisons of alternative models for the
response.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Hypothesis testing is a way of determining if a simpler model (under the
null hypothesis) is sufficient for explaining the variability in the
response or if a more complex model (under the alternative hypothesis)
is necessary. The simpler model is the result of placing
\emph{constraints} on the complex model.

Statistical inference then allows us to determine if we can discern the
difference between data generated under the simpler model and that
observed. That is, we seek evidence that a more complex model is needed
to capture the observed variability.

\end{tcolorbox}

The vast majority of scientific questions can be framed as a hypothesis
which places a constraint on a more complex model for the data
generating process. These constraints can in turn often be written as a
linear combination of the parameters.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

For those less familiar with matrix algebra, a linear combination simply
a sum of parameters that have been multiplied by constants. We can write
a linear combination as the product of two vectors, and a series of
linear combinations is the product of a matrix and a vector. Let
\(\mathbf{K}\) be a 2-by-3 matrix defined as

\[\mathbf{K} = \begin{pmatrix} 
   K_{1,1} & K_{1,2} & K_{1,3} \\
   K_{2,1} & K_{2,2} & K_{2,3} \end{pmatrix}.\]

Let \(\boldsymbol{\beta}\) be a column vector of length 3 defined as

\[\boldsymbol{\beta} = \begin{pmatrix}
   \beta_1 \\
   \beta_2 \\
   \beta_3 \end{pmatrix}.\]

The product \(\mathbf{K} \boldsymbol{\beta}\) is defined as

\[\mathbf{K}\boldsymbol{\beta} = \begin{pmatrix}
   K_{1,1} \beta_1 + K_{1,2} \beta_2 + K_{1,3} \beta_3 \\
   K_{2,1} \beta_1 + K_{2,2} \beta_2 + K_{2,3} \beta_3 \end{pmatrix}\]

which is a vector of length 2. Each element in
\(\mathbf{K}\boldsymbol{\beta}\) is a linear combination of the elements
of \(\boldsymbol{\beta}\).

\end{tcolorbox}

\begin{definition}[General Linear
Hypothesis]\protect\hypertarget{def-general-linear-hypothesis}{}\label{def-general-linear-hypothesis}

The general linear hypothesis framework refers to testing hypotheses of
the form

\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m} \qquad \text{vs.} \qquad H_1: \mathbf{K}\boldsymbol{\beta} \neq \mathbf{m}\]

where

\begin{itemize}
\tightlist
\item
  \(\boldsymbol{\beta}\) is the \((p+1)\)-length vector of the
  parameters (includes the intercept),
\item
  \(\mathbf{K}\) is an \(r\)-by-\((p+1)\) matrix that specifies the
  linear combinations defining the hypothesis of interest, and
\item
  \(\mathbf{m}\) is a vector of length \(r\) specifying the null values,
  the value of each linear combination under the null hypothesis (often
  a vector of 0's).
\end{itemize}

\end{definition}

Before discussing inference for this hypothesis, we discuss the most
common use of this framework. Consider the following linear model:

\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor 1})_i + \beta_2 (\text{Predictor 2})_i + \varepsilon_i.\]

Suppose we are interested in testing the following hypotheses:

\[H_0: \beta_1 = \beta_2 = 0 \qquad \text{vs.} \qquad H_1: \text{At least one } \beta_j \text{ not equal to 0}.\]

To express this in the general linear hypothesis framework, we must
identify the matrix \(\mathbf{K}\) and the vector \(\mathbf{m}\). To do
this, note that we can rewrite the null hypothesis as

\[
\begin{aligned}
  H_0: & \beta_1 = 0 \qquad \text{and} \\
  & \beta_2 = 0.
\end{aligned}
\]

Now, we identify the parameter vector as

\[\boldsymbol{\beta} = \begin{pmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2 \end{pmatrix}.\]

There are actually several choices for \(\mathbf{K}\), but we select the
most straight-forward that corresponds to how we rewrote the null
hypothesis:

\[\mathbf{K} = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \end{pmatrix} \qquad \text{with} \qquad \mathbf{m} = \begin{pmatrix}
0 \\ 0 \end{pmatrix}.\]

That is, the null hypothesis above can be stated as

\[H_0: \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \boldsymbol{\beta} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.\]

Notice that each row of this hypothesis corresponds to one of the
equalities expressed in the original statement of this hypothesis.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

When developing the matrix \(\mathbf{K}\), the number of rows
corresponds to the number of equal signs in the null hypothesis.

\end{tcolorbox}

The general linear hypothesis allows us to say something about multiple
parameters (or combinations of parameters) \emph{simultaneously}. Each
linear combination is not a separate hypothesis; together, they form a
``joint'' hypothesis. That is, we should think of each linear
combination defined by the rows of \(\mathbf{K}\) as ``and'' statements;
we want every statement to be true at the same time. The framework is
extremely flexible and can be used across several types of statistical
models. It allows us to write the hypotheses compactly, mostly for
communicating them to a computer. However, the framework alone does not
produce p-values for such tests. In order to obtain a p-value, we need a
model for the null distribution.

As the hypothesis involves many parameters, we cannot (and should not)
test each statement separately. To fully understand that statement
requires a background in statistical theory. We hand-wave this by saying
that our parameter estimates are related. This is somewhat intuitive.
Imagine trying to develop a line that runs through a cloud of points
(see Figure~\ref{fig-modeling-linear-hypotheses-line}); if we constrain
the line to go through the ``middle'' of the data (the point represented
by the average of the predictor and the average of the response), then
changing the slope of the line will necessarily change the intercept of
the line. We extend this intuition by claiming that the estimate of one
coefficient is related to the estimate of the other parameters in a
model.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-modeling-linear-hypotheses-line-1.pdf}

}

\caption{\label{fig-modeling-linear-hypotheses-line}Illustration of the
relationship between parameter estimates.}

\end{figure}

We know that the standard error is a measure of the variability in an
estimate; we could just as easily use the variance (the square of the
standard error). A convenient measure of the relationship between the
estimates is known as the covariance. We then store all the information
about how the parameter estimates vary and co-vary in the
variance-covariance matrix.

\begin{definition}[Variance-Covariance
Matrix]\protect\hypertarget{def-variance-covariance-matrix}{}\label{def-variance-covariance-matrix}

Let \(\boldsymbol{\beta}\) represent the \((p+1)\)-length vector of the
parameters and \(\widehat{\boldsymbol{\beta}}\) represent the \((p+1)\)
vector of the parameter \emph{estimates}. The variance-covariance matrix
of the parameter estimates is the \((p+1)\)-by-\((p+1)\) matrix
\(\boldsymbol{\Sigma}\) where

\begin{itemize}
\tightlist
\item
  the \(j\)-th diagonal element contains
  \(Var\left(\widehat{\beta}_j\right)\), and
\item
  the \((i,j)\)-th element contains the covariance between
  \(\widehat{\beta}_i\) and \(\widehat{\beta}_j\).
\end{itemize}

\end{definition}

The variance-covariance matrix is an extremely important concept in
statistical theory; here, we simply note that it contains information on
the structure of how the estimates are related to one another. We
further note that this is computed automatically in most software. We
are now in a place to discuss inference for the general linear
hypothesis.

\begin{definition}[Model for the Null Distribution with the General
Linear
Hypothesis]\protect\hypertarget{def-general-linear-hypothesis-null}{}\label{def-general-linear-hypothesis-null}

Let \(\widehat{\boldsymbol{\beta}}\) be the \((p+1)\) vector of
estimates for the parameter vector \(\boldsymbol{\beta}\), and let the
estimates have variance-covariance matrix \(\boldsymbol{\Sigma}\).
Assuming the null hypothesis

\[H_0: \mathbf{K} \boldsymbol{\beta} = \mathbf{m}\]

is true, under the conditions of the classical regression model
(Definition~\ref{def-classical-regression})

\[(1/r) \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim F_{r, n-p-1}.\]

\end{definition}

As with previous results about the sampling and/or null distribution,
the specifics of the above result are not as important as understanding
there exists a standardized statistic for which the null distribution
can be modeled explicitly, as an F-distribution with \(r\) numerator
degrees of freedom and \(n-p-1\) denominator degrees of freedom, and
that this model depends on specific conditions. While the denominator
degrees of freedom are associated with the scaling term for the residual
variance estimate, the numerator degrees of freedom are associated with
the complexity of the hypothesis (the number of rows of \(\mathbf{K}\)).

While the theory that provides the above results holds only for the
linear model under the classical regression model, the approach we have
outlined here will allow us to provide general results which are
applicable under many types of regression models.

\hypertarget{sec-modeling-large-sample-theory}{%
\chapter{Large Sample Theory}\label{sec-modeling-large-sample-theory}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The classical regression model
(Definition~\ref{def-classical-regression}) imposes several conditions
on the distribution of the error term. These conditions are needed to
depend on the model for the sampling distribution of the parameter
estimates we developed in that section
(Definition~\ref{def-ls-sampling-distribution}). However, these
conditions are not always reasonable. Fortunately, many of the
conditions can be relaxed. In this section, we consider relaxing the
``Normality'' condition. Changing the conditions we impose impacts how
we model the sampling distribution of our estimates (and therefore
impacts confidence intervals and p-values).

\hypertarget{two-types-of-models}{%
\section{Two Types of Models}\label{two-types-of-models}}

Models for the data generating process are broadly characterized into
one of three groups: parametric, semiparametric, and nonparametric.

\begin{definition}[Parametric
Model]\protect\hypertarget{def-parametric-model}{}\label{def-parametric-model}

A parametric model characterizes the distribution of the response using
a finite set of parameters; for our purposes, this generally means the
model \emph{fully} characterize the distribution of the response given
the predictors.

\end{definition}

\begin{definition}[Nonparametric
Model]\protect\hypertarget{def-nonparametric-model}{}\label{def-nonparametric-model}

A nonparametric model is unable to characterize the response using a
finite set of parameters; for our purposes, this generally means the
model makes no assumptions about the structure of the underlying
distribution of the response given the predictors. Only minimal
assumptions (such as independence between observations) are imposed.

\end{definition}

\begin{definition}[Semiparametric
Model]\protect\hypertarget{def-semiparametric-model}{}\label{def-semiparametric-model}

A semiparametric model specifies some components of the underlying
distribution of the response using a finite set of parameters but does
not fully characterize the distribution. This generally means that we
may specify the mean and/or variance of the response given the
predictors, but we do not characterize the distributional family of the
response.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Technically, semiparametric models are a subset nonparametric models.
However, semiparametric models are often considered a distinct type of
model because they have elements of both parametric models (there are
some parameters to be estimated) and nonparametric models (completely
data-driven).

\end{tcolorbox}

Parametric models make strong assumptions, but often make the analysis
straight-forward as we are able to make use of a large class of results
from statistical theory. This is very useful when we have a small sample
size in particular. Nonparametric models are extremely flexible, but
they require substantially large sample sizes. Semiparametric models, in
turn, often find a the ``sweet spot.'' They require larger samples than
a parametric model, but not as large as a nonparametric model. Further,
the scientific question of interest is still represented through
statements about parameters in the model, linking the interpretations
more directly with practical application and making the models more
interpretable.

The alternate characterization of the classical regression model
(Definition~\ref{def-alternate-characterization}) reveals that the
classical regression model is a parametric model. It completely
characterizes the distribution of the response:

\[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i \stackrel{\text{Ind}}{\sim} N\left(\beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i, \sigma^2\right).\]

Consider the aspects being \emph{structure} specified here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The mean response is given as a linear combination of the \(p\)
  predictors.
\item
  The variance of the response is constant for all combinations of the
  predictors.
\item
  Given the predictors, the response follows a Normal distribution.
\end{enumerate}

Each of the aspects of this structure can be very useful when working
with statistical theory; however, the questions often posed by
researchers do not concern the form of the distribution of the response
but only some aspect of the distribution. For example, the hypotheses we
have considered thus far in the text surround the parameters of the mean
model; that is, we have concerned ourselves only with questions
regarding the \emph{mean} response. That is, whether we model the
distribution of the response using a Normal distribution or some other
is not of interest; scientifically, we are interested in the mean
response. If we are willing to relax the distributional form of the
response, we are led to positing a semiparametric model.

\begin{definition}[Semiparametric Linear
Model]\protect\hypertarget{def-semiparametric-linear-model}{}\label{def-semiparametric-linear-model}

Suppose we no longer require that the error terms follow a Normal
distribution; however, we do continue to impose the remaining conditions
of the classical regression model. Then, our model could be written as

\[
\begin{aligned}
  E\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i \\
  Var\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &= \sigma^2
\end{aligned}
\]

where the responses are independent of one another given the predictors.

\end{definition}

Notice that this version of the model only specifies some aspects of the
response distribution; it specifies that the mean is a linear
combination of the predictors, and it specifies that the variance is
constant. However, it does not specify the functional form of the
distribution.

\hypertarget{large-sample-results}{%
\section{Large Sample Results}\label{large-sample-results}}

The primary benefit of a parametric model is that often the
distributional assumption trickles through the analysis and allows us to
exactly specify the model for the sampling distribution of the parameter
estimates (Definition~\ref{def-ls-sampling-distribution}, for example).
When we move to a semiparametric model, we need additional tools to
allow us to model the sampling distribution of our estimates. Large
sample theory is one such tool.

\begin{definition}[Large Sample
Theory]\protect\hypertarget{def-large-sample-theory}{}\label{def-large-sample-theory}

The phrase ``large sample theory'' (or ``asymptotics'') is used to
describe a scenario when the model for the sampling distribution (or
null distribution) of an estimate (or standardized statistic) can be
approximated as the sample size becomes infinitely large. That is, as
the sample size approaches infinity, the sampling distribution (or null
distribution) can be easily modeled using a known probability
distribution.

\end{definition}

Perhaps the most well-known example of large sample theory is the
Central Limit Theorem encountered in introductory statistics.

\begin{definition}[Central Limit
Theorem]\protect\hypertarget{def-clt}{}\label{def-clt}

Let \(Y_1, Y_2, \dotsc, Y_n\) be independent and identically distributed
random variables with finite mean \(\mu\) and variance \(\sigma^2\).
Then, as \(n\) approaches infinity, the distribution of the ratio

\[\frac{\sqrt{n}\left(\bar{Y} - \mu\right)}{\sigma}\]

approaches that of a Standard Normal random variable.

\end{definition}

Rephrasing in the language of this text, Definition~\ref{def-clt} states
that as the sample size gets large, the standardized distance between
the average response observed and the true average response can be
modeled using a Normal distribution with mean 0 and variance 1. Notice
that the theorem does not specify the distribution of the response
\(Y\); it only specifies the mean and variance. That is, we began with a
semiparametric model for the data generating process (the distribution
of the response is only partially specified) and yet obtained a model
for the sampling distribution or our statistic of interest. We exchanged
the condition that the response follow a Normal distribution (a more
classical approach) for the condition that ``the sample size be
sufficiently large'' (the large sample theory approach); under thess
revised conditions, we have a model for the sampling distribution of our
parameter estimate.

It turns out that similar results can be derived for the semiparametric
linear model. That is, in large samples, we can approximate the sampling
distribution of our estimates and the null distribution of our
standardized statistics.

\begin{definition}[Large Sample Model for the Sampling Distribution of
the Least Squares
Estimates]\protect\hypertarget{def-ls-sampling-distribution-large-samples}{}\label{def-ls-sampling-distribution-large-samples}

Suppose the classical regression conditions hold, with the exception of
the errors following a Normal distribution. As the sample size gets
large, we have that the distribution of the ratio

\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim N(0, 1)\]

for all \(j = 0, 1, \dotsc, p\). Further, under the null hypothesis

\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m}\]

we have

\[\left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim \chi^2_r.\]

\end{definition}

Notice that our statistics and standardized statistic have a similar
form as before; the difference is the probability model being used. In
place of a t-distribution, we have a Normal distribution. In place of
the F-distribution, we have a Chi-Square distribution. These large
sample results allow us to perform inference even if we are unwilling to
assume the errors follow a Normal distribution.

It is natural to ask how large of a sample size is required for these
models to be reasonable; there is no simple answer. Empirical studies
suggest that in practice, if we have at least 30 degrees of freedom for
estimating the error term, these results are often reasonable. However,
empirical studies also demonstrate that it does depend on the tail
behavior of the underlying population distribution; there are some
populations that begin to mimic these results at samples of size 10 and
others that require samples of size 10000. In practice, this is an
\emph{assumption} the analyst must determine whether to adopt.

Not all software implements methods for relying on these large sample
results. However, as the sample size gets large, it turns out that
classical inference and the large sample results coincide. That is, the
confidence intervals and p-values we would compute using the large
sample models and those obtained assuming the classical regression model
are nearly identical. Therefore, in practice, when the sample size is
large, we can rely on the default output even if we are unwilling to
assume the errors follow a Normal distribution.

\hypertarget{residual-bootstrap}{%
\section{Residual Bootstrap}\label{residual-bootstrap}}

An alternative to large sample theory is building an empirical model for
the sampling distribution (or null distribution) when working with a
semiparametric model; one process for this is known as bootstrapping.

\begin{definition}[Bootstrapping]\protect\hypertarget{def-bootstrapping}{}\label{def-bootstrapping}

A process of constructing a sampling distribution of the parameter
estimates through resampling. The observed data is resampled repeatedly,
and the parameters of interest are estimated in each resample. The
distribution of these estimates across the resamples is then used as an
empirical model of the corresponding sampling distributions.

\end{definition}

There are several bootstrapping algorithms; the most foundational for
regression modeling is the residual bootstrap.

\begin{definition}[Residual
Bootstrap]\protect\hypertarget{def-residual-bootstrap}{}\label{def-residual-bootstrap}

Suppose we observe a sample of size \(n\) and use the data to compute
the least squares estimates \(\widehat{\boldsymbol{\beta}}\) for the
parameters in the model

\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]

The residual bootstrap proceeds according to the following algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the residuals
\end{enumerate}

\[(\text{Residuals})_i = (\text{Response})_i - (\text{Predicted Response})_i\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Take a random sample of size \(n\) (with replacement) of the
  residuals; call these values \(e_1^*, \dotsc, e_n^*\).
\item
  Form ``new'' responses \(y_1^*, \dotsc, y_n^*\) according to
\end{enumerate}

\[y_i^* = \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i + e_i^*.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Obtain the least squares estimates \(\widehat{\boldsymbol{\alpha}}\)
  by finding the values of \(\boldsymbol{\alpha}\) that minimize
\end{enumerate}

\[\sum_{i=1}^{n} \left(y_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i\right)^2.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Repeat steps 2-4 \(m\) times.
\end{enumerate}

We often take \(m\) to be large (at least 1000). After each pass through
the algorithm, we retain the least squares estimates
\(\widehat{\boldsymbol{\alpha}}\) from the resample. The distribution of
these estimates across the resamples is a good empirical model for the
sampling distribution of the original least squares estimates.

\end{definition}

While the residual bootstrap is the foundation of many similar
algorithms, it is perhaps not as easy to understand as the
case-resampling bootstrap.

\begin{definition}[Case Resampling
Bootstrap]\protect\hypertarget{def-case-resampling-bootstrap}{}\label{def-case-resampling-bootstrap}

Suppose we observe a sample of size \(n\) and use the data to compute
the least squares estimates \(\widehat{\boldsymbol{\beta}}\) for the
parameters in the model

\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]

The case resampling bootstrap proceeds according to the following
algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a random sample of size \(n\) (with replacement) of the raw data
  (keeping all variables from the same observation together); denote the
  \(i\)-th selected response and predictors \((\text{Response})_i^*\)
  and \((\text{Predictor } j)_i^*\), respectively.
\item
  Obtain the least squares estimates \(\widehat{\boldsymbol{\alpha}}\)
  by finding the values of \(\boldsymbol{\alpha}\) that minimize
\end{enumerate}

\[\sum_{i=1}^{n} \left((\text{Response})_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i^*\right)^2.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Repeat steps 1-2 \(m\) times.
\end{enumerate}

We often take \(m\) to be large (at least 1000). After each pass through
the algorithm, we retain the least squares estimates
\(\widehat{\boldsymbol{\alpha}}\) from the resample. The distribution of
these estimates across the resamples is a good empirical model for the
sampling distribution of the original least squares estimates.

\end{definition}

The case-resampling bootstrap procedure is easier to visualize as we are
resampling the data observed. The resampling in the residual bootstrap
is a bit more indirect as the residuals are what is resampled; this
mimics generating new observations by ``jittering'' points away from the
estimated regression line. In both algorithms, the same model is refit
on each resample producing new estimates. The collection/distribution of
these estimates across the \(m\) resamples is our model for the sampling
distribution (which could be visualized using a histogram, for example).

The theoretical underpinnings of bootstrapping (and how it is
implemented efficiently in software) is beyond the scope of this text.
What we emphasize is that through this process, we construct a model for
the sampling distribution of the estimates, and that allows us to
compute confidence intervals. Further, the residual bootstrap requires
the same conditions as the classical regression model, with the
exception of requiring the errors to follow a Normal distribution. That
is, it has the same conditions as we stated for our semiparametric
regression model above.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Technically, the case-resampling bootstrap and the residual bootstrap
require different conditions, with the case-resampling bootstrap being
less restrictive. However, at this point, we do not make a distinction
between which bootstrap algorithm is utilized. We will discuss the
benefits of case-resampling later in the text.

\end{tcolorbox}

Bootstrapping is more computationally burdensome than large sample
theory, but it does not rely on the sample size being ``large enough.''

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While theoretically bootstrapping can be used with any sample size, it
has been shown to yield more reliable results in large samples.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While we have focused on the use of bootstrapping for computing a
confidence interval, the same procedures can be adapted to compute
p-values for hypothesis tests as well.

\end{tcolorbox}

\hypertarget{choosing-a-path}{%
\section{Choosing a Path}\label{choosing-a-path}}

We have discussed two alternatives to using inference results from the
classical regression model when we are unwilling to assume the errors
follow a Normal distribution. Further, we have discussed ways to
determine if the data is consistent with the assumption that the errors
have a Normal distribution (Chapter~\ref{sec-glm-assessing-conditions}).
As we have seen throughout this unit, while these approaches were
discussed in the context of the linear model, they illustrate a concept
that holds across many types of regression models --- there are
essentially three ways to build a model for the sampling distribution of
our parameter estimates.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

There are three options for modeling the sampling (null) distribution of
a parameter estimate (standardized statistic):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exact Probability Theory: often the result of assuming a parametric
  model, the sampling distribution of the resulting parameter estimates
  is derived explicitly using probability theory.
\item
  Large Sample Theory: often employed in semiparametric models, the
  sampling distribution of the resulting parameter estimates can be
  approximated as the sample size gets large.
\item
  Empirical: often employed in semiparametric models, the sampling
  distribution of the resulting parameter estimates is modeled through
  resampling.
\end{enumerate}

\end{tcolorbox}

We will see as we move throughout the text that we often move between
these various approaches. However, which approach we take is governed by
the conditions we are willing to impose on the data generating process.

We end with a common question: if we are able to model the sampling
distribution without fewer conditions, why would we not always take that
approach? The closer the conditions are to the true data generating
process, the more powerful our analysis; that is, if the errors are
truly Normally distributed, then imposing that condition will make it
more likely for us to find a signal that really exists. So, we battle
the tension of a more powerful analysis with one that is more flexible.
We adhere to the belief that we should choose the approach that is most
consistent with the available data.

\hypertarget{sec-modeling-splines}{%
\chapter{Modeling Curvature (Splines)}\label{sec-modeling-splines}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

In addition to conditions on the error term, the classical regression
model (Definition~\ref{def-classical-regression}) requires that the
predictors enter the model linearly. It is often the case, however, that
the relationship between the response and a predictor is not linear,
even after accounting for other predictors. Ignoring this curvature,
essentially leaving the deterministic portion of the model incorrectly
specified, can result in incorrect conclusions. Fortunately, the linear
model framework is flexible enough to model curvature. The apparent
contradiction that a ``linear'' model can address curvature comes from a
misunderstanding of what it means to be a ``linear'' model.

\begin{definition}[Linear
Model]\protect\hypertarget{def-linear-model}{}\label{def-linear-model}

A model is said to be linear if it can be expressed as a linear
combination of the \emph{parameters}. That is, the linearity does not
refer to the form of the predictors but the form of the parameters.

\end{definition}

The beauty of this understanding of linearity is that it allows us to
capture curvature, provided that we can represent that curvature through
the addition of additional predictors. As a simple example, suppose we
have a response that has a parabolic relationship with a predictor
(Figure~\ref{fig-modeling-splines-parabola}).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-modeling-splines-parabola-1.pdf}

}

\caption{\label{fig-modeling-splines-parabola}Illustration of a
parabolic relationship between a response and a predictor.}

\end{figure}

Such a relationship could be captured by the \emph{linear} model

\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor})_i^2 + \varepsilon_i.\]

The mean response will clearly generate curvature, but the deterministic
portion is linear in the parameters; let \(\mathbf{x}_i\) represent the
vector of all predictors for the \(i\)-th observation, including the
intercept. In this case we have

\[\mathbf{x}_i = \begin{pmatrix} 1 \\ (\text{Predictor})_i \end{pmatrix}.\]

And, let \(\boldsymbol{\beta}\) represent the parameter vector

\[\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}.\]

Then, we can write the above model as

\[(\text{Response})_i = \mathbf{x}_i^\top \boldsymbol{\beta} + \varepsilon_i\]

where we have that the deterministic portion is the product of two
vectors; being able to express the model in this form satisfies the
definition of a linear model.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

For those not as comfortable with matrix algebra, essentially, the model
will be linear in the parameters as long as we can express it in the
form

\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Something not involving parameters})_i + \varepsilon_i\]

even if the ``something not involving parameters'' consists of nonlinear
transformations of variables in our data set.

\end{tcolorbox}

We are interested in investigating transformations of the predictors to
add to the model that would capture curvature. In the above example, we
knew the form of the curvature we wanted to model (a parabola shifted up
from the origin). In practice, we often will not know the form of the
curvature, just that it exists (from the plots of the residuals against
the predictor). And, that curvature may not be modeled well with a
high-degree polynomial (or would require a polynomial of such a high
degree it would not be practical). In such cases, splines are very
useful.

\begin{definition}[Spline]\protect\hypertarget{def-spline}{}\label{def-spline}

A spline is a continuous piecewise polynomial used to model curvature.
The points that define the piecewise components are called \emph{knot
points}; the functional form is allowed to change at the knot points.

\end{definition}

\begin{definition}[Linear
Spline]\protect\hypertarget{def-linear-spline}{}\label{def-linear-spline}

A linear spline is a continuous piecewise linear function.

\end{definition}

A linear spline is perfect for capturing relationships which appear to
be linear over regions, but for which the relationship is different in
each of those regions. For example, a ``V'' relationship would suggest
that as the predictor increases, the response tends to decrease up to a
point (the bottom of the V); past that point (which is the ``knot
point'' here), as the predictor increases, the response tends to
increase as well. A linear spline can be placed into the linear model
framework.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Formula for Linear Spline}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

A response can be related to a predictor using a linear spline with
\(k\) knot points, call them \(t_1, t_2, \dotsc, t_k\) using the
following formula:

\begin{equation}\protect\hypertarget{eq-modeling-linear-spline}{}{(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor})_i + \sum_{j=1}^{k} \beta_{j+1} \left((\text{Predictor})_i - t_j\right)_{+} + \varepsilon_i}\label{eq-modeling-linear-spline}\end{equation}

where \(u_{+}\) takes the value \(u\) when \(u > 0\) and takes the value
0 otherwise. Capturing curvature using a linear spline with \(k\) knot
points requires \(k\) additional terms.

\end{tcolorbox}

Figure~\ref{fig-modeling-splines-linear-spline} illustrates a linear
spline with two knot points.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-modeling-splines-linear-spline-1.pdf}

}

\caption{\label{fig-modeling-splines-linear-spline}Illustration of a
linear spline with two know points.}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

In practice, the knot points for a linear spline are generally
determined by the discipline expert based on some scientific reason why
the relationship might change at that particular value of the predictor.

\end{tcolorbox}

While the above definition of the linear spline considers only a single
predictor, we can add a spline to a model which has additional
predictors. That is, we could consider a model like

\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor 1})_i + \beta_2 \left((\text{Predictor 1})_i - t_1\right)_+ + \beta_3(\text{Predictor } 2)_i + \varepsilon_i.\]

Here, we have placed a linear spline with a single knot point on the
first predictor, but the second predictor enters the model linearly.

Once we have the additional elements in the model to capture the
curvature, we can actually perform a hypothesis test to determine if the
additional complexity is needed. Consider
Equation~\ref{eq-modeling-linear-spline}, the hypothesis

\[H_0: \beta_2 = \beta_3 = \dotsc = \beta_{k+1} = 0\]

imposes the constraint that each of the spline components be removed
from the model. That is, under this hypothesis, a linear relationship is
sufficient for modeling the relationship between the response and the
predictor.

There are plenty of forms of curvature which would not be captured by a
linear spline. When we have a more complex relationship that needs to be
modeled, we use a restricted cubic spline.

\begin{definition}[Restricted Cubic
Spline]\protect\hypertarget{def-restricted-cubic-spline}{}\label{def-restricted-cubic-spline}

A restricted cubic spline is a continuous function comprised of
piecewise cubic polynomials for which the tails of the spline have been
restricted to be linear.

\end{definition}

Restricted cubic splines (related to natural splines in the
computational science community) are smooth at the knot points (meaning
they have nice mathematical properties). Further, it has been shown
empirically that restricted cubic splines are often flexible enough to
approximate nearly any nonlinear relationship. As flexible as they are,
what is really amazing is that we can embed restricted cubic splines
into the linear model framework.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Formula for Resctricted Cubic Spline}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

A response can be related to a predictor using a restricted cubic spline
with \(k\) knot points, call them \(t_1, t_2, \dotsc, t_k\) using the
following formula:

\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor})_i + \sum_{j=1}^{k-2} \beta_{j+1} x_{j,i} + \varepsilon_i\]

where

\begin{equation}\protect\hypertarget{eq-modeling-rcs}{}{
\begin{aligned}
  x_{j,i} &= \left((\text{Predictor})_i - t_j\right)^3_{+} - \frac{\left((\text{Predictor})_i - t_{k-1}\right)^3_{+} \left(t_k - t_j\right)}{t_k - t_{k-1}} \\
    &\qquad +\frac{\left((\text{Predictor})_i - t_k\right)^3_{+} \left(t_{k-1} - t_j\right)}{t_k - t_{k-1}}
\end{aligned}
}\label{eq-modeling-rcs}\end{equation}

and \(u_{+}\) is a function taking the value \(u\) when \(u > 0\) and
the value 0 otherwise. Capturing curvature using a restricted cubic
spline with \(k\) knot points requires \(k-2\) additional terms.

Empirical studies have shown that generally only \(k = 5\) knot points
are needed, and these are set at the 5-th, 27.5-th, 50-th, 72.5-th, and
95-th percentiles. This ensures there is enough data in each region to
appropriately capture the curvature.

\end{tcolorbox}

Similar to linear splines, we can add a restricted cubic spline for a
variable to a model which has additional predictors. For example,

\[
\begin{aligned}
(\text{Response})_i 
  &= \beta_0 + \beta_1 (\text{Predictor 1})_i + \beta_2 \left((\text{Predictor 1})_i - t_1\right)_+^3 \\
  &\qquad - \beta_2 \frac{\left((\text{Predictor 1})_i - t_2\right)_+^3 \left(t_3 - t_1\right)}{t_3 - t_2} + \beta_2 \frac{\left((\text{Predictor 1})_i - t_3\right)_+^3 \left(t_2 - t_1\right)}{t_3 - t_2} \\
  &\qquad + \beta_3 (\text{Predictor 2})_i + \varepsilon_i
\end{aligned}
\]

captures curvature in the first predictor using a restricted cubic
spline with three knot points while allowing the second predictor to
enter the model linearly. We note that while it appears this model is
much more complex, only a single additional term is needed to capture
the curvature on the first predictor.

Once we have the additional elements in the model to capture the
curvature with the spline, we can actually perform a hypothesis test to
determine if the additional complexity is needed. Consider the model in
Equation~\ref{eq-modeling-rcs}, the hypothesis

\[H_0: \beta_2 = \beta_3 = \dotsc = \beta_{k-1} = 0\]

imposes the constraint that each of the spline components be removed
from the model. That is, under this hypothesis, a linear relationship is
sufficient for modeling the relationship between the response and the
predictor.

Figure~\ref{fig-modeling-splines-rcs} illustrates a restricted cubic
spline with five knot points. We point out that there is quite a bit of
curvature here, and yet this is captured by a linear model!

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-modeling-splines-rcs-1.pdf}

}

\caption{\label{fig-modeling-splines-rcs}Illustration of a restricted
cubic spline with five knot points.}

\end{figure}

Both types of splines can be fit using standard software if we are
willing to program the above formulas; however, statistical software
often has a direct implementation.

There are nonparametric approaches to capturing curvature as well
(``loess'' curves are popular choices). We prefer splines to
nonparametric approaches as splines require less computation and can be
easily implemented in any software. Further, splines can be placed in a
semiparametric model allowing us to capture complex curvature without
extremely large sample sizes. Finally, since splines can be implemented
within a linear model, we can easily test to determine if the additional
complexity is needed.

Linear splines allow for very easy interpretation since the
relationships are linear in each region. Restricted cubic splines, in
contrast, have intractable interpretations but provide a lot of
flexibility. In general, if the predictor that requires a spline is the
primary variable of interest, we recommend trying a linear spline. If
however, the predictor is just being adjusted for in the model, and the
curvature is not of primary importance, using a restricted cubic spline
with five knot points is typically sufficient.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

When a categorical predictor is modeled using a series of indicator
variables, linearity cannot be violated for these components. This is
known as a ``saturated'' model because no simplifying assumptions about
the structure have been enforced. As a result, violations of linearity
and their subsequent adjustments (like splines) are only considered for
quantitative predictors.

\end{tcolorbox}

As in the previous chapters of this unit, we introduced splines in the
context of the linear model. However, they can be incorporated into a
vast number of regression models.

\part{Unit IV: Models for Repeated Measures}

The conditions we impose on the data generating process impact the model
of the sampling distribution of our parameter estimates, and the model
for the sampling distribution is necessary for performing inference. At
times, the conditions we are willing to impose are driven by the data
collection procedure. In this unit, we consider data that result from
study designs that lead to collections of observations being associated
with one another (beyond what can be explained by the predictors in the
model). This correlation between observations must be appropriately
modeled if we want to model the sampling distribution of our parameter
estimates.

Specifically, this unit considers the topic of repeatedly measuring the
response. This could be the result of a study design that requires the
response be measured routinely on the same participants (a ``pre-post''
study, for example); or, it could be the result of a study design that
first enrolls entire families and then records the response for each
member of the family. In the first example, it is reasonable to believe
the responses recorded on the same participant are associated with one
another in some way; in the second example, it is reasonable to believe
responses recorded on members of the same family are associated with one
another in some way. Understanding how to incorporate this relationship
in the model allows us to conduct such studies, which often have more
power to detect effects of interest.

\hypertarget{sec-rm-terminology}{%
\chapter{The Language of Repeated Measures}\label{sec-rm-terminology}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

\hypertarget{importance-of-study-design}{%
\section{Importance of Study Design}\label{importance-of-study-design}}

Study design is too often separated from the statistical analysis that
follows. However, in addition to informing the conclusions we draw
regarding the data, the study design helps in choosing an appropriate
analysis to address the question of interest. As an example, consider
the following study reported in Vittinghoff et al. (2012).

\begin{example}[Digestive
Enzymes]\protect\hypertarget{exm-rm-enzyme}{}\label{exm-rm-enzyme}

The ability of the bowels to properly absorb nutrients can be impacted
by a lack of digestive enzymes. This presents as excess fat in the
feces, which is in turn treated with pancreatic enzyme supplements. A
study was conducted comparing three forms of a particular enzyme
supplement; these were compared to no supplement (placebo) as a control.
Participants were given the supplement to take for a specified length of
time; then, the amount of fecal fat (g/day) present was recorded.
Interest is in determining if the amount of fecal fat produced, on
average, differs for any of the treatments.

\end{example}

Suppose we are given the data in Table~\ref{tbl-rm-enzyme-data-table} to
address the question posed in Example~\ref{exm-rm-enzyme}.

\hypertarget{tbl-rm-enzyme-data-table}{}
\begin{table}
\caption{\label{tbl-rm-enzyme-data-table}Fecal fat (g/day) present in participants given a pancreatic enzyme
supplement. }\tabularnewline

\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
\cellcolor{gray!6}{Placebo} & \cellcolor{gray!6}{44.5} & \cellcolor{gray!6}{33.0} & \cellcolor{gray!6}{19.1} & \cellcolor{gray!6}{9.4} & \cellcolor{gray!6}{71.3} & \cellcolor{gray!6}{51.2}\\
Tablet & 7.3 & 21.0 & 5.0 & 4.6 & 23.3 & 38.0\\
\cellcolor{gray!6}{Coated Capsule} & \cellcolor{gray!6}{12.4} & \cellcolor{gray!6}{25.6} & \cellcolor{gray!6}{22.0} & \cellcolor{gray!6}{5.8} & \cellcolor{gray!6}{68.2} & \cellcolor{gray!6}{52.6}\\
Uncoated Capsule & 3.4 & 23.1 & 11.8 & 4.6 & 25.6 & 36.0\\
\bottomrule
\end{tabular}
\end{table}

Consider the following generalized linear model to describe the data
generating process:

\begin{equation}\protect\hypertarget{eq-rm-terminology-glm}{}{(\text{Fat})_i = \beta_0 + \beta_1 (\text{Tablet})_i + \beta_2 (\text{Coated})_i + \beta_3 (\text{Uncoated})_i + \varepsilon_i,}\label{eq-rm-terminology-glm}\end{equation}

where \texttt{Tablet}, \texttt{Coated}, and \texttt{Uncoated} are
indicator variables capturing the impact of the categorical treatment
group. Our question of interest is captured by the testing

\[H_0: \beta_1 = \beta_2 = \beta_3 = 0 \qquad \text{vs.} \qquad H_1: \text{At least one } \beta_j \text{ differs}.\]

This analysis demonstrates no evidence (p = 0.168) the average amount of
fecal fat differs for any of the supplement forms (see
Figure~\ref{fig-rm-terminology-enzyme-plot}).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-rm-terminology-enzyme-plot-1.pdf}

}

\caption{\label{fig-rm-terminology-enzyme-plot}Fecal fat (g/day) present
in participants given a pancreatic enzyme supplement when we assume 24
independent participants.}

\end{figure}

Of course, the above analysis is predicated on the data being consistent
with the conditions for the classical regression model. For example, it
seems reasonable to assume that the fecal fat present in one subject is
independent of the fecal fat present in any other subject once we have
accounted for the type of supplement. Therefore, it seems reasonable
that any two fecal fat measurements above are independent once we have
accounted for the type of supplement received. This, however, follows
from how we assumed the data was collected --- that each measurement is
from a different subject.

Let's reconsider the above example but add a little more context to the
study design.

\begin{example}[Example~\ref{exm-rm-enzyme} Continued (Expanded
Context)]\protect\hypertarget{exm-rm-enzyme-expanded}{}\label{exm-rm-enzyme-expanded}

The study described in Example~\ref{exm-rm-enzyme} was actually
conducted as a cross-over study enrolling six participants. Each
participant was given one form of the enzyme (determined randomly) and
followed for a specified period of time at which point the amount of
fecal fat (g/day) present was obtained. Following a substantial wash-out
period the participant was assigned a different form of the supplement.
This continued until each participant had been assigned to all four
supplement forms.

Interest is in determining if the amount of fecal fat produced, on
average, differed for any of the treatments. However, it is known that
the amount of fecal fat present can vary substantially from one
individual to another due to dietary preferences; researchers would like
to account for the variation in the fecal fat across subjects when
performing the analysis.

\end{example}

Table~\ref{tbl-rm-enzyme-data-table-expanded} provides the same data as
Table~\ref{tbl-rm-enzyme-data-table} but with the additional context
given in Example~\ref{exm-rm-enzyme-expanded} --- specifically, that six
participants underwent each of the four treatments.

\hypertarget{tbl-rm-enzyme-data-table-expanded}{}
\begin{table}
\caption{\label{tbl-rm-enzyme-data-table-expanded}Fecal fat (g/day) of six participants enrolled in a cross-over study
examining four types of enzyme supplement. }\tabularnewline

\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & Subject 1 & Subject 2 & Subject 3 & Subject 4 & Subject 5 & Subject 6\\
\midrule
\cellcolor{gray!6}{Placebo} & \cellcolor{gray!6}{44.5} & \cellcolor{gray!6}{33.0} & \cellcolor{gray!6}{19.1} & \cellcolor{gray!6}{9.4} & \cellcolor{gray!6}{71.3} & \cellcolor{gray!6}{51.2}\\
Tablet & 7.3 & 21.0 & 5.0 & 4.6 & 23.3 & 38.0\\
\cellcolor{gray!6}{Coated Capsule} & \cellcolor{gray!6}{12.4} & \cellcolor{gray!6}{25.6} & \cellcolor{gray!6}{22.0} & \cellcolor{gray!6}{5.8} & \cellcolor{gray!6}{68.2} & \cellcolor{gray!6}{52.6}\\
Uncoated Capsule & 3.4 & 23.1 & 11.8 & 4.6 & 25.6 & 36.0\\
\bottomrule
\end{tabular}
\end{table}

The study design results in several measurements being taken on each
subject; further, the researchers believe that amount of fecal fat
present can vary substantially from one individual to another. This
additional information suggests that observations from the same
individual are associated in some way; it is therefore unreasonable to
assume the errors in Equation~\ref{eq-rm-terminology-glm} are
independent of one another.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

For those familiar, you may recognize the cross-over design described in
Example~\ref{exm-rm-enzyme-expanded} as a ``randomized complete block
design.'' Others may be familiar with the concept of ``paired data,'' of
which Example~\ref{exm-rm-enzyme-expanded} is a generalization. While
the idea is similar, the methods discussed in this text are a more
inclusive approach.

\end{tcolorbox}

Why would recording multiple observations on the same subject impact the
condition of independence? A cursory look at the data confirms the
researchers' beliefs: the values of fecal fat vary greatly from one
participant to another, ranging from 9 to 71 g/day in some cases.
However, the values of fecal fat recorded for a single participant do
not tend to vary to such a degree. That is, the variability
\emph{between} participants is substantially larger than the variability
\emph{within} a participant. As suggested by the researchers, this could
be explained by differences in participant diets. It is difficult to
detect differences between the supplement types since the differences
between subjects is so much larger.

To better understand why researchers would design such a study, we
review attributes of good study design. Generally speaking, there are
three components to any well-designed study: replication, randomization,
and reduction of extraneous noise.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-warning-color-frame, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, opacitybacktitle=0.6]

A study is not poor just because it lacks one of these elements. That
is, a study can provide meaningful insights even if it does not make use
of each of these elements; every study is unique and should be designed
to address the research objective. These elements are simply helpful in
creating study designs.

\end{tcolorbox}

\begin{definition}[Replication]\protect\hypertarget{def-replication}{}\label{def-replication}

Replication results from taking measurements on different units (or
subjects) for which you expect the results to be similar. That is, any
variability across the units is due to natural variability within the
population.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-warning-color-frame, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, opacitybacktitle=0.6]

The term ``replication'' is also used in the context of discussing
whether the results of a study are replicable. While our use of the term
is about replicating a measurement process within a study, this does not
downplay the importance of replicating an entire study.

\end{tcolorbox}

Replication allows us to estimate subject-to-subject variability. Our
intuition is that more data is better; in fact, increasing the sample
size (the number of \emph{unique} subjects on which we collect data)
will result in less variability in our estimates. That is, the sampling
distribution for the parameter estimates will be narrower. Increased
replication also leads to increased power --- the ability to detect a
signal when it really exists (as the null distribution will also be
narrowed).

\begin{definition}[Randomization]\protect\hypertarget{def-randomization}{}\label{def-randomization}

Randomization can refer to random \emph{selection} or random
\emph{allocation}.

Random selection refers to the use of a random mechanism to select units
from the population. Random selection minimizes bias.

Random allocation refers to the use of a random mechanism when assigning
units to a specific treatment group in a controlled experiment. Random
allocation eliminates confounding and permits causal interpretations.

\end{definition}

There are many forms of random sampling. Some sampling schemes ensure
each collection of subjects is equally likely (e.g., simple random
sample), while others over-sample from underrepresented subpopulations
(e.g., stratified random sample). Each scheme shares the goal of
eliminating bias, making the data more representative of the target
population. While random sampling is the ideal, it is not always
feasible. In clinical trials, for example, patients must elect to
participate, thereby making the sample not random. When a random sample
is not possible, summarizing the data to ensure it is representative of
the target population is critical.

There are many forms of random allocation. Some randomization schemes
ensure each treatment group is equally likely, while others assign
participants to the active treatment with a higher probability than to
the placebo. Other randomization schemes, like that mentioned in
Example~\ref{exm-rm-enzyme-expanded} randomizes the \emph{order} of
treatments. Each scheme shares the goal of eliminating confounding,
allowing for causal interpretations. Whenever possible, random
allocation is utilized, but it is not always feasible. Perhaps most
famously, it would be unethical to conduct a randomized controlled trial
to investigate the link between smoking and cancer. Therefore,
observational studies were utilized to establish this link.

\begin{definition}[Reduction of
Noise]\protect\hypertarget{def-noise-reduction}{}\label{def-noise-reduction}

Reducing extraneous sources of variability can be accomplished by fixing
extraneous variables or through blocking. These actions reduce the
number of differences between the units under study.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Tension between Lab Settings and Reality}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Scientists and engineers are trained to control unwanted sources of
variability (or sources of error in the data generating process). This
creates a tension between what is observed in the study (under ``lab''
settings) and what is observed in practice (under ``real-world''
settings). This tension always exists, and the proper balance depends on
the goals of the researchers.

\end{tcolorbox}

Intuitively, the less variation in the response, the easier it is to
detect a signal. This leads naturally to saying that we could eliminate
extraneous variability if the groups were \emph{identical}; that results
in using the same subjects in multiple groups, resulting in taking
repeated measurements on the subjects --- blocking on the participant.

\begin{definition}[Blocking]\protect\hypertarget{def-blocking}{}\label{def-blocking}

Blocking is a way of minimizing the variability contributed by an
inherent characteristic that results in dependent observations. In some
cases, the blocks are the unit of observation which is sampled from a
larger population, and multiple observations are taken on each unit. In
other cases, the blocks are formed by grouping the units of observations
according to an inherent characteristic; in these cases that shared
characteristic can be thought of having a value that was sampled from a
larger population.

In both cases, the observed blocks can be thought of as a random sample;
within each block, we have multiple observations, and the observations
from the same block are more similar than observations from different
blocks.

\end{definition}

Blocking is useful when we can identify the nuisance characteristic in
advance of data collection. Blocking is a way of ensuring the treatment
groups are similar because the same subjects (with respect to this
particular characteristic) end up in each treatment group. We know that
random allocation eliminates confounding because it ensures that, on
average, treatment groups are similar. Blocking alone does not eliminate
confounding; it must be combined with randomization. However, if we
account for the blocking in the analysis, we are able to sharpen our
estimates because not only has balance occurred, we are able to explain
a portion of the variability within each treatment group.

\hypertarget{studies-with-repeated-measures}{%
\section{Studies with Repeated
Measures}\label{studies-with-repeated-measures}}

Studies that have repeated measurements taken on subjects typically
violate the condition of independence. While the above design concepts
apply broadly, the following terminology is specific to such studies.

\begin{definition}[Repeated
Measures]\protect\hypertarget{def-repeated-measures}{}\label{def-repeated-measures}

The phrase ``repeated measures'' refers to data for which the observed
responses can be grouped based on some nuisance variable (typically the
participant), and this grouping captures some inherent characteristic
such that observations within a group tend to be more alike than
observations across groups.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

The ``paired data'' setting (typically studied alongside the ``paired
t-test'') is a special case of using blocking with two observations per
block, resulting in repeated measures.

\end{tcolorbox}

As discussed in Example~\ref{exm-rm-enzyme-expanded}, the large
variability in fecal fat across participants relative to the variability
in fecal fat within participants can mask the treatment effect if not
accounted for appropriately. From a more theoretical perspective, this
difference in the variability induces a correlation structure among the
error in the responses.

\begin{definition}[Correlation
Structure]\protect\hypertarget{def-correlation-structure}{}\label{def-correlation-structure}

The correlation structure quantifies the strength and direction of the
relationship between the errors in the observed responses.

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Ignoring the correlation structure does not tend to affect the parameter
estimates, but it often affects the resulting standard errors, thereby
impacting confidence intervals and p-values.

\end{tcolorbox}

Unfortunately, there is no way to predict the impact of ignoring the
correlation structure on the standard errors of our estimates. As a
result, ignoring the correlation structure could result in confidence
intervals that are too wide or too narrow (and p-values that are too
large or too small). In short, ignoring the correlation structure in the
data can result in inappropriate inference. In order to obtain
appropriate inference, we need to account for the structure in our
model!

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

When the data is correlated, it must be taken into account in all
aspects of the analysis, from graphics to inference.

\end{tcolorbox}

In order to illustrate the impact of the correlation structure on an
analysis, let us revisit visualizing the data from the digestive enzymes
study. Recall that Figure~\ref{fig-rm-terminology-enzyme-plot}
visualized the data from Example~\ref{exm-rm-enzyme} when we assumed the
data came from 24 independent participants;
Figure~\ref{fig-rm-terminology-enzyme-plot-expanded} visualizes the same
data with the context from Example~\ref{exm-rm-enzyme-expanded} included
--- namely that there were only six participants, each measured under
multiple treatments. While there are some exceptions, notice how the
lines do not ``mix'' often; that is, some participants tend to have less
fecal fat than others, regardless of the form of the supplement given.
The differences between Figure~\ref{fig-rm-terminology-enzyme-plot} and
Figure~\ref{fig-rm-terminology-enzyme-plot-expanded} highlights that the
differences in the overall average response for each supplement type is
small relative to the variability across participants; as a result,
focusing on the overall average response alone
(Figure~\ref{fig-rm-terminology-enzyme-plot}) make it difficult to
detect differences between the supplement types. However, if we examine
the differences in the supplement types \emph{within each participant}
(Figure~\ref{fig-rm-terminology-enzyme-plot-expanded}), we can see that
for nearly every participant, the fecal fat is reduced with the
supplement (compared to placebo). It may not be immediately obvious if
there is a difference among the form of the supplement, but it seems
clear that having the supplement is better than not having it.
Accounting for the variability in the response across participants
changes our conclusions.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./images/fig-rm-terminology-enzyme-plot-expanded-1.pdf}

}

\caption{\label{fig-rm-terminology-enzyme-plot-expanded}Fecal fat
(g/day) of six participants enrolled in a cross-over study examining
four types of enzyme supplement. Data from the same participant
connected to illustrate the correlation structure.}

\end{figure}

Notice that when examining
Figure~\ref{fig-rm-terminology-enzyme-plot-expanded}, we were not
interested in comparing one participant to another; our focus was still
on comparing supplement types. The additional grouping was simply to
account for the relationship between the observations. Not all
``grouping'' variables are the same. We think about variables
differently depending on their role in the model for the data generating
process. Loosely, we can categorize factors as either fixed or random
effects.

\begin{definition}[Fixed
Effect]\protect\hypertarget{def-fixed-effect}{}\label{def-fixed-effect}

Fixed effects are terms in the model for which we are interested in both
the specific grouping levels, and we are interested in characterizing
the relationship between these levels and the response.

\end{definition}

The treatment/factor of interest is nearly always a fixed effect. In
addition, fixed effects can include anything we want to account for in
the model in such a way that if we were to repeat the study, the same
levels would be visible again. In Example~\ref{exm-rm-enzyme-expanded},
the form of the supplement is the fixed effect. If we were to repeat the
study, we would still expect to use these same four levels (placebo,
tablet, coated and uncoated capsule). And, we are interested in
examining the impact of supplement type on the resulting fecal fat.

\begin{definition}[Random
Effect]\protect\hypertarget{def-random-effect}{}\label{def-random-effect}

Random effects are terms in the model that capture the correlation
induced due to an inherent characteristic that varies across the
population. We are \emph{not} interested in the specific grouping
levels, and we either are not interested in the relationship with the
response.

\end{definition}

We are rarely interested in saying how random effects impact the
response; so, the treatment/factor of interest is rarely a random
effect. In Example~\ref{exm-rm-enzyme-expanded}, the participant is a
random effect. If we were to repeat the study, it is unlikely we would
use these same six individuals. Instead of quantifying the difference
between participants, we wanted to think about this variable because
groups of observations on the same participant are more alike than
observations across participants.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

In order to distinguish fixed and random effects in a model, think about
repeating the study; would you care if the levels of the factor were to
change? Are you interested in comparing the first level to the second?
If you answer ``yes'' to these questions, the factor is most likely a
fixed effect.

\end{tcolorbox}

The impacts of fixed and random effects in modeling are studied in
Chapter~\ref{sec-rm-mixed-models}. In brief, regression modeling is
about partitioning variability. When we are able to identify additional
sources of variability (like how the response varies across individuals
in the population), we are able to improve our estimation of the effects
of interest.

In Example~\ref{exm-rm-enzyme-expanded}, each observation is from a
unique combination of the participant on which it was observed and the
treatment assigned at that time (a cross-over study). This is not the
only form of a study that can result in repeated measurements. Other
common study designs include longitudinal studies, cross sectional
studies with subsampling, and studies that utilize cluster samples.

\begin{definition}[Cross-Over
Study]\protect\hypertarget{def-cross-over-study}{}\label{def-cross-over-study}

A cross-over study exposes each participant to multiple treatments.
Whenever possible, the order of the treatments is randomly determined.
This is equivalent to a randomized complete block design in which the
blocks are the participants. When the treatments are believed to have a
lingering effect, a wash-out period between treatments is used to
minimize the impact of previous treatments on the treatment the
participant is currently being exposed to.

\end{definition}

\begin{definition}[Randomized Complete Block
Design]\protect\hypertarget{def-rcbd}{}\label{def-rcbd}

A randomized complete block design is an example of a controlled
experiment utilizing blocking. Each treatment is randomized to
observations within blocks in such a way that every treatment is present
within the block and the same number of observations are assigned to
each treatment within each block.

\end{definition}

\begin{definition}[Longitudinal
Study]\protect\hypertarget{def-longitudinal-study}{}\label{def-longitudinal-study}

A longitudinal study repeatedly measures the response on each subject at
various points in time.

\end{definition}

All clinical trials follow subjects over time; a longitudinal study
measures the response of interest multiple times over the course of the
trial, resulting in repeated measures. In a longitudinal study, interest
is often in modeling the overall trajectory across subjects instead of
the trajectory for subjects individually. We are generally interested in
modeling the trajectory of the response over the time interval. For
example, we may be interested in the size of a tumor each month for the
first year after being treated with radiation.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

There are many similarities between longitudinal studies and time-series
data as each follows data over time. We do see some differences.
Time-series data is often focused on business applications while
longitudinal studies are more common in the biological sciences.
Time-series data often models a single ``stream'' that is quite long.
Longitudinal studies have several ``streams'' (one for each subject),
but these tend to be a bit shorter as we do not have constant follow-up.
In time-series data, it is often believed that the previous response is
useful in predicting the next response; in longitudinal data, we do
believe there is correlation among the errors in the model, but we do
not generally use the value of the previous observation itself in making
the next prediction but instead model with time as the predictor.

\end{tcolorbox}

\begin{definition}[Cross Sectional
Study]\protect\hypertarget{def-cross-sectional-study}{}\label{def-cross-sectional-study}

A cross sectional study considers data from a single snapshot in time.

\end{definition}

Cross sectional studies are generally what we imagine when we first
learn about data collection in an introductory course. We note that
individual observations in a cross sectional study may not have been
taken at exactly the same time, but it is believed that time does not
have an impact. For example, we might record the size of a tumor one
month after treatment with radiation. While the study might enroll
participants over the course of two years, we record the measurement at
one snapshot in time (one month after treatment) on each participant,
and we believe the participants from each year of the study should be
representative of the same group.

Longitudinal studies clearly involve repeated measures. Whether a cross
sectional study has repeated measures depends on the study design. In
the biological sciences, it is common to employ subsampling in cross
sectional studies.

\begin{definition}[Subsampling]\protect\hypertarget{def-subsampling}{}\label{def-subsampling}

Subsampling occurs when several measurements are taken on each subject
under the same treatment, possibly at unique locations.

\end{definition}

In a study with subsampling, the unit of observation is the subject
itself. As an example, a person may be assigned to a particular
treatment to improve eyesight. The subject's eyesight is then measured
in each eye; the person is the unit of measurement but we obtain two
measurements of the response (eyesight), one corresponding to each eye.
This is sometimes referred to as ``pseudo-replication,'' and many
researchers can mistakenly believe they have a larger sample size than
exists in reality; the observations recorded on the same subject are
related and should not be treated as independent observations. While it
is common to average the subsamples, doing so results in a loss of
information compared to modeling the correlation structure on the
original data.

\begin{definition}[Cluster
Samples]\protect\hypertarget{def-cluster-samples}{}\label{def-cluster-samples}

Stratified sampling divides a population into groups and samples from
within each group; in contrast, cluster sampling divides the population
into groups and randomly samples a few groups and takes measurements
from within the group.

\end{definition}

Observations from the same cluster are typically related. For example,
if we are studying the nutrition levels of citizens within a particular
state, we may sample specific counties first, and then sample
participants from within the chosen counties. Citizens from the same
county may be related since they have similar access to healthy food
options.

Regardless of the study design, when we recognize clusters of
observations which have some relationship beyond that explained by the
fixed effects of interest, we need to model that correlation structure.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

When we use the phrase ``repeated measures,'' we mean repeatedly
measuring the same response. All regression models make use of multiple
variables measured on the same subject. The models discussed in this
unit refer to measuring the same response repeatedly.

\end{tcolorbox}

When representing data from a repeated-measures study, it is useful to
convey the correlation structure in the graphic as well (as we did in
Figure~\ref{fig-rm-terminology-enzyme-plot-expanded}). This is not
always straight-forward. When the number of subjects in the study is not
overwhelming, a spaghetti plot can be useful. These are particularly
useful for studies that follow subjects over time and can be helpful in
illustrating the trend over time.

\begin{definition}[Spaghetti
Plot]\protect\hypertarget{def-spaghetti-plot}{}\label{def-spaghetti-plot}

A spaghetti plot is a scatterplot that displays the trends within a
subject, highlighting the correlation structure by connecting points
from the same subject.

\end{definition}

Other times, using color or other aesthetics is needed to illustrate the
correlation in the data.

\hypertarget{sec-rm-mixed-models}{%
\chapter{Mixed Effects Models}\label{sec-rm-mixed-models}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Chapter~\ref{sec-rm-terminology} discussed how studies with repeated
measures induce a correlation structure on the data. In this chapter and
the next, we consider two approaches for modeling repeated measures
data. In this chapter, we focus on a flexible modeling framework that
builds up the data generating process in stages, recognizing the
relationship between observations at each stage.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While we discuss these methods in the context of a linear model, these
methods can be extended to other modeling frameworks.

\end{tcolorbox}

\hypertarget{partitioning-variability}{%
\section{Partitioning Variability}\label{partitioning-variability}}

In order to motivate the modeling approach developed in this chapter, we
first discuss the various reasons the value of the response is not the
same for all observations. We can view regression models as trying to
explain why the response values differ across observations. The more
reasons we can put in place, the more variability we are able to
explain. By naming these sources of variability, we are able to
construct a corresponding model by building it up in stages.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Statistical models partition the variability in the response.

\end{tcolorbox}

While our discussion generalizes to many types of studies, it helps to
imagine measuring the response of interest at several points across time
for several subjects (Figure~\ref{fig-rm-mixed-models-variability}). For
example, imagine tracking a child's weight as they age. We can imagine
an overall trend --- as a child ages, their weight increases.

\begin{figure}

{\centering \includegraphics{./images/gifvarplotalt.jpg}

}

\caption{\label{fig-rm-mixed-models-variability}Illustration of various
sources of variability in the data generating process.}

\end{figure}

Further, we may want to allow this trend to depend on a key factors (or
other covariates). For example, we might posit that the child's weight
is higher for those in one medical treatment group compared to another.
Our research questions are generally at this stage --- characterizing
the impacts of fixed effects on the response (in this case, time and
treatment groups).

Of course, the trajectory of any particular subject (across their
repeated observations) will differ from the overall trajectory (and
differ from one subject to another). Some children naturally have larger
or smaller weights than others. At any point in time, a subject may have
a trajectory that is above average; others will have a trajectory that
is below average. This vertical shift or ``bump'' in the position of the
trajectory captures the biological variation between subjects. All
observations measured on the same subject will share a similar ``bump,''
creating a relationship between observations. This between-subject
variability is primarily what contributes to the correlation structure
in the response.

For any subject, the actual response is likely to not lie directly on
their individual trajectory. For example, a child's growth may not
follow a smooth growth-curve even if we model it in that way. This is
the result of natural biological fluctuations within the subject. As
such, observations measured close together in time can tend to be more
alike than those measured further apart in time. As an example, if a
child's weight is slightly above their individual trajectory at this
moment, it is likely to be above their trajectory an hour from now.
However, if a child's weight is slightly above their individual
trajectory at this moment, it does not really tell us anything about
whether their weight will be slightly above or below their individual
trajectory a year from now. Therefore, magnitudes of this within-subject
source of variability are thought to be related when close in time and
independent otherwise.

Finally, it is unlikely that the observed response is equal to the
\emph{actual} response as a result of measurement error. The weight of a
child will be subject to the accuracy and precision of the scale,
whether the subject was measured with or without clothing and shoes,
etc. The magnitude of such errors are thought to be independent of one
another.

What we are seeing in this discussion is that the ``error'' term we
considered in the linear model of the previous units is actually the
result of several sources of variability.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

Observations from the same block tend to be high or low (relative to the
average) together. These blocks could be due to repeated measures on the
same subject or observations clustered together due to some other
characteristic. While this ``between-subject'' variability induces a
correlation structure among observations from the same block, it is
common to think that observations from different blocks are independent.

Observations from within a block recorded close together in time are
likely to be related, inducing a ``within-subject'' correlation. It is
common to think that observations far apart in time are independent.

\end{tcolorbox}

Taking into account all sources of variability can result in a very
complex model (and this continues to be an area of active research). In
practice, we can make simplifying assumptions about the data generating
process that allows us to rely on a simpler construct. For example, we
may assume the data are collected far enough apart in time so that the
within-subject correlation is negligible.

\hypertarget{model-formulation}{%
\section{Model Formulation}\label{model-formulation}}

While our discussion above illustrates how the various sources of
error/variability build on one another to create the observed data, we
really worked backward. That is, we started with the overall trend and
decomposed it to arrive at the data observed. When we model, we want to
work in the other direction, building the model in stages. This is known
as a hierarchical model.

\begin{definition}[Hierarchical
Model]\protect\hypertarget{def-hierarchical-model}{}\label{def-hierarchical-model}

A hierarchical model breaks the data generating process into smaller
stages and posits a model for each stage. The stages are determined by
defining a hierarchy of units and thereby capturing the sources of
variability.

\end{definition}

While a hierarchical model could have an arbitrary number of stages, for
a large number of applications, it suffices to consider the model being
composed of only two stages: the individual-level (or within subject)
and the population-level (or between subject).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Remember, repeated measures can be the result of clusters of
observations; so, the term ``within-subject'' should be interpreted as
``within-block.''

\end{tcolorbox}

The individual-level stage posits a model for the observations within a
subject (or block). That is, this model characterizes the relationship
between the response and only those variables that change across
observations on a single subject. Conceptually, this is the model we
would construct if we only had data for a single subject. As a result,
this model only includes ``within-individual'' predictors --- those that
vary within a subject. In biological settings, this is most common when
measurements are taken across time; for example, following a child over
several years, the weight and height of the child will change. However,
it is unlikely that the highest level of education achieved by the
child's parents is likely to change over this time frame. Therefore,
when modeling the weight of the child, time and the child's height would
be within-individual predictors, and the education level of the parents
would be a ``between-subject'' predictor --- those that change from one
subject to another but remain constant for all observations from the
same subject.

\begin{definition}[Individual-Level
Model]\protect\hypertarget{def-individual-model}{}\label{def-individual-model}

The individual-level model characterizes the response for the \(i\)-th
subject (or block) only.

\end{definition}

Consider the study to investigate the impact of four methods of
delivering a pancreatic enzyme supplement
(Example~\ref{exm-rm-enzyme-expanded}). The individual-level model would
characterize the fecal fat (response) observed within each subject.
There are only a few variables in this data: the fecal fat, the
supplement type, and the participant identification. The fecal fat is
the response. The supplement type is the factor of interest, and in this
case, happens to be an individual-level variable as it changes
\emph{within} each participant.

Our individual-level model can make use of only the individual-level
predictors; in this case, it can only depend on the supplement type. We
would like to allow the fecal fat observed to depend on the type of
supplement; we also acknowledge the potential for measurement error.
This leads to a model of the form

\begin{equation}\protect\hypertarget{eq-rm-mixed-individual}{}{(\text{Fecal Fat})_i = \alpha_{0,i} + \alpha_{1,i} (\text{Tablet})_{i,j} + \alpha_{2,i} (\text{Coated})_{i,j} + \alpha_{3,i} (\text{Uncoated})_{i,j} + \varepsilon_{i,j}}\label{eq-rm-mixed-individual}\end{equation}

where here \(i\) is indexing the subject and \(j\) the observation
within each subject. If we only had data on a single subject, we would
compare the fecal fat for this subject under each of the supplement
types; that is exactly what Equation~\ref{eq-rm-mixed-individual} does
(while allowing for measurement error). We must describe the
distribution of random variables, such as the error term in
Equation~\ref{eq-rm-mixed-individual}, that appear in the model. It is
common to assume these errors are independent and follow a Normal
distribution; specifically,

\[\varepsilon_{i,j} \stackrel{\text{IID}}{\sim} N\left(0, \sigma^2\right).\]

What is unique about this model formulation is at this point, the
\emph{parameters} themselves (the \(\alpha\) terms in
Equation~\ref{eq-rm-mixed-individual}) are allowed to vary across
subjects. How those vary is determined by the population-level model.

\begin{definition}[Population-Level
Model]\protect\hypertarget{def-population-model}{}\label{def-population-model}

The population-level model characterizes how the \emph{parameters} of
the individual-level model vary across subjects (or blocks) in the
population.

\end{definition}

While we do not actually proceed in this way, conceptually, the
population-level model is constructed by (a) computing the parameter
estimates from the individual-level model, (b) treating those estimates
as responses in a new model, and (c) modeling those estimates as
functions of between-subject predictors.

This second-stage model makes use of between-subject predictors. For
Example~\ref{exm-rm-enzyme-expanded}, the participant identification is
the random effect and is constant across all observations from the same
subject (and is therefore a between-subject variable). To construct the
population-level model, we need to think about how the \(\alpha\) terms
from Equation~\ref{eq-rm-mixed-individual} might (if at all) vary across
individuals in the population. There is no single answer to how this
model is constructed; instead, the model should communicate the
researchers' beliefs about the data generating process. For example,
suppose we are willing to believe the following:

\begin{itemize}
\tightlist
\item
  The delivery of the supplement affects each individual in a similar
  way. That is, if the tablet is best for one person, we believe it is
  best for everyone. And, the only reason it may not appear this way in
  the observed data is due to random noise (which we captured by
  \(\varepsilon_{i,j}\) in the individual-level model).
\item
  The baseline level of fat in each person is fundamentally different.
  That is, due to diet, genetics etcetera, each person has a unique
  level of fat. Some people have more than others, and this amount
  varies randomly in the population, but it is centered on some value.
\end{itemize}

Again, we might disagree on whether these two assumptions are
appropriate, but once we agree on these assumptions, it guides the
model. The first assumption above says that the differences due to the
supplement types (the treatment effects) in the individual-level model
\(\left(\alpha_{1,i}, \alpha_{2,i}, \alpha_{3,i}\right)\) are the same
for every individual. That is,

\begin{equation}\protect\hypertarget{eq-rm-mixed-population1}{}{
\begin{aligned}
  \alpha_{1,i} &= \beta_1 \\
  \alpha_{2,i} &= \beta_2 \\
  \alpha_{3,i} &= \beta_3,
\end{aligned}
}\label{eq-rm-mixed-population1}\end{equation}

where \(\beta_1, \beta_2, \beta_3\) are unknown parameters. These
parameters capture the difference in the fecal fat, on average, between
supplement types.

The second assumption says that the the baseline level of fat under the
placebo arm, captured by the intercept term \(\alpha_{0,i}\) in
Equation~\ref{eq-rm-mixed-individual}, differs across subjects randomly.
That is,

\begin{equation}\protect\hypertarget{eq-rm-mixed-population2}{}{
\begin{aligned}
  \alpha_{0,i} &= \beta_0 + b_{0, i} \\
  b_{0,i} &\sim N\left(0, \sigma^2_0\right)
\end{aligned}
}\label{eq-rm-mixed-population2}\end{equation}

where \(\beta_0\) and \(\sigma^2_0\) are unknown parameters. While
\(\beta_0\) captures the overall fecal fat, on average, under the
placebo arm, \(\sigma^2_0\) captures the \emph{variability} in the
average fecal fat across individuals in the population taking the
placebo. Together, Equation~\ref{eq-rm-mixed-population1} and
Equation~\ref{eq-rm-mixed-population2} create the population-level
model.

In this population-level model, we are allowing the intercept term to
have a random component as a result of the variability across subjects
(hence the name ``random effect'' for the subject identifier in this
case). However, the impact of the treatments are fixed for all subjects
(hence the name ``fixed effect'' for the factor of interest in this
case).

Technically, the individual-level and population-level models together
fully specify the hierarchical model for the data generating process.
However, combining these into a single equation can be instructive.
Substituting the population-level model
(Equation~\ref{eq-rm-mixed-population1} and
Equation~\ref{eq-rm-mixed-population2}) into the individual-level model
(Equation~\ref{eq-rm-mixed-individual}), we have that the response is
characterized by

\begin{equation}\protect\hypertarget{eq-rm-mixed-full}{}{
\begin{aligned}
  (\text{Fecal Fat})_{i,j} 
    &= \left(\beta_0 + b_{0,i}\right) + \beta_1 (\text{Tablet})_{i,j} + \beta_2 (\text{Coated})_{i,j} + \beta_3 (\text{Uncoated})_{i,j} + \varepsilon_{i,j} \\
  \varepsilon_{i,j} &\stackrel{\text{IID}}{\sim} N\left(0, \sigma^2\right) \\
  b_{0,i} &\stackrel{\text{IID}}{\sim} N\left(0, \sigma^2_0\right).
\end{aligned}
}\label{eq-rm-mixed-full}\end{equation}

The idea that some of our effects (coefficients) in a model are fixed
(not allowed to vary across individuals) and some are random (allowed to
vary across individuals) leads to our description of this approach as a
mixed-effects model.

\begin{definition}[Mixed-Effects
Model]\protect\hypertarget{def-mixed-effects-model}{}\label{def-mixed-effects-model}

A mixed-effects model denotes a hierarchical model for which some
effects are fixed (not allowed to vary across subjects) and others are
random (allowed to vary across subjects).

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Specifying the mixed-effects model as a single model can help with
specifying the model in statistical software.

\end{tcolorbox}

Despite the fully parametric nature of a mixed-effects model, inference
on the fixed effects is generally carried out using large-sample theory.
Generally, we do not test the random effects; instead, we leave them in
the model to capture the correlation we believe is present in the data.
Recall that we concluded there was no evidence (p = 0.168) the average
fecal fat was associated with the type of supplement when we ignored the
correlation in the data. However, if we account for the correlation
using the mixed-effects model in Equation~\ref{eq-rm-mixed-full}, we
have strong evidence (p \textless{} 0.001) the average fecal fat differs
for at least one of the supplement types. Correctly accounting for the
correlation structure resulted in a more powerful analysis.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-warning-color-frame, breakable, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, opacitybacktitle=0.6]

Proper inference in mixed-effects models is debated among statisticians.
The two leading statistical software packages (SAS and R) disagree on
implementation. SAS provides default p-values for each fixed-effect
parameter; R does not.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Considerations when Building a Mixed-Effects Model}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Constructing a mixed-effects model can feel overwhelming at times. We
urge you to keep the following ideas in mind:

\begin{itemize}
\tightlist
\item
  Construct a model that preserves the behavior at the individual-level.
  This is generally the stage in which we have more scientific
  intuition.
\item
  Allow the behavior to vary across subjects, which corresponds to
  allowing the parameters to vary. Choose which parameters to vary based
  on discipline expertise.
\item
  The variation in the parameters naturally induces a correlation
  structure in the data.
\end{itemize}

\end{tcolorbox}

In the above discussion, we considered the error term at the
individual-level model to be independent and identically distributed. In
theory, we could allow a correlation to exist here as well; for example,
we may want observations closer together in time to be correlated. This
can dramatically increase the complexity of the model fit and often
requires custom software.

Before closing this chapter, we address the conditions of a
mixed-effects model. We have the same conditions on the individual-level
model as we do in classical regression models
(Definition~\ref{def-classical-regression}). Further, those conditions
can be assessed and relaxed in the same way. However, care must be taken
if bootstrapping is used to relax the Normality condition; often times,
the bootstrap algorithms need to be custom-written to take advantage of
the hierarchy in the data. The conditions on the random effects are not
easily assessed. In particular, the distributional assumption of
Normality for the random effects is rarely questioned.

In this text, our emphasis is on understanding and interpreting such
models. Other texts consider the theoretical underpinnings of such model
in more detail.

\hypertarget{sec-rm-gee}{%
\chapter{Generalized Estimating Equations}\label{sec-rm-gee}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

Chapter~\ref{sec-rm-terminology} discussed how studies with repeated
measures induce a correlation structure on the data. The previous
chapter addressed this correlation structure by developing a
hierarchical model in stages, capturing the correlation structure as a
by-product. That is, we did not model the correlation directly; instead,
by first describing the individual-level model and then allowing the
parameters of that model to vary across individuals in the population,
the correlation structure was handled naturally. In this chapter, we
consider an alternate approach where we focus on modeling the overall
average trajectory and the the correlation structure directly. While
very general, this approach is particularly popular in longitudinal
studies (Definition~\ref{def-longitudinal-study}).

\hypertarget{correlation-structrues}{%
\section{Correlation Structrues}\label{correlation-structrues}}

Chapter~\ref{sec-rm-terminology} defined the correlation structure as a
summary of the relationship among the errors in the response. In a mixed
effects model (Definition~\ref{def-mixed-effects-model}), we considered
the various sources of variability as contributing to the correlation
structure; in this chapter, we are interested in modeling the structure
directly. As a result, we are interested in the overall impact of the
sources of variability on this structure. By specifying this structure,
at least approximately, we are able to adjust the standard errors of our
parameter estimates to obtain appropriate inference.

We can think of the correlation on the error terms of our model as a
combination of between-subject and within-subject sources of
variability. While we may not be discussing the specific sources of
variability, they are just as important as before to help us determine
an appropriate form of the correlation structure. We are generally
willing to assume that observations from different subjects (or blocks)
are independent; therefore, when we describe the correlation structure
of the errors, we need only focus on the correlation of the observations
from the same subject. Further, we assume that the correlation structure
is the same for every subject. Therefore, there is only one correlation
structure to be specified, and it will be shared across all subjects.

Recall from your introductory course that the correlation coefficient
captures the strength and direction of the linear relationship between
two variables and must be a value between -1 and 1. If each subject has
five observations (for example), then we need to describe the
relationship between any pair of these five observations. That is, we
need \(\binom{5}{2} = 10\) correlation coefficients. We store these in a
matrix

\begin{equation}\protect\hypertarget{eq-rm-correlation}{}{\Gamma = \begin{pmatrix} 
1 & \rho_{1,2} & \rho_{1,3} & \rho_{1,4} & \rho_{1,5} \\
\cdot & 1 & \rho_{2,3} & \rho_{2,4} & \rho_{2,5} \\
\cdot & \cdot & 1 & \rho_{3,4} & \rho_{3,5} \\
\cdot & \cdot & \cdot & 1 & \rho_{4,5} \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix},}\label{eq-rm-correlation}\end{equation}

where the lower-half is determined from the upper-half since the
correlation between the \(i\)-th and \(j\)-th observations is the same
as the correlation between the \(j\)-th and \(i\)-th observations; that
is, \(\rho_{i,j} = \rho_{j,i}\).

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Properties of Correlation Matrices}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

Every correlation matrix has the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is a square matrix, and the dimension is determined by the number
  of observations within a subject.
\item
  It is symmetric (the transpose is the same as the original matrix).
  That is, \(\rho_{i,j} = \rho_{j,i}\).
\item
  The diagonal entries are always 1; any value is perfectly correlated
  with itself.
\item
  All off-diagonal elements must be between -1 and 1.
\end{enumerate}

\end{tcolorbox}

A correlation matrix is very similar to a variance-covariance matrix; in
fact, we can think of a correlation matrix as a standardized
variance-covariance matrix.

The correlation matrix in Equation~\ref{eq-rm-correlation} makes no
assumptions about the structure of the off-diagonal elements (other than
they are values between -1 and 1). This is known as an unstructured
form.

\begin{definition}[Unstructured Correlation
Structure]\protect\hypertarget{def-unstructured-correlation-structure}{}\label{def-unstructured-correlation-structure}

An unstructured correlation structure suggests that the correlation
between any two errors within a subject can take on any value. We only
require that it be a valid correlation matrix.

\end{definition}

If we think of each correlation as an additional parameter to estimate,
then we have just specified an additional \(\binom{m}{2}\) parameters to
our model, where \(m\) is the number of repeated observations on a
subject. We are essentially choosing not to place any structure on the
correlation matrix and allow the data to completely determine the
structure. This can be useful if we have no intuition about the sources
of variability; however, it requires a lot of data as we have added a
large number of parameters to the model.

As in any model, there is tension between specifying a model which is
flexible and one which is more tractable. We often impose some
simplifying structure on the correlation matrix. While there are several
possible structures, we discuss the most common. On the other extreme
from the unstructured correlation matrix discussed above is to assume
the observations within a subject are independent of one another.

\begin{definition}[Independence Correlation
Structure]\protect\hypertarget{def-independence-correlation-structure}{}\label{def-independence-correlation-structure}

An independence correlation structure suggests there is no correlation
among any of the error terms within a subject. If there are five
observations within a block, this has the form

\[\Gamma = \begin{pmatrix} 
1 & 0 & 0 & 0 & 0 \\
\cdot & 1 & 0 & 0 & 0 \\
\cdot & \cdot & 1 & 0 & 0 \\
\cdot & \cdot & \cdot & 1 & 0 \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\end{definition}

While we already assume that observations between subjects are
independent, the independence structure goes further and essentially
says all observations are independent. At first glance, this would seem
to revert back to the classical regression model, which we have already
established is inappropriate for repeated measures. However, we will
argue later that using such a structure does have some differences.

When we feel that observations from the same subject are associated
primarily because they are from the same subject, and that the order of
the observations within the subject is irrelevant, a compound symmetric
correlation structure is appropriate.

\begin{definition}[Compound Symmetric Correlation
Structure]\protect\hypertarget{def-compound-symmetric-correlation-structure}{}\label{def-compound-symmetric-correlation-structure}

A compound symmetric correlation structure, also known as an
\emph{exchangeable} correlation structure, suggests the correlation
between any two errors within a subject is equal. If there are five
observations within a block, this has the form

\[\Gamma = \begin{pmatrix} 
1 & \rho & \rho & \rho & \rho \\
\cdot & 1 & \rho & \rho & \rho \\
\cdot & \cdot & 1 & \rho & \rho \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\end{definition}

The compound symmetric structure adds only one additional parameter to
our model and actually models well many scenarios. When we do believe
that the order of the observations within a subject is important, and
that observations occurring closer together (generally in time) are more
highly correlated than observations further apart in time, an
autoregressive structure is appropriate.

\begin{definition}[Autoregressive Correlation
Structure]\protect\hypertarget{def-autoregressive-correlation-structure}{}\label{def-autoregressive-correlation-structure}

An autoregressive correlation structure suggests the correlation between
two observations diminishes as the observations get further apart
(generally, further apart in time). We generally only consider the
autoregressive structure of degree 1 here; if there are five
observations within a block, this has the form

\[\Gamma = \begin{pmatrix} 
1 & \rho & \rho^2 & \rho^3 & \rho^4 \\
\cdot & 1 & \rho & \rho^2 & \rho^3 \\
\cdot & \cdot & 1 & \rho & \rho^2 \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\end{definition}

The autoregressive structure is borrowed from the time-series
literature. It is primarily useful when we are taking observations
somewhat close together in time. Like the compound symmetric structure,
it only adds a single parameter to the model.

Regardless of which of the structures we believe is beneath the data, we
also assume stationarity.

\begin{definition}[Stationarity]\protect\hypertarget{def-stationarity}{}\label{def-stationarity}

The assumption of stationarity states that the correlation structure
does not depend on time, only the distance between the observations.

\end{definition}

Essentially, stationarity says that at no point does the structure
evolve as the study continues; that is, we cannot include a parameter
such as \(\rho^{(\text{time})}\).

Choosing an appropriate structure is often guided by discipline
expertise regarding how the sources of variability combine and impact
the relationship between the responses. When multiple sources of
variability have competing structures, we generally adopt the structure
of the ``dominant'' source. However, it turns out that the choice of the
structure need not have a large impact on the analysis --- simply
indicating in the analysis that there is a potential for correlation can
be sufficient. This is the idea behind the approach we describe.

\hypertarget{the-key-to-success-of-generalized-estimating-equations}{%
\section{The Key to Success of Generalized Estimating
Equations}\label{the-key-to-success-of-generalized-estimating-equations}}

In the previous section, we considered models for the correlation
structure that result from the combination of the various sources of
variability in the data generating process. This structure will be used
within the generalized estimating equation (GEE) approach.

\begin{definition}[Generalized Estimating Equations
(GEE)]\protect\hypertarget{def-gee}{}\label{def-gee}

Generalized estimating equations can be used to estimate the parameters
of a model while accounting for the correlation among observations. In
addition to specifying a model for the overall average response, a
``working'' structure is specified for the correlation of observations
from the same subject. The working structure is updated during the
estimation process and used to adjust the standard errors of the
parameter estimates in the mean model.

\end{definition}

Recall that our inference on the parameters requires us to compute the
variance-covariance matrix of the corresponding parameter estimates.
When a model is estimated using generalized estimating equations, the
model we specify for the correlation structure is known as the
``working'' correlation matrix; this is then updated using the observed
data when computing the variance-covariance matrix. As a result, the
variance-covariance matrix we use is not based solely on the specified
model but is a blend of the model specified and the observed data; this
is known as the robust sandwich estimator.

\begin{definition}[Robust Sandwich
Estimator]\protect\hypertarget{def-robust-sandwich-estimator}{}\label{def-robust-sandwich-estimator}

The robust sandwich estimator of the variance-covariance matrix of the
parameter estimates from the mean model balances the relationship
between the parameter estimates specified by the model (and the
``working'' correlation matrix) with the relationship suggested by the
observed data. Specifically, it has the form

\[\widehat{\boldsymbol{\Sigma}} = \widehat{\mathbf{U}} \widehat{\mathbf{U}}^{-1/2} \mathbf{R} \widehat{\mathbf{U}}^{-1/2} \widehat{\mathbf{U}}\]

where \(\mathbf{U}\) represents the model-based variance-covariance
matrix if the structure specified by the working correlation matrix were
completely correct, and \(\mathbf{R}\) represents the correction factor
estimated from the residuals (an empirical estimate).

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Big Idea}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, opacitybacktitle=0.6]

The use of the robust sandwich variance-covariance estimator is what
makes the GEE approach unique and so powerful.

\end{tcolorbox}

While the structure of \(\mathbf{U}\) is beyond the scope of this text,
we can think of it as what the computer does by default when we specify
a model under the classical conditions. Essentially, the use of the
robust sandwich estimator in the GEE framework means our posited
correlation structure need not be correct; it is okay if \(\mathbf{U}\)
is wrong. With enough data, the inference will be the same regardless of
the structure we choose. What we are really specifying is that there is
a potential for correlation among these observations. Of course, the
better the specified model for the correlation structure, the less
adjustment that is needed and the more powerful the results.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

It is the use of the robust-sandwich estimator that makes specifying the
``independent'' correlation structure different than assuming the
classical regression model. In classical regression, inference is based
on assuming independence. In a GEE framework, the correlation structure
will be updated after we assume independence.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, titlerule=0mm, bottomtitle=1mm, colback=white, coltitle=black, rightrule=.15mm, leftrule=.75mm, toprule=.15mm, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-note-color-frame, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, opacitybacktitle=0.6]

While we are discussing the use of the robust-sandwich estimator as a
way of adjusting for the correlation present, we note that this will
also adjust for violations in constant variance as a result.

\end{tcolorbox}

Consider the study to investigate the impact of four methods of
delivering a pancreatic enzyme supplement
(Example~\ref{exm-rm-enzyme-expanded}). In the GEE approach, we focus on
specifying a model for the overall mean response; that is, our modeling
is very similar to what we would do if we ignored the correlation
altogether:

\begin{equation}\protect\hypertarget{eq-rm-mixed-gee}{}{(\text{Fecal Fat})_{i,j} = \beta_0 + \beta_1 (\text{Tablet})_{i,j} + \beta_2 (\text{Coated})_{i,j} + \beta_3 (\text{Uncoated})_{i,j} + \varepsilon_{i,j}.}\label{eq-rm-mixed-gee}\end{equation}

Notice that our model does acknowledge that multiple observations were
collected on each subject (we used both an \(i\) and \(j\) index). The
difference between the GEE approach and the classical approach is that
we now specify a correlation structure for the errors. As described in
Example~\ref{exm-rm-enzyme-expanded}, there are four observations within
each participant. Since we believe that diet is the primary reason for
differences in fecal fat across individuals, the primary source of
variability is the participant. This leads us to posit the following
compound symmetric working correlation structure:

\[\Gamma = \begin{pmatrix} 1 & \rho & \rho & \rho \\
\cdot & 1 & \rho & \rho \\
\cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

We would then be sure to use the robust-sandwich estimator when
computing the standard errors of the parameter estimates in
Equation~\ref{eq-rm-mixed-gee}.

When we fit a model using generalized estimating equations, we are
fitting a semiparametric model --- specifying the model for the mean
response and the correlation structure. Inference on parameters is then
carried out using large-sample theory. Recall that we concluded there
was no evidence (p = 0.168) the average fecal fat was associated with
the type of supplement when we ignored the correlation in the data.
However, if we account for the correlation estimating the parameters
using generalized estimating equations with a compound-symmetric working
correlation matrix, we have strong evidence (p = 0.003) the average
fecal fat differs for at least one of the supplement types. Correctly
accounting for the correlation structure resulted in a more powerful
analysis.

\hypertarget{comparison-of-gee-and-mixed-effects-approaches}{%
\section{Comparison of GEE and Mixed Effects
Approaches}\label{comparison-of-gee-and-mixed-effects-approaches}}

While both mixed effects models and estimation via generalized
estimating equations account for the correlation structure, the two
approaches differ in many ways. The mixed effects modeling approach is
fully parametric, while estimating via GEE is semiparametric.

More broadly, these represent two different approaches to repeated
measures data: subject-specific and population-averaged.

\begin{definition}[Subject Specific
Models]\protect\hypertarget{def-subject-specific}{}\label{def-subject-specific}

Also known as conditional modeling, the subject-specific approach models
at the subject-level and addresses the correlation indirectly through
the inclusion of random effects.

\end{definition}

\begin{definition}[Population Averaged
Models]\protect\hypertarget{def-population-averaged}{}\label{def-population-averaged}

Also known as marginal modeling, the population-averaged approach posits
a model for the mean response directly and addresses the correlation
through directly modeling its structure.

\end{definition}

If you have more intuition about how the response should be
characterized at the individual level, then the subject-specific
(mixed-effects) approach will be more tractable. If you have more
intuition about how the response should be characterized on average
across individuals, then the population-averaged (estimated with GEE's)
approach will be more tractable. The GEE approach is fairly robust to
misspecifications of the working correlation structure and constant
variance conditions. The mixed-effects model approach allows us to
quantify the variability in an effect across subjects in the population.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Rosner2006}{}}%
Rosner, Bernard. 2006. \emph{Fundamentals of Biostatistics}. 6th ed. CA:
Thomson-Brooks/Cole.

\leavevmode\vadjust pre{\hypertarget{ref-Vittinghoff2012}{}}%
Vittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E.
McCulloch. 2012. \emph{Regression Methods in Biostatistics: Linear
Logistic, Survival, and Repeated Measures Models}. 2nd ed. NY:
Springer-Verlag.

\leavevmode\vadjust pre{\hypertarget{ref-Wickham2014}{}}%
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (10): 1--23.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{glossary}{%
\chapter{Glossary}\label{glossary}}

\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\dist}[1]{\stackrel{\text{#1}}{\sim}}
\providecommand{\ind}[1]{\mathbb{I}\left(#1\right)}
\providecommand{\bm}[1]{\mathbf{#1}}
\providecommand{\bs}[1]{\boldsymbol{#1}}
\providecommand{\Ell}{\mathcal{L}}
\providecommand{\indep}{\perp\negthickspace\negmedspace\perp}

The following key terms were defined in the text; each term is presented
with a link to where the term was first encountered in the text.

\begin{description}
\tightlist
\item[Alternate Characterization of the Classical Regression Model
(Definition~\ref{def-alternate-characterization})]
Under the classical regression conditions on the error term (see
Definition~\ref{def-classical-regression}), we can characterize the
classical regression model as
\end{description}

\[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i \stackrel{\text{Ind}}{\sim} N\left(\beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i, \sigma^2\right).\]

Here, the symbol \(\mid\) is read ``given'' and means that the
distribution of the response is specified after knowing the values of
the predictors. That is, the distribution of the response depends on
these variables.

\begin{description}
\tightlist
\item[Autoregressive Correlation Structure
(Definition~\ref{def-autoregressive-correlation-structure})]
An autoregressive correlation structure suggests the correlation between
two observations diminishes as the observations get further apart
(generally, further apart in time). We generally only consider the
autoregressive structure of degree 1 here; if there are five
observations within a block, this has the form
\end{description}

\[\Gamma = \begin{pmatrix} 
1 & \rho & \rho^2 & \rho^3 & \rho^4 \\
\cdot & 1 & \rho & \rho^2 & \rho^3 \\
\cdot & \cdot & 1 & \rho & \rho^2 \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\begin{description}
\tightlist
\item[Bernoulli Distribution
(Definition~\ref{def-bernoulli-distribution})]
Let \(X\) be a discrete random variable taking the value 0 or 1. \(X\)
is said to have a Bernoulli distribution with density
\end{description}

\[f(x) = \theta^x (1 - \theta)^{1 - x} \qquad x \in \{0, 1\},\]

where \(0 < \theta < 1\) is the probability that \(X\) takes the value
1.

\begin{itemize}
\tightlist
\item
  \(E(X) = \theta\)
\item
  \(Var(X) = \theta(1 - \theta)\)
\end{itemize}

We write \(X \sim Ber(\theta)\), which is read ``X has a Bernoulli
distribution with probability \(\theta\).''

\begin{description}
\tightlist
\item[Blocking (Definition~\ref{def-blocking})]
Blocking is a way of minimizing the variability contributed by an
inherent characteristic that results in dependent observations. In some
cases, the blocks are the unit of observation which is sampled from a
larger population, and multiple observations are taken on each unit. In
other cases, the blocks are formed by grouping the units of observations
according to an inherent characteristic; in these cases that shared
characteristic can be thought of having a value that was sampled from a
larger population.
\end{description}

In both cases, the observed blocks can be thought of as a random sample;
within each block, we have multiple observations, and the observations
from the same block are more similar than observations from different
blocks.

\begin{description}
\tightlist
\item[Bootstrapping (Definition~\ref{def-bootstrapping})]
A process of constructing a sampling distribution of the parameter
estimates through resampling. The observed data is resampled repeatedly,
and the parameters of interest are estimated in each resample. The
distribution of these estimates across the resamples is then used as an
empirical model of the corresponding sampling distributions.
\item[Case Resampling Bootstrap
(Definition~\ref{def-case-resampling-bootstrap})]
Suppose we observe a sample of size \(n\) and use the data to compute
the least squares estimates \(\widehat{\boldsymbol{\beta}}\) for the
parameters in the model
\end{description}

\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]

The case resampling bootstrap proceeds according to the following
algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a random sample of size \(n\) (with replacement) of the raw data
  (keeping all variables from the same observation together); denote the
  \(i\)-th selected response and predictors \((\text{Response})_i^*\)
  and \((\text{Predictor } j)_i^*\), respectively.
\item
  Obtain the least squares estimates \(\widehat{\boldsymbol{\alpha}}\)
  by finding the values of \(\boldsymbol{\alpha}\) that minimize
\end{enumerate}

\[\sum_{i=1}^{n} \left((\text{Response})_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i^*\right)^2.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Repeat steps 1-2 \(m\) times.
\end{enumerate}

We often take \(m\) to be large (at least 1000). After each pass through
the algorithm, we retain the least squares estimates
\(\widehat{\boldsymbol{\alpha}}\) from the resample. The distribution of
these estimates across the resamples is a good empirical model for the
sampling distribution of the original least squares estimates.

\begin{description}
\tightlist
\item[Categorical Variable (Definition~\ref{def-categorical-variable})]
Also called a ``qualitative variable,'' a measurement on a subject which
denotes a grouping or categorization.
\item[Central Limit Theorem (Definition~\ref{def-clt})]
Let \(Y_1, Y_2, \dotsc, Y_n\) be independent and identically distributed
random variables with finite mean \(\mu\) and variance \(\sigma^2\).
Then, as \(n\) approaches infinity, the distribution of the ratio
\end{description}

\[\frac{\sqrt{n}\left(\bar{Y} - \mu\right)}{\sigma}\]

approaches that of a Standard Normal random variable.

\begin{description}
\tightlist
\item[Chi-Square Distribution
(Definition~\ref{def-chi-square-distribution})]
Let \(X\) be a continuous random variable. \(X\) is said to have a
Chi-Square distribution if the density is given by
\end{description}

\[f(x) = \frac{1}{2^{\nu/2}\Gamma (\nu/2)}\;x^{\nu/2-1}e^{-x/2} \qquad x > 0,\]

where \(\nu > 0\) is the degrees of freedom.

We write \(X \sim \chi^2_{\nu}\), which is read ``X has a Chi-Square
distribution with \(\nu\) degrees of freedom.''

\begin{description}
\tightlist
\item[Classical Regression (Conditions on Predictors)
(Definition~\ref{def-classical-regression-cont})]
The classical regression model
(Definition~\ref{def-classical-regression}) places the following
conditions on the predictors:
\end{description}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each predictor is measured without error.
\item
  Each predictor has an additive linear effect on the response.
\end{enumerate}

\begin{description}
\tightlist
\item[Classical Regression Model
(Definition~\ref{def-classical-regression})]
In the ``classical regression model,'' we place the following four
conditions on the distribution of the error \(\varepsilon_i\):
\end{description}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The average error across all levels of the predictors is 0;
  mathematically, we write
  \(E\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = 0\).
\item
  The variance of the errors is constant across all levels of the
  predictors; mathematically, we write
  \(Var\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = \sigma^2\)
  for some unknown constant \(\sigma^2 > 0\). This is sometimes referred
  to as homoskedasticity.
\item
  The error terms are independent; in particular, the magnitude of the
  error for one observation does not influence the magnitude of the
  error for any other observation.
\item
  The distribution of the errors follows a Normal distribution with the
  above mean and variance.
\end{enumerate}

\begin{description}
\tightlist
\item[Cluster Samples (Definition~\ref{def-cluster-samples})]
Stratified sampling divides a population into groups and samples from
within each group; in contrast, cluster sampling divides the population
into groups and randomly samples a few groups and takes measurements
from within the group.
\item[Codebook (Definition~\ref{def-codebook})]
Also called a ``data dictionary,'' a codebook provides complete
information regarding the variables contained within a dataset.
\item[Compound Symmetric Correlation Structure
(Definition~\ref{def-compound-symmetric-correlation-structure})]
A compound symmetric correlation structure, also known as an
\emph{exchangeable} correlation structure, suggests the correlation
between any two errors within a subject is equal. If there are five
observations within a block, this has the form
\end{description}

\[\Gamma = \begin{pmatrix} 
1 & \rho & \rho & \rho & \rho \\
\cdot & 1 & \rho & \rho & \rho \\
\cdot & \cdot & 1 & \rho & \rho \\
\cdot & \cdot & \cdot & 1 & \rho \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\begin{description}
\tightlist
\item[Confidence Interval (Definition~\ref{def-confidence-interval})]
An interval (range of values) estimate of a parameter that incorporates
the variability in the statistic. The process of constructing \(k\)\%
confidence intervals results in them containing the parameter of
interest in \(k\)\% of \emph{repeated} studies. The value of \(k\) is
called the \emph{confidence level}.
\item[Confidence Interval for Parameters Under Classical Model
(Definition~\ref{def-classical-ci})]
Under the classical regression conditions
(Definition~\ref{def-classical-regression}), a \(100c\)\% confidence
interval for the parameter \(\beta_j\) is given by
\end{description}

\[\widehat{\beta}_j \pm t_{n-p-1, 0.5(1+c)} \sqrt{Var\left(\widehat{\beta}_j\right)}.\]

where \(t_{n-p-1, 0.5(1+c)}\) is the \(0.5(1+c)\) quantile from the
\(t_{n-p-1}\) distribution, known as the critical value for the
confidence interval.

\begin{description}
\tightlist
\item[Confounding (Definition~\ref{def-confounding})]
When the effect of a variable on the response is mis-represented due to
the presence of a third, potentially unobserved, variable known as a
confounder.
\item[Correlation Structure
(Definition~\ref{def-correlation-structure})]
The correlation structure quantifies the strength and direction of the
relationship between the errors in the observed responses.
\item[Cross Sectional Study
(Definition~\ref{def-cross-sectional-study})]
A cross sectional study considers data from a single snapshot in time.
\item[Cross-Over Study (Definition~\ref{def-cross-over-study})]
A cross-over study exposes each participant to multiple treatments.
Whenever possible, the order of the treatments is randomly determined.
This is equivalent to a randomized complete block design in which the
blocks are the participants. When the treatments are believed to have a
lingering effect, a wash-out period between treatments is used to
minimize the impact of previous treatments on the treatment the
participant is currently being exposed to.
\item[Cumulative Distribution Function (CDF) (Definition~\ref{def-cdf})]
Let \(X\) be a random variable; the cumulative distribution function
(CDF) is defined as
\end{description}

\[F(u) = Pr(X \leq u).\]

For a continuous random variable, we have that

\[F(u) = \int_{-\infty}^{u} f(x) dx\]

implying that the density function is the derivative of the CDF. For a
discrete random variable

\[F(u) = \sum_{x \leq u} f(x).\]

\begin{description}
\tightlist
\item[Density Function (Definition~\ref{def-density-function})]
A density function \(f\) relates the potential values of a random
variable \(X\) with the probability those values occur. For a
\emph{continuous} random variable, the probability the random variable
\(X\) falls within an interval \((a, b)\) is given by
\end{description}

\[Pr(a \leq X \leq b) = \int_{a}^{b} f(x) dx.\]

For a \emph{discrete} random variable, the probability the random
variable \(X\) is equal to the value \(u\) is given by

\[Pr(X = u) = f(u).\]

\begin{description}
\tightlist
\item[Distribution (Definition~\ref{def-distribution})]
The pattern of variability corresponding to a set of values.
\item[Estimate of the Variance of the Errors
(Definition~\ref{def-estimate-sigma2})]
The unknown variance in the linear model, which captures the variability
in the response for any set of predictors (also called the residual
variance), is estimated by
\end{description}

\[\widehat{\sigma}^2 = \frac{1}{n-p-1} \sum\limits_{i=1}^{n} \left((\text{Response})_i - \widehat{\beta}_0 - \sum\limits_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_{i}\right)^2.\]

\begin{description}
\tightlist
\item[F-Distribution (Definition~\ref{def-f-distribution})]
Let \(X\) be a continuous random variable. \(X\) is said to have an
F-distribution if the density is given by
\end{description}

\[f(x) = \frac{\Gamma((r + s)/2)}{(\Gamma(r/2) \Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \qquad x > 0,\]

where \(r,s > 0\) are the numerator and denominator degrees of freedom,
respectively.

We write \(X \sim F_{r, s}\), which is read ``X has an F-distribution
with r numerator degrees of freedom and s denominator degrees of
freedom.''

\begin{description}
\tightlist
\item[Fixed Effect (Definition~\ref{def-fixed-effect})]
Fixed effects are terms in the model for which we are interested in both
the specific grouping levels, and we are interested in characterizing
the relationship between these levels and the response.
\item[General Linear Hypothesis
(Definition~\ref{def-general-linear-hypothesis})]
The general linear hypothesis framework refers to testing hypotheses of
the form
\end{description}

\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m} \qquad \text{vs.} \qquad H_1: \mathbf{K}\boldsymbol{\beta} \neq \mathbf{m}\]

where

\begin{itemize}
\tightlist
\item
  \(\boldsymbol{\beta}\) is the \((p+1)\)-length vector of the
  parameters (includes the intercept),
\item
  \(\mathbf{K}\) is an \(r\)-by-\((p+1)\) matrix that specifies the
  linear combinations defining the hypothesis of interest, and
\item
  \(\mathbf{m}\) is a vector of length \(r\) specifying the null values,
  the value of each linear combination under the null hypothesis (often
  a vector of 0's).
\end{itemize}

\begin{description}
\tightlist
\item[General Linear Model (Definition~\ref{def-general-linear-model})]
The general linear model views the response (outcome) as a linear
combination of several predictors:
\end{description}

\[
\begin{aligned}
  (\text{Response})_i 
    &= \beta_0 + \beta_1 (\text{Predictor 1})_{i} + \beta_2 (\text{Predictor 2})_{i} + \dotsb + 
      \beta_p (\text{Predictor } p)_{i} + \varepsilon_i \\
    &= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i} + \varepsilon_i
\end{aligned}
\]

where \(n\) is the number of subjects in the sample, \(p < n\) is the
number of predictors in the model, and \(\varepsilon_i\) is a random
variable that captures the error in the response.

\begin{description}
\tightlist
\item[Generalized Estimating Equations (GEE) (Definition~\ref{def-gee})]
Generalized estimating equations can be used to estimate the parameters
of a model while accounting for the correlation among observations. In
addition to specifying a model for the overall average response, a
``working'' structure is specified for the correlation of observations
from the same subject. The working structure is updated during the
estimation process and used to adjust the standard errors of the
parameter estimates in the mean model.
\item[Hierarchical Model (Definition~\ref{def-hierarchical-model})]
A hierarchical model breaks the data generating process into smaller
stages and posits a model for each stage. The stages are determined by
defining a hierarchy of units and thereby capturing the sources of
variability.
\item[Independence Correlation Structure
(Definition~\ref{def-independence-correlation-structure})]
An independence correlation structure suggests there is no correlation
among any of the error terms within a subject. If there are five
observations within a block, this has the form
\end{description}

\[\Gamma = \begin{pmatrix} 
1 & 0 & 0 & 0 & 0 \\
\cdot & 1 & 0 & 0 & 0 \\
\cdot & \cdot & 1 & 0 & 0 \\
\cdot & \cdot & \cdot & 1 & 0 \\
\cdot & \cdot & \cdot & \cdot & 1 \end{pmatrix}.\]

\begin{description}
\tightlist
\item[Indicator Variables (Definition~\ref{def-indicator-variables})]
Also called ``dummy variables,'' these are a set of binary variables
that capture the grouping defined by a categorical variable for
regression modeling.
\item[Individual-Level Model (Definition~\ref{def-individual-model})]
The individual-level model characterizes the response for the \(i\)-th
subject (or block) only.
\item[Interaction (Definition~\ref{def-interaction})]
An interaction term allows the effect of a predictor on the response to
depend on the value of a second predictor (capturing an effect
modification).
\end{description}

\begin{itemize}
\tightlist
\item
  The interaction term is created by adding the product of the two
  predictors under consideration to the model.
\end{itemize}

\begin{description}
\tightlist
\item[Intercept (Definition~\ref{def-intercept})]
The population intercept, denoted \(\beta_0\), is the \emph{mean}
response when all predictors take the value zero.
\item[Large Sample Model for the Sampling Distribution of the Least
Squares Estimates
(Definition~\ref{def-ls-sampling-distribution-large-samples})]
Suppose the classical regression conditions hold, with the exception of
the errors following a Normal distribution. As the sample size gets
large, we have that the distribution of the ratio
\end{description}

\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim N(0, 1)\]

for all \(j = 0, 1, \dotsc, p\). Further, under the null hypothesis

\[H_0: \mathbf{K}\boldsymbol{\beta} = \mathbf{m}\]

we have

\[\left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim \chi^2_r.\]

\begin{description}
\tightlist
\item[Large Sample Theory (Definition~\ref{def-large-sample-theory})]
The phrase ``large sample theory'' (or ``asymptotics'') is used to
describe a scenario when the model for the sampling distribution (or
null distribution) of an estimate (or standardized statistic) can be
approximated as the sample size becomes infinitely large. That is, as
the sample size approaches infinity, the sampling distribution (or null
distribution) can be easily modeled using a known probability
distribution.
\item[Least Squares Estimation (Definition~\ref{def-least-squares})]
The method of least squares may be used to estimate the coefficients
(parameters) of a linear model. In particular, we choose the values of
the coefficients that minimize
\end{description}

\[\sum\limits_{i=1}^{n} \left((\text{Response})_i - \beta_0 - \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i}\right)^2.\]

The resulting ``least squares'' estimates are denoted
\(\widehat{\beta}_0, \widehat{\beta}_1, \dotsc, \widehat{\beta}_p\).

\begin{description}
\tightlist
\item[Linear Model (Definition~\ref{def-linear-model})]
A model is said to be linear if it can be expressed as a linear
combination of the \emph{parameters}. That is, the linearity does not
refer to the form of the predictors but the form of the parameters.
\item[Linear Spline (Definition~\ref{def-linear-spline})]
A linear spline is a continuous piecewise linear function.
\item[Longitudinal Study (Definition~\ref{def-longitudinal-study})]
A longitudinal study repeatedly measures the response on each subject at
various points in time.
\item[Mean and Variance of a Random Variable
(Definition~\ref{def-rv-mean-variance})]
Suppose \(X\) is a random variable with density function \(f\). If \(X\)
is a continuous random variable, then the mean and variance are given by
\end{description}

\[
\begin{aligned}
  E(X) &= \int x f(x) dx \\
  Var(X) &= \int \left(x - E(X)\right)^2 f(x) dx.
\end{aligned}
\]

If \(X\) is a discrete random variable, then the mean and variance are
given by

\[
\begin{aligned}
  E(X) &= \sum x f(x) \\
  Var(X) &= \sum \left(x - E(X)\right)^2 f(x).
\end{aligned}
\]

\begin{description}
\tightlist
\item[Mixed-Effects Model (Definition~\ref{def-mixed-effects-model})]
A mixed-effects model denotes a hierarchical model for which some
effects are fixed (not allowed to vary across subjects) and others are
random (allowed to vary across subjects).
\item[Model for the Null Distribution with the General Linear Hypothesis
(Definition~\ref{def-general-linear-hypothesis-null})]
Let \(\widehat{\boldsymbol{\beta}}\) be the \((p+1)\) vector of
estimates for the parameter vector \(\boldsymbol{\beta}\), and let the
estimates have variance-covariance matrix \(\boldsymbol{\Sigma}\).
Assuming the null hypothesis
\end{description}

\[H_0: \mathbf{K} \boldsymbol{\beta} = \mathbf{m}\]

is true, under the conditions of the classical regression model
(Definition~\ref{def-classical-regression})

\[(1/r) \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right)^\top \left(\mathbf{K}\widehat{\boldsymbol{\Sigma}}\mathbf{K}^\top\right)^{-1} \left(\mathbf{K}\widehat{\boldsymbol{\beta}} - \mathbf{m}\right) \sim F_{r, n-p-1}.\]

\begin{description}
\tightlist
\item[Multicollinearity (Definition~\ref{def-multicollinearity})]
When two predictors are highly correlated with one another, we say that
there is multicollinearity in the model.
\item[Nonparametric Model (Definition~\ref{def-nonparametric-model})]
A nonparametric model is unable to characterize the response using a
finite set of parameters; for our purposes, this generally means the
model makes no assumptions about the structure of the underlying
distribution of the response given the predictors. Only minimal
assumptions (such as independence between observations) are imposed.
\item[Normal (Gaussian) Distribution
(Definition~\ref{def-normal-distribution})]
Let \(X\) be a continuous random variable. \(X\) is said to have a
Normal (or Gaussian) distribution if the density is given by
\end{description}

\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} \qquad -\infty < x < \infty,\]

where \(\mu\) is any real number and \(\sigma^2 > 0\).

\begin{itemize}
\tightlist
\item
  \(E(X) = \mu\)
\item
  \(Var(X) = \sigma^2\)
\end{itemize}

We write \(X \sim N\left(\mu, \sigma^2\right)\), which is read ``X has a
Normal distribution with mean \(\mu\) and variance \(\sigma^2\).'' This
short-hand implies the density above.

\begin{description}
\tightlist
\item[Numeric Variable (Definition~\ref{def-numeric-variable})]
Also called a ``quantitative variable,'' a measurement on a subject
which takes on a numeric value \emph{and} for which ordinary arithmetic
makes sense.
\item[Observational Study (Definition~\ref{def-observational-study})]
A study in which each participant ``self-selects'' into one of groups
being compared in the study. The phrase ``self-selects'' is used very
loosely here and can include studies in which the groups are defined by
an inherent characteristic, the groups are determined according to a
non-random mechanism, and each participant chooses the group to which
they belong.
\item[P-Value (Definition~\ref{def-pvalue})]
The probability, assuming the null hypothesis is true, that we would
observe a statistic, from sampling variability alone, as extreme or more
so as that observed in our sample. This quantifies the strength of
evidence against the null hypothesis. Smaller values indicate stronger
evidence.
\item[P-Value for Testing if Parameter Belongs in Model Under Classical
Model (Definition~\ref{def-classical-p})]
Under the classical regression conditions
(Definition~\ref{def-classical-regression}), the p-value for testing the
hypotheses
\end{description}

\[H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0\]

is given by

\[Pr\left(\lvert T\rvert > \lvert\frac{\widehat{\beta}_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}}\rvert\right)\]

where \(T \sim t_{n-p-1}\).

\begin{description}
\tightlist
\item[Parameter (Definition~\ref{def-parameter})]
Numeric quantity which summarizes the distribution of a variable within
the \emph{population} of interest. Generally denoted by Greek letters in
statistical formulas.
\item[Parametric Model (Definition~\ref{def-parametric-model})]
A parametric model characterizes the distribution of the response using
a finite set of parameters; for our purposes, this generally means the
model \emph{fully} characterize the distribution of the response given
the predictors.
\item[Population (Definition~\ref{def-population})]
The collection of subjects we would like to say something about.
\item[Population Averaged Models
(Definition~\ref{def-population-averaged})]
Also known as marginal modeling, the population-averaged approach posits
a model for the mean response directly and addresses the correlation
through directly modeling its structure.
\item[Population-Level Model (Definition~\ref{def-population-model})]
The population-level model characterizes how the \emph{parameters} of
the individual-level model vary across subjects (or blocks) in the
population.
\item[Random Effect (Definition~\ref{def-random-effect})]
Random effects are terms in the model that capture the correlation
induced due to an inherent characteristic that varies across the
population. We are \emph{not} interested in the specific grouping
levels, and we either are not interested in the relationship with the
response.
\item[Random Variable (Definition~\ref{def-random-variable})]
A random variable represents a measurement that will be collected and
for which the value cannot be predicted with certainty; they are
generally represented with a capital letter. Continuous random variables
represent quantitative measurements while discrete random variables
represent qualitative measurements.
\item[Randomization (Definition~\ref{def-randomization})]
Randomization can refer to random \emph{selection} or random
\emph{allocation}.
\end{description}

Random selection refers to the use of a random mechanism to select units
from the population. Random selection minimizes bias.

Random allocation refers to the use of a random mechanism when assigning
units to a specific treatment group in a controlled experiment. Random
allocation eliminates confounding and permits causal interpretations.

\begin{description}
\tightlist
\item[Randomized Clinical Trial
(Definition~\ref{def-randomized-clinical-trial})]
Also called a ``controlled experiment,'' a study in which each
participant is randomly assigned to one of the groups being compared in
the study.
\item[Randomized Complete Block Design (Definition~\ref{def-rcbd})]
A randomized complete block design is an example of a controlled
experiment utilizing blocking. Each treatment is randomized to
observations within blocks in such a way that every treatment is present
within the block and the same number of observations are assigned to
each treatment within each block.
\item[Reduction of Noise (Definition~\ref{def-noise-reduction})]
Reducing extraneous sources of variability can be accomplished by fixing
extraneous variables or through blocking. These actions reduce the
number of differences between the units under study.
\item[Reference Group (Definition~\ref{def-reference-group})]
The group defined by having all indicator variables for a particular
categorical variable set to zero.
\item[Repeated Measures (Definition~\ref{def-repeated-measures})]
The phrase ``repeated measures'' refers to data for which the observed
responses can be grouped based on some nuisance variable (typically the
participant), and this grouping captures some inherent characteristic
such that observations within a group tend to be more alike than
observations across groups.
\item[Replication (Definition~\ref{def-replication})]
Replication results from taking measurements on different units (or
subjects) for which you expect the results to be similar. That is, any
variability across the units is due to natural variability within the
population.
\item[Residual (Definition~\ref{def-residual})]
A residual for the \(i\)-th observation is the difference between an
observed value and the predicted response:
\end{description}

\[
\begin{aligned}
  (\text{Residual})_i 
    &= (\text{Observed Response})_i - (\text{Predicted Response})_i \\
    &= (\text{Response})_i - \left(\widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i\right).
\end{aligned}
\]

\begin{description}
\tightlist
\item[Residual Bootstrap (Definition~\ref{def-residual-bootstrap})]
Suppose we observe a sample of size \(n\) and use the data to compute
the least squares estimates \(\widehat{\boldsymbol{\beta}}\) for the
parameters in the model
\end{description}

\[(\text{Response})_i = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i + \varepsilon_i.\]

The residual bootstrap proceeds according to the following algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the residuals
\end{enumerate}

\[(\text{Residuals})_i = (\text{Response})_i - (\text{Predicted Response})_i\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Take a random sample of size \(n\) (with replacement) of the
  residuals; call these values \(e_1^*, \dotsc, e_n^*\).
\item
  Form ``new'' responses \(y_1^*, \dotsc, y_n^*\) according to
\end{enumerate}

\[y_i^* = \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_i + e_i^*.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Obtain the least squares estimates \(\widehat{\boldsymbol{\alpha}}\)
  by finding the values of \(\boldsymbol{\alpha}\) that minimize
\end{enumerate}

\[\sum_{i=1}^{n} \left(y_i^* - \alpha_0 - \sum_{j=1}^{p} \alpha_j (\text{Predictor } j)_i\right)^2.\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Repeat steps 2-4 \(m\) times.
\end{enumerate}

We often take \(m\) to be large (at least 1000). After each pass through
the algorithm, we retain the least squares estimates
\(\widehat{\boldsymbol{\alpha}}\) from the resample. The distribution of
these estimates across the resamples is a good empirical model for the
sampling distribution of the original least squares estimates.

\begin{description}
\tightlist
\item[Response Variable (Definition~\ref{def-response})]
Also called the ``outcome,'' this is the primary variable of interest in
the research question; it is the variable we either want to explain or
predict.
\item[Restricted Cubic Spline
(Definition~\ref{def-restricted-cubic-spline})]
A restricted cubic spline is a continuous function comprised of
piecewise cubic polynomials for which the tails of the spline have been
restricted to be linear.
\item[Robust Sandwich Estimator
(Definition~\ref{def-robust-sandwich-estimator})]
The robust sandwich estimator of the variance-covariance matrix of the
parameter estimates from the mean model balances the relationship
between the parameter estimates specified by the model (and the
``working'' correlation matrix) with the relationship suggested by the
observed data. Specifically, it has the form
\end{description}

\[\widehat{\boldsymbol{\Sigma}} = \widehat{\mathbf{U}} \widehat{\mathbf{U}}^{-1/2} \mathbf{R} \widehat{\mathbf{U}}^{-1/2} \widehat{\mathbf{U}}\]

where \(\mathbf{U}\) represents the model-based variance-covariance
matrix if the structure specified by the working correlation matrix were
completely correct, and \(\mathbf{R}\) represents the correction factor
estimated from the residuals (an empirical estimate).

\begin{description}
\tightlist
\item[Sample (Definition~\ref{def-sample})]
The collection of subjects for which we actually obtain measurements
(data).
\item[Sampling Distribution of the Least Squares Estimates
(Definition~\ref{def-ls-sampling-distribution})]
Under the classical regression conditions
(Definition~\ref{def-classical-regression}), we have that
\end{description}

\[\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim t_{n - p - 1}.\]

The denominator \(\sqrt{Var\left(\widehat{\beta}_j\right)}\) is known as
the \emph{standard error} of the estimate \(\widehat{\beta}_j\). This
formula holds for all \(j = 0, 1, \dotsc, p\).

\begin{description}
\tightlist
\item[Semiparametric Linear Model
(Definition~\ref{def-semiparametric-linear-model})]
Suppose we no longer require that the error terms follow a Normal
distribution; however, we do continue to impose the remaining conditions
of the classical regression model. Then, our model could be written as
\end{description}

\[
\begin{aligned}
  E\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i \\
  Var\left[(\text{Response})_i \mid (\text{Predictors 1 through } p)_i\right]
    &= \sigma^2
\end{aligned}
\]

where the responses are independent of one another given the predictors.

\begin{description}
\tightlist
\item[Semiparametric Model (Definition~\ref{def-semiparametric-model})]
A semiparametric model specifies some components of the underlying
distribution of the response using a finite set of parameters but does
not fully characterize the distribution. This generally means that we
may specify the mean and/or variance of the response given the
predictors, but we do not characterize the distributional family of the
response.
\item[Slope (Definition~\ref{def-slope})]
The coefficient for the \(j\)-th predictor, denoted \(\beta_j\), is the
change in the mean response associated with a one unit increase in
Predictor \(j\), \emph{holding all other predictors fixed}.
\item[Spaghetti Plot (Definition~\ref{def-spaghetti-plot})]
A spaghetti plot is a scatterplot that displays the trends within a
subject, highlighting the correlation structure by connecting points
from the same subject.
\item[Spline (Definition~\ref{def-spline})]
A spline is a continuous piecewise polynomial used to model curvature.
The points that define the piecewise components are called \emph{knot
points}; the functional form is allowed to change at the knot points.
\item[Stationarity (Definition~\ref{def-stationarity})]
The assumption of stationarity states that the correlation structure
does not depend on time, only the distance between the observations.
\item[Statistic (Definition~\ref{def-statistic})]
Numeric quantity which summarizes the distribution of a variable within
the observed \emph{sample}.
\item[Statistical Inference (Definition~\ref{def-inference})]
The process of using a sample to characterize some aspect of the
underlying population.
\item[Subgroup Analysis (Definition~\ref{def-subgroup-analysis})]
Refers to repeating a specified analysis (e.g., regression model) within
various levels of a categorical predictor.
\end{description}

\begin{itemize}
\tightlist
\item
  This will appropriately estimate the effect modification.
\item
  This results in a loss of information because \emph{all parameters}
  are forced to vary across the subgroups.
\end{itemize}

\begin{description}
\tightlist
\item[Subject Specific Models (Definition~\ref{def-subject-specific})]
Also known as conditional modeling, the subject-specific approach models
at the subject-level and addresses the correlation indirectly through
the inclusion of random effects.
\item[Subsampling (Definition~\ref{def-subsampling})]
Subsampling occurs when several measurements are taken on each subject
under the same treatment, possibly at unique locations.
\item[Unstructured Correlation Structure
(Definition~\ref{def-unstructured-correlation-structure})]
An unstructured correlation structure suggests that the correlation
between any two errors within a subject can take on any value. We only
require that it be a valid correlation matrix.
\item[Variable (Definition~\ref{def-variable})]
A measurement, or category, describing some aspect of the subject.
\item[Variance-Covariance Matrix
(Definition~\ref{def-variance-covariance-matrix})]
Let \(\boldsymbol{\beta}\) represent the \((p+1)\)-length vector of the
parameters and \(\widehat{\boldsymbol{\beta}}\) represent the \((p+1)\)
vector of the parameter \emph{estimates}. The variance-covariance matrix
of the parameter estimates is the \((p+1)\)-by-\((p+1)\) matrix
\(\boldsymbol{\Sigma}\) where
\end{description}

\begin{itemize}
\tightlist
\item
  the \(j\)-th diagonal element contains
  \(Var\left(\widehat{\beta}_j\right)\), and
\item
  the \((i,j)\)-th element contains the covariance between
  \(\widehat{\beta}_i\) and \(\widehat{\beta}_j\).
\end{itemize}

\begin{description}
\tightlist
\item[t-Distribution (Definition~\ref{def-t-distribution})]
Let \(X\) be a continuous random variable. \(X\) is said to have a
t-distribution if the density is given by
\end{description}

\[f(x) = \frac{\Gamma \left(\frac{\nu+1}{2} \right)} {\sqrt{\nu\pi}\,\Gamma \left(\frac{\nu}{2} \right)} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}} \qquad x > 0\]

where \(\nu > 0\) is the degrees of freedom.

We write \(X \sim t_{\nu}\), which is read ``X has a t-distribution with
\(\nu\) degrees of freedom.''



\end{document}
