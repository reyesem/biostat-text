{"title":"Essential Probability","markdown":{"headingText":"Essential Probability","headingAttr":{"id":"sec-essential-probability","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n{{< include _setupcode.qmd >}}\n\nThe discipline of Statistics uses data to make inference on a population.  In turn, statistical theory is built on probability --- the discipline of mathematics that studies and models random processes.  While we do not need to be experts in probability to be practitioners of statistical methodology, a foundation in models from probability is helpful for seeing common threads in statistical modeling.  This chapter provides a brief introduction to the most relevant aspects of probability theory necessary for engaging with the remainder of the text.\n\n\n## Density Functions as Models\nAny process for which the outcome cannot be predicted with certainty is a random process.  Typically, probability is taught from a mathematical perspective, with a goal of constructing a coherent and complete framework for characterizing such random processes.  Here, our goal is to introduce key probability concepts by relating them to their data-centric analogues.  That is, we want to think of probability in light of how we will use it in statistical analysis.\n\nEach time we collect data, we can think of each observation as the result of a random process.  These observations are recorded as variables in our dataset.  In probability, a random variable is used to represent a measurement that results from a random process.  Just as we have both _quantitative_ and _qualitative_ variables, there are _continuous_ and _discrete_ random variables.\n\n:::{#def-random-variable}\n## Random Variable\nA random variable represents a measurement that will be collected and for which the value cannot be predicted with certainty; they are generally represented with a capital letter.  Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements.\n:::\n\nConsider measuring a single variable on a sample of $n$ participants.  Then, we might represent the measurements we will obtain as $X_1, X_2, \\dots, X_n$.\n\n:::{.callout-note}\nThere are many ways to interpret probability.  In classical (\"frequentist\") statistics, we think of probability as the likelihood of an event over repeated experimentation.  Therefore, probability does not describe events that have already occurred; we can only describe the likelihood of future events.  \n:::\n\nEach of our random variables $X_1, X_2, \\dotsc, X_n$ will be observations from some underlying population.  As we described in previous chapters, the distribution of the population is unknown.  However, we might posit a model for this distribution.  This is our primary use of probability theory in statistics --- to model distributions.  The most common way to represent a probability model is through its density function.\n\n:::{#def-density-function}\n## Density Function\nA density function $f$ relates the potential values of a random variable $X$ with the probability those values occur.  For a _continuous_ random variable, the probability the random variable $X$ falls within an interval $(a, b)$ is given by\n\n$$Pr(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx.$$\n\nFor a _discrete_ random variable, the probability the random variable $X$ is equal to the value $u$ is given by\n\n$$Pr(X = u) = f(u).$$\n:::\n\n:::{.callout-note}\nIn a probability course, there is often a distinction made between probability density functions (continuous random variables) and probability mass functions (discrete random variables).  We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable.\n:::\n\nWith few exceptions, we will be working with continuous random variables.  As a result, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we obtain probabilities by computing the area under the curve.  Again, drawing connections to data analysis, we can think of a density function as a mathematical formula representing a smooth histogram.  The area under the curve for any region gives the proportion of the population which has a value in that region.  That is, we get the probability that a random variable will be in an interval by integrating the density function over that interval.  @fig-essential-probability-density illustrates this idea; we have a hypothetical dataset that has been summarized using a histogram; we overlay a density function (with the corresponding mathematical model that describes this density function).  The figure shows how the sample (summarized in the histogram) is approximating the population (the density function).\n\n```{r}\n#| label: fig-essential-probability-density\n#| echo: false\n#| fig-cap: Illustration of a density function representing the distribution of the population and a histogram from a representative sample.\n#| fig-alt: Histogram with a curve overlaid.\n\nset.seed(123)\n\ntibble(\n  y = rnorm(1000, mean = 0, sd = 1)\n) |>\n  ggplot(mapping = aes(x = y)) +\n  geom_histogram(binwidth = 0.2,\n                 aes(y = stat(density)), fill = \"grey75\", color = \"black\") +\n  stat_function(fun = dnorm, color = \"blue\", linewidth = 1.1) +\n  annotate(\"label\", x = 1.5, y = 0.35, \n           label = 'f(x) == frac(1, sqrt(2*pi)) * \n                                e^(-frac(1, 2)*x^2)',\n           parse = TRUE) +\n  labs(y = NULL,\n       x = \"Value of the Variable\") +\n  theme_minimal(14) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n```\n\nEspecially for visualization, the density function is the most common way of characterizing a probability model.  However, computing the probability using the density is problematic due to the integration required.  Many software programs address this by working with the cumulative distribution function (CDF).\n\n:::{#def-cdf}\n## Cumulative Distribution Function (CDF)\nLet $X$ be a random variable; the cumulative distribution function (CDF) is defined as\n\n$$F(u) = Pr(X \\leq u).$$\n\nFor a continuous random variable, we have that\n\n$$F(u) = \\int_{-\\infty}^{u} f(x) dx$$\n\nimplying that the density function is the derivative of the CDF.  For a discrete random variable\n\n$$F(u) = \\sum_{x \\leq u} f(x).$$\n:::\n\nWorking with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer) and we use the result to compute probabilities directly.\n\n:::{.callout-tip}\n## Big Idea\nDensity functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurrence.  However, for computational reasons, we often work with the cumulative distribution function which provides the probability a random variable is less than or equal to a value.\n:::\n\n\n## Summarizing Distributions (Parameters)\nMost scientific questions are focused on the location or spread of a distribution.  For example, we are interested in estimating the average yield of a crop, or the variance in the amount of sleep among college students.  Introductory statistics introduces summaries of location and spread within the sample (e.g., sample mean for location and sample variance for spread).  Analogous summaries exist for density functions.  \n\nIn particular, the mean of a random variable (denoted by $E(X)$) and the variance of a random variable (denoted by $Var(X)$) are measures of the location and spread, respectively, of the distribution represented by its corresponding density function.  When the density function is a model for the population, these represent the parameters of the population --- the same parameters we estimate and make inference on using our data analysis.  For completeness, we present the computational formulas for the mean and variance of a random variable, but we do not make use of these formulas moving forward.  Instead, we simply note that these formulas are similar to their sample counterparts.\n\n:::{#def-rv-mean-variance}\n## Mean and Variance of a Random Variable\nSuppose $X$ is a random variable with density function $f$.  If $X$ is a continuous random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\int x f(x) dx \\\\\n  Var(X) &= \\int \\left(x - E(X)\\right)^2 f(x) dx.\n\\end{aligned}\n$$\n\nIf $X$ is a discrete random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\sum x f(x) \\\\\n  Var(X) &= \\sum \\left(x - E(X)\\right)^2 f(x).\n\\end{aligned}\n$$\n:::\n\nAs we have stated, the distribution of the population is generally unknown.  If we were able to fully specify the density function for the population, then there would be no need for statistical analysis.  Instead, the model is generally posited up to some unknown values (parameters).  For example, a researcher might posit that within the population, the time until a medical device fails could be modeled using the density\n\n$$f(x) = \\frac{1}{\\mu} e^{-\\frac{x}{\\mu}} \\qquad x > 0.$$\n\nHere, the researcher has really posited a form of the model, but not the exact model as $\\mu$ is unknown.  The value $\\mu$ represents the average response (which could be confirmed using the formulas in the above definition).  In such cases, making inference on the parameters allows us to characterize the distribution of the population.\n\n:::{.callout-tip}\n## Big Idea\nWhen a probability model is specified for a population, it is generally specified up to some unknown parameter(s).  Making inference on the unknown parameter(s) therefore characterizes the distribution --- characterizes the manner in which the response varies across individuals in the population.\n:::\n\n\n\n## Specific Models for Populations\nWhile we could posit any non-negative function as a model for a density function, there are some models that are very common.  The most common model for the population of a continuous random variable is the Normal distribution.\n\n:::{#def-normal-distribution}\n## Normal (Gaussian) Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a Normal (or Gaussian) distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\qquad -\\infty < x < \\infty,$$\n\nwhere $\\mu$ is any real number and $\\sigma^2 > 0$.  \n\n- $E(X) = \\mu$\n- $Var(X) = \\sigma^2$\n\nWe write $X \\sim N\\left(\\mu, \\sigma^2\\right)$, which is read \"X has a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\"  This short-hand implies the density above.\n:::\n\nThis model is a bell-shaped distribution centered at the mean $\\mu$.  While this is a common model, it should not be assumed by default.  In future chapters, we will consider methods for assessing whether assuming a Normal distribution is reasonable.\n\nWhen a response is binary (assumes one of two values), it is a Bernoulli distribution.  In order to make use of this distribution, we typically define one of the two possible outcomes as a \"success\" and the other as a \"failure.\"  For example,\n\n$$X = \\begin{cases} 1 & \\text{if a success is observed} \\\\ 0 & \\text{if a success is not observed.} \\end{cases}$$\n\n:::{#def-bernoulli-distribution}\n## Bernoulli Distribution\nLet $X$ be a discrete random variable taking the value 0 or 1.  $X$ is said to have a Bernoulli distribution with density\n\n$$f(x) = \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1\\},$$\n\nwhere $0 < \\theta < 1$ is the probability that $X$ takes the value 1.\n\n- $E(X) = \\theta$\n- $Var(X) = \\theta(1 - \\theta)$\n\nWe write $X \\sim Ber(\\theta)$, which is read \"X has a Bernoulli distribution with probability $\\theta$.\"\n:::\n\n:::{.callout-note}\nA generalization of the Bernoulli distribution is the Binomial distribution.  So, we sometimes hear people refer to a Bernoulli distribution as \"a Binomial distribution with a single event.\"\n:::\n\n\n\n## Models for Sampling Distributions and Null Distributions\nA statistical analysis does not exist in a vacuum.  Instead, based on the context of the study, we make assumptions about the process which generated the data.  The conditions we are willing to assume govern how we model the sampling distribution or null distribution.  Occasionally, we can lean on statistical theory to say how the sampling distribution or null distribution will behave.  That is, under certain conditions, statistical theory tells us what the appropriate model is.  In these situations, there are some common models.\n\nThe t-distribution is a bell-shaped distribution, similar to the Normal distribution but with wider tails.  It has a single parameter, known as the degrees of freedom.  Note that unlike many other distributions, this parameter (the degrees of freedom) is not associated with the location of the distribution.  Instead, the parameter governs the spread (but is not the variance).\n\n:::{#def-t-distribution}\n## t-Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a t-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}} \\qquad x > 0$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim t_{\\nu}$, which is read \"X has a t-distribution with $\\nu$ degrees of freedom.\"\n:::\n\nThe Chi-Square distribution is a skewed distribution (looks like a giant slide).  It has a single parameter, known as the degrees of freedom.  The degrees of freedom for this distribution characterize both the location and spread simultaneously.\n\n:::{#def-chi-square-distribution}\n## Chi-Square Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a Chi-Square distribution if the density is given by\n\n$$f(x) = \\frac{1}{2^{\\nu/2}\\Gamma (\\nu/2)}\\;x^{\\nu/2-1}e^{-x/2} \\qquad x > 0,$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim \\chi^2_{\\nu}$, which is read \"X has a Chi-Square distribution with $\\nu$ degrees of freedom.\"\n:::\n\nThe F-distribution is a skewed distribution.  It has two parameters, known as the numerator and denominator degrees of freedom.  While neither variable is the mean or variance, together these two parameters characterize both the location and the spread.\n\n:::{#def-f-distribution}\n## F-Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have an F-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma((r + s)/2)}{(\\Gamma(r/2) \\Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \\qquad x > 0,$$\n\nwhere $r,s > 0$ are the numerator and denominator degrees of freedom, respectively.\n\nWe write $X \\sim F_{r, s}$, which is read \"X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.\"\n:::\n\n```{r}\n#| label: fig-essential-probability-comparisons\n#| echo: false\n#| fig-cap: Comparison of various common distributions.\n#| fig-alt: Several density curves overlaid.\n\ntibble(\n  distribution = rep(c(\"Normal\", \"Chi-Square\", \"t\", \"F\"), each = 1000),\n  x = rep(seq(-3, 3, length.out = 1000), times = 4),\n  y = case_when(\n    distribution == \"Normal\" ~ dnorm(x, mean = 0, sd = 1),\n    distribution == \"Chi-Square\" & x > 0 ~ dchisq(x, df = 3),\n    distribution == \"t\" ~ dt(x, df = 3),\n    distribution == \"F\" & x > 0 ~ df(x, df1 = 2, df2 = 16)\n  )\n) |>\n  drop_na() |>\n  ggplot(mapping = aes(x = x, y = y, color = distribution)) +\n  geom_line(linewidth = 1.1) +\n  labs(y = NULL,\n       x = NULL,\n       color = NULL) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\",\n                     breaks = c(\"Chi-Square\", \"F\", \"Normal\", \"t\"),\n                     labels = expression({chi^2}[3], F[{2*\",\"*16}], N*\"(0,1)\", t[3])) +\n  theme_minimal(14) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.position = \"bottom\")\n```\n\nThe formulas above are ugly, but we will not be working with them directly.  Instead, statistical software has these distributions embedded.  The key idea here is that when we know the model for a sampling distribution, we are able to rely on that model in order to obtain confidence intervals.  And, when we have a model for the null distribution, we are able to rely on that model to obtain p-values.  These models are behind default implementations of statistical methods in software.\n\n:::{.callout-tip}\n## Big Idea\nSome probability models occur so frequently that we give them names for easy reference.  Some models are common for modeling the population, in which case they are defined in terms of unknown parameters to be estimated.  Some models are common for modeling sampling distributions or null distributions, in which case their form will be explicitly determined according to statistical theory.\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["mystyles.css"],"output-file":"01d-essential-probability.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["refs482notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":true,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"01d-essential-probability.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["refs482notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}