{
  "hash": "2a320b8fb5347091ed9a8558718da1bd",
  "result": {
    "markdown": "# Assessing the Conditions for the General Linear Model {#sec-glm-assessing-conditions}\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the previous chapter, we presented a model for the sampling distribution (@def-ls-sampling-distribution) of the least squares estimates.  This model allows us to make inference on the unknown parameters that govern the mean response of the general linear model (@def-general-linear-model).  However, our model for the sampling distribution presented assumes all the conditions for the classical regression model (@def-classical-regression) hold.  We should not blindly make assumptions.  Instead, we should ensure our data is consistent with any conditions we impose.\n\nThe majority of the conditions in the classical regression model are placed on the error term, a random variable that we never observe in practice.  This means that the conditions cannot be assessed using the errors directly.  Instead, modeling conditions are assessed graphically using residuals.\n\n:::{#def-residual}\n## Residual\nA residual for the $i$-th observation is the difference between an observed value and the predicted response:\n\n$$\n\\begin{aligned}\n  (\\text{Residual})_i \n    &= (\\text{Observed Response})_i - (\\text{Predicted Response})_i \\\\\n    &= (\\text{Response})_i - \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_i\\right).\n\\end{aligned}\n$$\n\n:::\n\n:::{.callout-tip}\n## Big Idea\nIf a condition holds on the distribution of the errors, we expect the residuals to adopt specific behavior.  Therefore, while the conditions are placed on the error, they are _assessed_ using the residuals.  The conditions, however, are _not_ about the residuals.\n:::\n\nIt is important to assess the conditions we place on the model as they determine the model for the sampling distribution (and null distribution).  That is, if the conditions we have assumed are incorrect, our p-values and confidence intervals will be invalid.\n\n:::{.callout-tip}\n## Big Idea\nThe assumptions we are willing to make about a data generating process (in particular, the random component of our regression model) determine the form of the model for the sampling distributions (null distributions) of the resulting estimates (standardized statistics).\n:::\n\n:::{.callout-note}\nYou will notice we often jump between \"conditions\" and \"assumptions\" as if they are interchangeable.  They are often used synonymously in the literature.  However, as a distinction, _conditions_ are the mathematical properties that must be met in order to justify the statistical theory; in practice, we make _assumptions_ about which conditions we believe are reasonable.\n\nWe can never prove a condition holds; therefore, we must always make assumptions.\n:::\n\n\nAs a general rule, if our data is consistent with the conditions we have assumed, then the error term is just noise; that is, it should not have any signal left.  We would therefore expect the residuals to lack any patterns; if we find patterns in the residuals, it suggests there is some structure our model has ignored.  While there are many methods available for detecting patterns in the residuals, we prefer a graphical approach.  Since we cannot verify a condition holds, we will always be making assumptions.  By taking a graphical approach to assessment, we are embracing the subjective nature of such investigations; other approaches can give the appearance that there is more certainty in the conclusion than actually exists.\n\n@tbl-glm-assessing-conditions-residual-plots aligns the conditions we place on the model with the graphic used for assessment.\n\n\n::: {#tbl-glm-assessing-conditions-residual-plots .cell tbl-cap='Method of graphical assessment for the conditions of the classical regression model.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Condition </th>\n   <th style=\"text-align:left;\"> Graphical Assessment </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Error is 0, on average, for all predictors </td>\n   <td style=\"text-align:left;\"> Residual vs. Predicted Values </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Errors are independent </td>\n   <td style=\"text-align:left;\"> Time-series Plot of Residuals </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Homoskedasticity </td>\n   <td style=\"text-align:left;\"> Residual vs. Predicted Values </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Errors are Normally distributed </td>\n   <td style=\"text-align:left;\"> Probability Plot of Residuals </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> No Measurement Error </td>\n   <td style=\"text-align:left;\"> None (discipline expertise) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Predictor enters linearly </td>\n   <td style=\"text-align:left;\"> Residual vs. Predictor </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe conditions on the error are assessed in the same way they are in simple linear regression, covered in an introductory course.  So, we only briefly review them here.\n\n:::{.callout-note}\n## Using the Plot of the Residuals vs. Predicted Values\nThis plot is used to assess both the \"mean 0\" and \"constant variance\" conditions.  We are looking for trends in the _location_ and in the _spread_, respectively, to assess these conditions.\n:::\n\nIf we observe a trend in the _location_ of the residuals as we move left-to-right on the graphic, we have evidence that the \"mean 0\" condition is violated.  If we observe a trend in the _spread_ of the residuals as we move left-to-right across the graphic, we have evidence that the \"constant variance\" condition is violated.  As we move forward, we will examine techniques to relax the condition of constant variance.  While there is no \"fix\" for violations of the \"mean 0\" condition, we will study scenarios for which the response and predictors are not linearly related.  What is amazing about this graphic is that we have reduced a multi-dimensional problem to two dimensions.\n\n:::{.callout-note}\n## Using the Time-Series Plot of the Residuals\nThis graph plots the residuals against the order in which the data was collected.  Any trends in the residuals (in location _or_ spread) indicates evidence the errors collected close together in time may be associated in some way, violating the assumption of independence.\n:::\n\nThe creation of a time-series plot is only reasonable/useful if we know the order in which the data was collected.  A cross-sectional analysis (single snapshot in time), for example has no natural ordering.  It is important to note that a time-series plot only allows us to examine dependence as a result of time.  If the errors are correlated due to some other factor (for example, geographical location or family groups), the violation may not be detected.  When we are able to identify the dependence structure, we can incorporate it into the model, as discussed in later units (@sec-rm-terminology).  Regardless of whether the graphic can be constructed, a thorough assessment of independence always requires discipline expertise and a critical review of the data collection plan.\n\n:::{.callout-note}\n## Using the Probability Plot of the Residuals\nAlso called a \"QQ Plot,\" a probability plot examines the relationship between the observed residuals and the expected (or \"theoretical\") values we would expect if the errors followed a Normal distribution.  Any departures from a straight line indicate evidence the errors do not follow a Normal distribution.\n:::\n\nThe assumption of Normality is the strongest of the four conditions; that is, this condition goes beyond characterizing an aspect of the error distribution to specifying the functional family to which the distribution belongs.  As we will see, this condition is the easiest to relax.\n\nRecall that in addition to conditions on the stochastic portion of the model, we have considered conditions on the predictors as well (@def-classical-regression-cont).  \n\n:::{.callout-note}\n## Assessing Measurement Error in the Predictors\nDetermining whether a predictor is subject to measurement error relies on discipline expertise.\n:::\n\nIf the predictors are measured with (non-negligible) error, our estimates are biased and therefore unreliable.  Methods for addressing predictors that are subject to measurement error are beyond the scope of the text.\n\nRequiring that the predictors enter the model linearly is a refinement of the \"mean 0\" condition; that is, one way that we often misspecify the deterministic portion of the model is to attempt to model curvature in the data using a line.  It should not be a surprise then that assessing the linearity of the predictors is similar to assessing the \"mean 0\" condition.\n\n:::{.callout-note}\n## Assessing Linearity of the Predictors\nIf the relationship between the response and predictor is adequately explained by a linear relationship, then there should not be any structure in the _location_ of the residuals when examined against the predictor.  \n:::\n\nThat is, when assessing the \"mean 0\" condition, we examine the residuals against the predicted values; when assessing the linearity condition, we examine the residuals against each _quantitative_ predictor.  \n\n:::{.callout-note}\nWe will discuss the incorporation of categorical predictors in the next chapter; however, we note that the linearity assumption only applies to quantitative predictors.\n:::\n\nThe data will not always be consistent with the conditions we would like to place on the model.  Proceeding as if the conditions are reasonable when they are not can lead to invalid inference and incorrect conclusions.  Discarding the results misses out on potential insights the data offers.  Fortunately, the modeling framework is flexible enough to be relaxed to address violations of the conditions, which we examine toward the end of this unit in the text.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}