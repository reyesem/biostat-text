{
  "hash": "2a4667322e5166e1c3b286d34d039062",
  "result": {
    "markdown": "# Glossary\n\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.\n\n\n\nAIC (@def-aic) \n: Given a model $\\mathcal{M}$, Akaike's information criterion (AIC) is defined as\n\n$$\\text{AIC}_{\\mathcal{M}} = -2\\ln\\left(\\widehat{L}_{\\mathcal{M}}\\right) + 2p_{\\mathcal{M}}$$\n\nwhere $\\widehat{L}_{\\mathcal{M}}$ represents the likelihood function corresponding to model $\\mathcal{M}$ when evaluated at the maximum likelihood estimates, and $p_{\\mathcal{M}}$ is the number of parameters in $\\mathcal{M}$.\n\nAlternate Characterization of the Classical Regression Model (@def-alternate-characterization) \n: Under the classical regression conditions on the error term (see @def-classical-regression), we can characterize the classical regression model as\n\n$$(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i \\dist{Ind} N\\left(\\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i, \\sigma^2\\right).$$\n\nHere, the symbol $\\mid$ is read \"given\" and means that the distribution of the response is specified after knowing the values of the predictors.  That is, the distribution of the response depends on these variables.\n\nAutoregressive Correlation Structure (@def-autoregressive-correlation-structure) \n: An autoregressive correlation structure suggests the correlation between two observations diminishes as the observations get further apart (generally, further apart in time).  We generally only consider the autoregressive structure of degree 1 here; if there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & \\rho & \\rho^2 & \\rho^3 & \\rho^4 \\\\\n\\cdot & 1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\cdot & \\cdot & 1 & \\rho & \\rho^2 \\\\\n\\cdot & \\cdot & \\cdot & 1 & \\rho \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nBIC (@def-bic) \n: Given a model $\\mathcal{M}$, Schwarz's Bayesian information criterion (BIC) is defined as\n\n$$\\text{BIC}_{\\mathcal{M}} = -2\\ln\\left(\\widehat{L}_{\\mathcal{M}}\\right) + p_{\\mathcal{M}} \\ln(n).$$\n\nBernoulli Distribution (@def-bernoulli-distribution) \n: Let $X$ be a discrete random variable taking the value 0 or 1.  $X$ is said to have a Bernoulli distribution with density\n\n$$f(x) = \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1\\},$$\n\nwhere $0 < \\theta < 1$ is the probability that $X$ takes the value 1.\n\n- $E(X) = \\theta$\n- $Var(X) = \\theta(1 - \\theta)$\n\nWe write $X \\sim Ber(\\theta)$, which is read \"X has a Bernoulli distribution with probability $\\theta$.\"\n\nBlocking (@def-blocking) \n: Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.\n\nIn both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.\n\nBootstrapping (@def-bootstrapping) \n: A process of constructing a sampling distribution of the parameter estimates through resampling.  The observed data is resampled repeatedly, and the parameters of interest are estimated in each resample.  The distribution of these estimates across the resamples is then used as an empirical model of the corresponding sampling distributions.\n\nCase Resampling Bootstrap (@def-case-resampling-bootstrap) \n: Suppose we observe a sample of size $n$ and use the data to compute the least squares estimates $\\widehat{\\bs{\\beta}}$ for the parameters in the model\n\n$$(\\text{Response})_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i + \\varepsilon_i.$$\n\nThe case resampling bootstrap proceeds according to the following algorithm:\n\n  1. Take a random sample of size $n$ (with replacement) of the raw data (keeping all variables from the same observation together); denote the $i$-th selected response and predictors $(\\text{Response})_i^*$ and $(\\text{Predictor } j)_i^*$, respectively.\n  2. Obtain the least squares estimates $\\widehat{\\bs{\\alpha}}$ by finding the values of $\\bs{\\alpha}$ that minimize \n  \n  $$\\sum_{i=1}^{n} \\left((\\text{Response})_i^* - \\alpha_0 - \\sum_{j=1}^{p} \\alpha_j (\\text{Predictor } j)_i^*\\right)^2.$$\n  \n  3. Repeat steps 1-2 $m$ times.\n\nWe often take $m$ to be large (at least 1000).  After each pass through the algorithm, we retain the least squares estimates $\\widehat{\\bs{\\alpha}}$ from the resample.  The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.\n\nCategorical Variable (@def-categorical-variable) \n: Also called a \"qualitative variable,\" a measurement on a subject which denotes a grouping or categorization.\n\nCensored Data (@def-censored-data) \n: Censored data is a special case of missing data for which a bound on the missing value is known.  In survival analysis, the response of interest (time to an event) is subject to censoring.\n\nCentral Limit Theorem (@def-clt) \n: Let $Y_1, Y_2, \\dotsc, Y_n$ be independent and identically distributed random variables with finite mean $\\mu$ and variance $\\sigma^2$.  Then, as $n$ approaches infinity, the distribution of the ratio\n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{\\sigma}$$\n\napproaches that of a Standard Normal random variable.\n\nChi-Square Distribution (@def-chi-square-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Chi-Square distribution if the density is given by\n\n$$f(x) = \\frac{1}{2^{\\nu/2}\\Gamma (\\nu/2)}\\;x^{\\nu/2-1}e^{-x/2} \\qquad x > 0,$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim \\chi^2_{\\nu}$, which is read \"X has a Chi-Square distribution with $\\nu$ degrees of freedom.\"\n\nClassical Regression (Conditions on Predictors) (@def-classical-regression-cont) \n: The classical regression model (@def-classical-regression) places the following conditions on the predictors:\n\n  1. Each predictor is measured without error.\n  2. Each predictor has an additive linear effect on the response.\n\nClassical Regression Model (@def-classical-regression) \n: In the \"classical regression model,\" we place the following four conditions on the distribution of the error $\\varepsilon_i$:\n\n  1. The average error across all levels of the predictors is 0; mathematically, we write $E\\left(\\varepsilon_i \\mid (\\text{Predictors 1 - }p)_i\\right) = 0$.\n  2. The variance of the errors is constant across all levels of the predictors; mathematically, we write $Var\\left(\\varepsilon_i \\mid (\\text{Predictors 1 - }p)_i\\right) = \\sigma^2$ for some unknown constant $\\sigma^2 > 0$.  This is sometimes referred to as homoskedasticity.\n  3. The error terms are independent; in particular, the magnitude of the error for one observation does not influence the magnitude of the error for any other observation.\n  4. The distribution of the errors follows a Normal distribution with the above mean and variance.\n\nCluster Samples (@def-cluster-samples) \n: Stratified sampling divides a population into groups and samples from within each group; in contrast, cluster sampling divides the population into groups and randomly samples a few groups and takes measurements from within the group.\n\nCodebook (@def-codebook) \n: Also called a \"data dictionary,\" a codebook provides complete information regarding the variables contained within a dataset.\n\nComplete Data (@def-complete-data) \n: This term describes a data set for which the event is observed on all subjects.\n\nCompound Symmetric Correlation Structure (@def-compound-symmetric-correlation-structure) \n: A compound symmetric correlation structure, also known as an _exchangeable_ correlation structure, suggests the correlation between any two errors within a subject is equal.  If there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & \\rho & \\rho & \\rho & \\rho \\\\\n\\cdot & 1 & \\rho & \\rho & \\rho \\\\\n\\cdot & \\cdot & 1 & \\rho & \\rho \\\\\n\\cdot & \\cdot & \\cdot & 1 & \\rho \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nConfidence Interval (@def-confidence-interval) \n: An interval (range of values) estimate of a parameter that incorporates the variability in the statistic.  The process of constructing $k$% confidence intervals results in them containing the parameter of interest in $k$% of _repeated_ studies.  The value of $k$ is called the _confidence level_.\n\nConfidence Interval for Parameters Under Classical Model (@def-classical-ci) \n: Under the classical regression conditions (@def-classical-regression), a $100c$% confidence interval for the parameter $\\beta_j$ is given by\n\n$$\\widehat{\\beta}_j \\pm t_{n-p-1, 0.5(1+c)} \\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}.$$\n\nwhere $t_{n-p-1, 0.5(1+c)}$ is the $0.5(1+c)$ quantile from the $t_{n-p-1}$ distribution, known as the critical value for the confidence interval.\n\nConfounding (@def-confounding) \n: When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.\n\nCorrelation Structure (@def-correlation-structure) \n: The correlation structure quantifies the strength and direction of the relationship between the errors in the observed responses.\n\nCox Proportional Hazards Model (@def-cph) \n: The proportional hazards model (or Cox proportional hazards model, or PH model, or Cox PH model) is a model for the hazard function that enforces the assumption of proportional hazards; it has the following form:\n  \n$$\\lambda\\left(t \\mid (\\text{Predictors})_i\\right) = \\lambda_0(t) e^{\\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i},$$\n\nwhere the form of $\\lambda_0(t)$, known as the baseline hazard, is not specified.\n\nCross Sectional Study (@def-cross-sectional-study) \n: A cross sectional study considers data from a single snapshot in time.\n\nCross-Over Study (@def-cross-over-study) \n: A cross-over study exposes each participant to multiple treatments.  Whenever possible, the order of the treatments is randomly determined.  This is equivalent to a randomized complete block design in which the blocks are the participants.  When the treatments are believed to have a lingering effect, a wash-out period between treatments is used to minimize the impact of previous treatments on the treatment the participant is currently being exposed to.\n\nCumulative Distribution Function (CDF) (@def-cdf) \n: Let $X$ be a random variable; the cumulative distribution function (CDF) is defined as\n\n$$F(u) = Pr(X \\leq u).$$\n\nFor a continuous random variable, we have that\n\n$$F(u) = \\int_{-\\infty}^{u} f(x) dx$$\n\nimplying that the density function is the derivative of the CDF.  For a discrete random variable\n\n$$F(u) = \\sum_{x \\leq u} f(x).$$\n\nCumulative Hazard (@def-cumulative-hazard) \n: For any time $t$, we have that\n\n$$S(t) = e^{-\\int_{0}^{t} \\lambda(u) du}$$\n\nwhere \n\n$$\\Lambda(t) = -\\int_{0}^{t} \\lambda(u) du$$\n\nis known as the cumulative hazard function.\n\nDensity Function (@def-density-function) \n: A density function $f$ relates the potential values of a random variable $X$ with the probability those values occur.  For a _continuous_ random variable, the probability the random variable $X$ falls within an interval $(a, b)$ is given by\n\n$$Pr(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx.$$\n\nFor a _discrete_ random variable, the probability the random variable $X$ is equal to the value $u$ is given by\n\n$$Pr(X = u) = f(u).$$\n\nDistribution (@def-distribution) \n: The pattern of variability corresponding to a set of values.\n\nEstimate of the Variance of the Errors (@def-estimate-sigma2) \n: The unknown variance in the linear model, which captures the variability in the response for any set of predictors (also called the residual variance), is estimated by\n\n$$\\widehat{\\sigma}^2 = \\frac{1}{n-p-1} \\sum\\limits_{i=1}^{n} \\left((\\text{Response})_i - \\widehat{\\beta}_0 - \\sum\\limits_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_{i}\\right)^2.$$\n\nEvent Time and Censoring Indicator (@def-event-time) \n: Let $T_i$ and $C_i$ represent the survival time and the censoring time for the $i$-th subject.  When data is subject to right censoring, we observe\n\n$$X_i = \\min\\left\\{T_i, C_i\\right\\}$$\n\nwhich is known as the _event_ time.  We also observe whether this observation was triggered by the actual event or censoring:\n  \n$$\\delta_i = \\begin{cases} 1 & \\text{if } T_i \\leq C_i \\\\ 0 & \\text{if } T_i > C_i. \\end{cases}$$\n\nThis is known as the _censoring indicator_.\n\nF-Distribution (@def-f-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have an F-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma((r + s)/2)}{(\\Gamma(r/2) \\Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \\qquad x > 0,$$\n\nwhere $r,s > 0$ are the numerator and denominator degrees of freedom, respectively.\n\nWe write $X \\sim F_{r, s}$, which is read \"X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.\"\n\nFixed Effect (@def-fixed-effect) \n: Fixed effects are terms in the model for which we are interested in both the specific grouping levels, and we are interested in characterizing the relationship between these levels and the response.\n\nGauss-Newton Method (@def-gauss-newton) \n: The Gauss-Newton method is the most commonly used optimization routine for statistical applications; this method relies on linearizing the problem using a Taylor Series approximation.\n\nGeneral Linear Hypothesis (@def-general-linear-hypothesis) \n: The general linear hypothesis framework refers to testing hypotheses of the form \n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m} \\qquad \\text{vs.} \\qquad H_1: \\bm{K}\\bs{\\beta} \\neq \\bm{m}$$\n\nwhere\n\n  - $\\bs{\\beta}$ is the $(p+1)$-length vector of the parameters (includes the intercept),\n  - $\\bm{K}$ is an $r$-by-$(p+1)$ matrix that specifies the linear combinations defining the hypothesis of interest, and\n  - $\\bm{m}$ is a vector of length $r$ specifying the null values, the value of each linear combination under the null hypothesis (often a vector of 0's).\n\nGeneral Linear Model (@def-general-linear-model) \n: The general linear model views the response (outcome) as a linear combination of several predictors:\n  \n$$\n\\begin{aligned}\n  (\\text{Response})_i \n    &= \\beta_0 + \\beta_1 (\\text{Predictor 1})_{i} + \\beta_2 (\\text{Predictor 2})_{i} + \\dotsb + \n      \\beta_p (\\text{Predictor } p)_{i} + \\varepsilon_i \\\\\n    &= \\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_{i} + \\varepsilon_i\n\\end{aligned}\n$$\n\nwhere $n$ is the number of subjects in the sample, $p < n$ is the number of predictors in the model, and $\\varepsilon_i$ is a random variable that captures the error in the response.\n\nGeneralized Estimating Equations (GEE) (@def-gee) \n: Generalized estimating equations can be used to estimate the parameters of a model while accounting for the correlation among observations.  In addition to specifying a model for the overall average response, a \"working\" structure is specified for the correlation of observations from the same subject.  The working structure is updated during the estimation process and used to adjust the standard errors of the parameter estimates in the mean model.\n\nGeneralized Least Squares (@def-nlm-gls) \n: The semiparametric nonlinear model can be generalized to capture non-constant variance.  Specifically, we specify the _mean_ and _variance_ of the response given the predictors\n\n$$\n\\begin{aligned}\n  E\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]\n    &= f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right) \\\\\n  Var\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]\n    &= g\\left((\\text{Predictors})_i, \\bs{\\beta}, \\bs{\\gamma}\\right).\n\\end{aligned}\n$$\n\nSuch a model is fit with the method of generalized least squares (as opposed to \"ordinary\" least squares) in which we alternate between (a) minimizing a weighted distance between the observed response and the mean function and (b) minimizing the distance between the squared residuals and the variance function.  That is, we minimize\n\n$$\n\\begin{aligned}\n  &\\sum_{i=1}^{n} \\frac{1}{g\\left((\\text{Predictors})_i, \\bs{\\beta}, \\bs{\\gamma}\\right)} \\left[(\\text{Response})_i - f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right)\\right]^2 \\\\\n  &\\sum_{i=1}^{n} \\left(g\\left((\\text{Predictors})_i, \\bs{\\beta}, \\bs{\\gamma}\\right) - \\left[(\\text{Response})_i - f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right)\\right]^2\\right)^2.\n\\end{aligned}\n$$\n\nHazard Function (@def-hazard) \n: For any time $t$, the hazard function $\\lambda(t)$ is the instantaneous mortality rate per unit time:\n  \n$$\\lambda(t) = \\lim_{h \\rightarrow 0} \\frac{Pr(t \\leq T \\leq t + h \\mid T > t)}{h} = \\frac{f(t)}{S(t)} = -  \\frac{\\frac{d}{dt} S(t)}{S(t)} = -\\frac{d}{dt} \\log(S(t)).$$\n\nHierarchical Model (@def-hierarchical-model) \n: A hierarchical model breaks the data generating process into smaller stages and posits a model for each stage.  The stages are determined by defining a hierarchy of units and thereby capturing the sources of variability.\n\nIndependence Correlation Structure (@def-independence-correlation-structure) \n: An independence correlation structure suggests there is no correlation among any of the error terms within a subject.  If there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & 0 & 0 & 0 & 0 \\\\\n\\cdot & 1 & 0 & 0 & 0 \\\\\n\\cdot & \\cdot & 1 & 0 & 0 \\\\\n\\cdot & \\cdot & \\cdot & 1 & 0 \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nIndicator Variables (@def-indicator-variables) \n: Also called \"dummy variables,\" these are a set of binary variables that capture the grouping defined by a categorical variable for regression modeling.\n\nIndividual-Level Model (@def-individual-model) \n: The individual-level model characterizes the response for the $i$-th subject (or block) only.\n\nInformation Criteria (@def-information-criteria) \n: Information criteria refer to metrics that balance the model fit (through the likelihood function) with the model complexity through some penalty term.\n\nInteraction (@def-interaction) \n: An interaction term allows the effect of a predictor on the response to depend on the value of a second predictor (capturing an effect modification).\n\n  - The interaction term is created by adding the product of the two predictors under consideration to the model.\n\nIntercept (@def-intercept) \n: The population intercept, denoted $\\beta_0$, is the _mean_ response when all predictors take the value zero.\n\nInterpretation of Parameters for the Proportional Hazards Model (@def-hr) \n: Consider modeling the hazard function using the proportional hazards model of @def-cph.  The coefficient on the $j$-th predictor $\\beta_j$ is the log-hazard ratio associated with a one-unit increase in the $j$ predictor, holding all other predictors fixed.\n\nInterpretation of Parameters in a Logistic Regression Model (@def-logistic-interpretation) \n: Let $\\beta_j$ be the parameter associated with the $j$-th predictor in a logistic regression model (@def-logistic-regression).  Then, $\\beta_j$ represents the log-OR (\"log odds ratio\") associated with a one-unit increase in the $j$-th predictor holding all other predictors fixed.\n\nInterval Censoring (@def-interval-censoring) \n: Interval censoring refers to scenarios when the event time is known to have occurred within some interval, but the exact time is unknown.\n\nKaplan-Meier Estimator (@def-kaplan-meier) \n: Also known as the product-limit estimator, the Kaplan-Meier estimator is the limit of the life-table estimate as we allow the intervals to shrink to a single event time:\n  \n$$\\widehat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_{t_i}}{n_{t_i}}\\right)$$\n\nwhere $t_i$ is the $i$-th survival time where the event of interest was observed.\n\nLarge Sample Model for the Sampling Distribution of the Least Squares Estimates (@def-ls-sampling-distribution-large-samples) \n: Suppose the classical regression conditions hold, with the exception of the errors following a Normal distribution.  As the sample size gets large, we have that the distribution of the ratio\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim N(0, 1)$$\n\nfor all $j = 0, 1, \\dotsc, p$.  Further, under the null hypothesis\n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m}$$\n\nwe have\n\n$$\\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim \\chi^2_r.$$\n\nLarge Sample Model for the Sampling Distribution of the Least Squares Estimates in Nonlinear Models (@def-nlm-samp-distns) \n: Consider a nonlinear model as described in @def-semiparametric-nonlinear-model.  Assuming the form of the model is correctly specified, as the sample size gets large, we have that\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim N(0, 1)$$\n\nfor all $j = 1, \\dotsc, p$.  Further, under the null hypothesis\n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m}$$\n\nwe have that\n\n$$\\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim \\chi^2_r$$\n\nwhere $r$ is the rank (number of rows) of $\\bm{K}$ and $\\widehat{\\bs{\\Sigma}}$ is the estimated variance-covariance matrix of the parameter estimates.\n\nLarge Sample Sampling Distribution of Parameter Estimates in Logistic Regression (@def-nlm-logistic-samp-distns) \n: Consider the logistic regression model in @def-logistic-regression.  Assuming the form of the model is correctly specified with parameter vector $\\bs{\\beta}$, as the sample size gets large, we have that\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim N(0, 1)$$\n\nfor all $j = 1, \\dotsc, p$.  Further, under the null hypothesis\n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m}$$\n\nwe have\n\n$$\\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim \\chi^2_r$$\n\nwhere $r$ is the rank (number of rows) of $\\bm{K}$.\n\nLarge Sample Theory (@def-large-sample-theory) \n: The phrase \"large sample theory\" (or \"asymptotics\") is used to describe a scenario when the model for the sampling distribution (or null distribution) of an estimate (or standardized statistic) can be approximated as the sample size becomes infinitely large.  That is, as the sample size approaches infinity, the sampling distribution (or null distribution) can be easily modeled using a known probability distribution.\n\nLeast Squares Estimation (@def-least-squares) \n: The method of least squares may be used to estimate the coefficients (parameters) of a linear model.  In particular, we choose the values of the coefficients that minimize\n\n$$\\sum\\limits_{i=1}^{n} \\left((\\text{Response})_i - \\beta_0 - \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_{i}\\right)^2.$$\n\nThe resulting \"least squares\" estimates are denoted $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\dotsc, \\widehat{\\beta}_p$.\n\nLeft Censoring (@def-left-censoring) \n: Left censoring refers to scenarios when an _upper_ bound is known on the response.\n\nLife Table (@def-life-table) \n: Life tables are a method of estimating overall survival over key intervals of time, generally constructed for a single population.\n\nLikelihood Function (@def-likelihood-function) \n: For a fully parametric model, the likelihood function $\\Ell(\\bs{\\beta}, \\text{Observed Data})$ captures how likely the observed data is to be realized in a future study under a specific set of parameters.  This is directly related to the density function of the parametric model assumed.\n\nLinear Model (@def-linear-model) \n: A model is said to be linear if it can be expressed as a linear combination of the _parameters_.  That is, the linearity does not refer to the form of the predictors but the form of the parameters.\n\nLinear Spline (@def-linear-spline) \n: A linear spline is a continuous piecewise linear function.\n\nLog-Rank Test (@def-log-rank) \n: The log-rank test formally compares $k$ survival curves by testing the hypotheses\n\n$$\n\\begin{aligned}\n  H_0&: S_1(t) = S_2(t) = \\dotsb = S_k(t) \\ \\forall t \\quad \\text{vs.} \\\\\n  H_1&: \\text{At least one } S_k \\text{ differs for at least one } t.\n\\end{aligned}\n$$\n\nLogistic Regression Model (@def-logistic-regression) \n: A model for binary responses where the response, given the predictors, has a Bernoulli distribution such that\n\n$$Pr\\left((\\text{Response})_i = 1 \\mid (\\text{Predictors})_i\\right) = \n    \\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}$$ {#eq-nlm-logistic-model}\n\nand all responses are independent of one another.\n\nLongitudinal Study (@def-longitudinal-study) \n: A longitudinal study repeatedly measures the response on each subject at various points in time.\n\nMaximum Likelihood Estimation (@def-mle) \n: The method of maximum likelihood estimation chooses parameter estimates to maximize the likelihood function under an assumed parametric model.  The resulting estimates are known as maximum likelihood estimates.\n\nMean and Variance of a Random Variable (@def-rv-mean-variance) \n: Suppose $X$ is a random variable with density function $f$.  If $X$ is a continuous random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\int x f(x) dx \\\\\n  Var(X) &= \\int \\left(x - E(X)\\right)^2 f(x) dx.\n\\end{aligned}\n$$\n\nIf $X$ is a discrete random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\sum x f(x) \\\\\n  Var(X) &= \\sum \\left(x - E(X)\\right)^2 f(x).\n\\end{aligned}\n$$\n\nMixed-Effects Model (@def-mixed-effects-model) \n: A mixed-effects model denotes a hierarchical model for which some effects are fixed (not allowed to vary across subjects) and others are random (allowed to vary across subjects).\n\nModel for the Null Distribution with the General Linear Hypothesis (@def-general-linear-hypothesis-null) \n: Let $\\widehat{\\bs{\\beta}}$ be the $(p+1)$ vector of estimates for the parameter vector $\\bs{\\beta}$, and let the estimates have variance-covariance matrix $\\bs{\\Sigma}$.  Assuming the null hypothesis \n\n$$H_0: \\bm{K} \\bs{\\beta} = \\bm{m}$$\n\nis true, under the conditions of the classical regression model (@def-classical-regression)\n\n$$(1/r) \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim F_{r, n-p-1}.$$\n\nModel for the Sampling Distribution of Life-Table Estimates (@def-samp-distn-lifetable) \n: Consider estimating the survival $S(t)$ at time $t$ using life-table estimate $\\widehat{S}(t)$.  As the sample size increases, we have that\n\n$$\\frac{\\widehat{S}(t) - S(t)}{\\widehat{\\sigma}^2} \\sim N(0, 1),$$\n\nwhere\n\n$$\\widehat{\\sigma} = \\widehat{S}(t) \\sqrt{\\sum\\limits_{i=1}^t \\frac{d_t}{\\left(n_{t} - w_t/2\\right)\\left(n_{t} - w_t/2 - d_t\\right)}}$$\n\nand the estimate $\\widehat{S}(0) = 1$ has no error.\n\nModel for the Sampling Distribution of the Kaplan-Meier Estimator (@def-samp-distn-km) \n: Consider estimating the survival $S(t)$ at time $t$ using the Kaplan-Meier estimator $\\widehat{S}(t)$.  As the sample size increases, we have that\n\n$$\\frac{\\widehat{S}(t) - S(t)}{\\widehat{\\sigma}^2} \\sim N(0, 1)$$\n\nwhere\n\n$$\\widehat{\\sigma} = \\widehat{S}(t) \\sqrt{\\sum\\limits_{t_i \\leq t} \\frac{d_{t_i}}{n_{t_i} \\left(n_{t_i} - d_{t_i}\\right)}}$$\n\nand the estimate $\\widehat{S}(0) = 1$ has no error.\n\nMortality Rate (@def-mortality) \n: For any particular time $t$, the morality rate $m(t)$ is the proportion of the population that experiences the event between times $t$ and $t + 1$, among individuals that are event-free at time $t$.  That is,\n\n$$m(t) = Pr(t \\leq T \\leq t + 1 \\mid T > t) = 1 - \\frac{S(t + 1)}{S(t)}.$$\n\nMulticollinearity (@def-multicollinearity) \n: When two predictors are highly correlated with one another, we say that there is multicollinearity in the model.\n\nNonlinear Model (@def-nlm) \n: A model is said to be nonlinear if it cannot be written as a linear combination of the parameters.\n\nNonparametric Model (@def-nonparametric-model) \n: A nonparametric model is unable to characterize the response using a finite set of parameters; for our purposes, this generally means the model makes no assumptions about the structure of the underlying distribution of the response given the predictors.  Only minimal assumptions (such as independence between observations) are imposed.\n\nNormal (Gaussian) Distribution (@def-normal-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Normal (or Gaussian) distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\qquad -\\infty < x < \\infty,$$\n\nwhere $\\mu$ is any real number and $\\sigma^2 > 0$.  \n\n- $E(X) = \\mu$\n- $Var(X) = \\sigma^2$\n\nWe write $X \\sim N\\left(\\mu, \\sigma^2\\right)$, which is read \"X has a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\"  This short-hand implies the density above.\n\nNumeric Variable (@def-numeric-variable) \n: Also called a \"quantitative variable,\" a measurement on a subject which takes on a numeric value _and_ for which ordinary arithmetic makes sense.\n\nObservational Study (@def-observational-study) \n: A study in which each participant \"self-selects\" into one of groups being compared in the study. The phrase \"self-selects\" is used very loosely here and can include studies in which the groups are defined by an inherent characteristic, the groups are determined according to a non-random mechanism, and each participant chooses the group to which they belong.\n\nOdds (@def-odds) \n: The odds of an event with probability $p$ is defined as\n\n$$\\frac{p}{1-p}.$$\n\nOdds Ratio (@def-or) \n: The odds ratio is a method of comparing two events; typically, it is formed by the ratio of the odds of the same event under two different scenarios.  Let $p_1$ be the probability of the event under scenario 1 and let $p_2$ be the probability of an event under scenario 2; then, the odds of the event under scenario 1 are\n\n$$\\gamma_1 = \\frac{p_1}{1 - p_1},$$\n\nand the odds of the event under scenario 2 are\n\n$$\\gamma_2 = \\frac{p_2}{1 - p_2}.$$\n\nThe odds ratio comparing scenario 1 to scenario 2 is\n\n$$OR = \\frac{\\gamma_1}{\\gamma_2} = \\left(\\frac{p_1}{1 - p_1}\\right) \\left(\\frac{1 - p_2}{p_2}\\right).$$\n\nOverfitting (@def-overfitting) \n: Overfitting refers to constructing a model that accurately predicts the observed data at the expense of accurate variance estimation and model parsimony.  As a result, the model will have poor prediction for future observations.\n\nP-Value (@def-pvalue) \n: The probability, assuming the null hypothesis is true, that we would observe a statistic, from sampling variability alone, as extreme or more so as that observed in our sample.  This quantifies the strength of evidence against the null hypothesis.  Smaller values indicate stronger evidence.\n\nP-Value for Testing if Parameter Belongs in Model Under Classical Model (@def-classical-p) \n: Under the classical regression conditions (@def-classical-regression), the p-value for testing the hypotheses \n\n$$H_0: \\beta_j = 0 \\qquad \\text{vs.} \\qquad H_1: \\beta_j \\neq 0$$\n\nis given by \n\n$$Pr\\left(\\abs{T} > \\abs{\\frac{\\widehat{\\beta}_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}}}\\right)$$\n\nwhere $T \\sim t_{n-p-1}$.\n\nParameter (@def-parameter) \n: Numeric quantity which summarizes the distribution of a variable within the _population_ of interest.  Generally denoted by Greek letters in statistical formulas.\n\nParametric Model (@def-parametric-model) \n: A parametric model characterizes the distribution of the response using a finite set of parameters; for our purposes, this generally means the model _fully_ characterize the distribution of the response given the predictors.\n\nPopulation (@def-population) \n: The collection of subjects we would like to say something about.\n\nPopulation Averaged Models (@def-population-averaged) \n: Also known as marginal modeling, the population-averaged approach posits a model for the mean response directly and addresses the correlation through directly modeling its structure.\n\nPopulation-Level Model (@def-population-model) \n: The population-level model characterizes how the _parameters_ of the individual-level model vary across subjects (or blocks) in the population.\n\nPower of the Mean Model (@def-pom) \n: The power of the mean model allows the variance to be specified as a power of the mean response function.  Specifically, we consider\n\n$$\n\\begin{aligned}\n  E\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]\n    &= f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right) \\\\\n  Var\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]\n    &= \\sigma^2 \\left[f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right)\\right]^{2\\theta}\n\\end{aligned}\n$$\n\nProportional Hazards (@def-proportional-hazards) \n: Let $\\lambda_1(t)$ and $\\lambda_2(t)$ represent the hazard functions for two different groups.  The assumption of proportional hazards states that\n\n$$\\frac{\\lambda_2(t)}{\\lambda_1(t)} = e^{\\gamma}$$\n\nfor some fixed $\\gamma$.  That is, the ratio of the hazard functions does not depend on time.\n\nR-squared (@def-r-squared) \n: The proportion of the variability in the response explained by the model. When the response is quantitative, R-squared is defined as\n\n$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum_{i=1}^{n} (\\text{Residuals})_i^2}{\\sum_{i=1}^{n} \\left((\\text{Response})_i - (\\text{Overall Average Response})\\right)^2}.$$\n\nWhen the response is categorical (such as logistic regression), there is no unique definition of R-squared.\n\nRandom Censoring (@def-random-censoring) \n: Random censoring is a form of right censoring when subjects are withdrawn from the study at any time.  It is typically assumed that the event time and the censoring time are independent of one another.\n\nRandom Effect (@def-random-effect) \n: Random effects are terms in the model that capture the correlation induced due to an inherent characteristic that varies across the population.  We are _not_ interested in the specific grouping levels, and we either are not interested in the relationship with the response.\n\nRandom Variable (@def-random-variable) \n: A random variable represents a measurement that will be collected and for which the value cannot be predicted with certainty; they are generally represented with a capital letter.  Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements.\n\nRandomization (@def-randomization) \n: Randomization can refer to random _selection_ or random _allocation_.\n\nRandom selection refers to the use of a random mechanism to select units from the population.  Random selection minimizes bias.\n\nRandom allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment.  Random allocation eliminates confounding and permits causal interpretations.\n\nRandomized Clinical Trial (@def-randomized-clinical-trial) \n: Also called a \"controlled experiment,\" a study in which each participant is randomly assigned to one of the groups being compared in the study.\n\nRandomized Complete Block Design (@def-rcbd) \n: A randomized complete block design is an example of a controlled experiment utilizing blocking. Each treatment is randomized to observations within blocks in such a way that every treatment is present within the block and the same number of observations are assigned to each treatment within each block.\n\nReduction of Noise (@def-noise-reduction) \n: Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or through blocking.  These actions reduce the number of differences between the units under study.\n\nReference Group (@def-reference-group) \n: The group defined by having all indicator variables for a particular categorical variable set to zero.\n\nRepeated Measures (@def-repeated-measures) \n: The phrase \"repeated measures\" refers to data for which the observed responses can be grouped based on some nuisance variable (typically the participant), and this grouping captures some inherent characteristic such that observations within a group tend to be more alike than observations across groups.\n\nReplication (@def-replication) \n: Replication results from taking measurements on different units (or subjects) for which you expect the results to be similar.  That is, any variability across the units is due to natural variability within the population.\n\nResidual (@def-residual) \n: A residual for the $i$-th observation is the difference between an observed value and the predicted response:\n\n$$\n\\begin{aligned}\n  (\\text{Residual})_i \n    &= (\\text{Observed Response})_i - (\\text{Predicted Response})_i \\\\\n    &= (\\text{Response})_i - \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_i\\right).\n\\end{aligned}\n$$\n\nResidual Bootstrap (@def-residual-bootstrap) \n: Suppose we observe a sample of size $n$ and use the data to compute the least squares estimates $\\widehat{\\bs{\\beta}}$ for the parameters in the model\n\n$$(\\text{Response})_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i + \\varepsilon_i.$$\n\nThe residual bootstrap proceeds according to the following algorithm:\n\n  1. Compute the residuals \n  \n  $$(\\text{Residuals})_i = (\\text{Response})_i - (\\text{Predicted Response})_i$$\n  \n  2. Take a random sample of size $n$ (with replacement) of the residuals; call these values $e_1^*, \\dotsc, e_n^*$.\n  3. Form \"new\" responses $y_1^*, \\dotsc, y_n^*$ according to\n  \n  $$y_i^* = \\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_i + e_i^*.$$\n  \n  4. Obtain the least squares estimates $\\widehat{\\bs{\\alpha}}$ by finding the values of $\\bs{\\alpha}$ that minimize \n  \n  $$\\sum_{i=1}^{n} \\left(y_i^* - \\alpha_0 - \\sum_{j=1}^{p} \\alpha_j (\\text{Predictor } j)_i\\right)^2.$$\n  \n  5. Repeat steps 2-4 $m$ times.\n\nWe often take $m$ to be large (at least 1000).  After each pass through the algorithm, we retain the least squares estimates $\\widehat{\\bs{\\alpha}}$ from the resample.  The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.\n\nResponse Variable (@def-response) \n: Also called the \"outcome,\" this is the primary variable of interest in the research question; it is the variable we either want to explain or predict.\n\nRestricted Cubic Spline (@def-restricted-cubic-spline) \n: A restricted cubic spline is a continuous function comprised of piecewise cubic polynomials for which the tails of the spline have been restricted to be linear.\n\nRight Censoring (@def-right-censoring) \n: Right censoring refers to scenarios when a _lower_ bound is known on the response.\n\nRobust Sandwich Estimator (@def-robust-sandwich-estimator) \n: The robust sandwich estimator of the variance-covariance matrix of the parameter estimates from the mean model balances the relationship between the parameter estimates specified by the model (and the \"working\" correlation matrix) with the relationship suggested by the observed data.  Specifically, it has the form\n\n$$\\widehat{\\bs{\\Sigma}} = \\widehat{\\bm{U}} \\widehat{\\bm{U}}^{-1/2} \\bm{R} \\widehat{\\bm{U}}^{-1/2} \\widehat{\\bm{U}}$$\n\nwhere $\\bm{U}$ represents the model-based variance-covariance matrix if the structure specified by the working correlation matrix were completely correct, and $\\bm{R}$ represents the correction factor estimated from the residuals (an empirical estimate).\n\nSample (@def-sample) \n: The collection of subjects for which we actually obtain measurements (data).\n\nSampling Distribution of the Least Squares Estimates (@def-ls-sampling-distribution) \n: Under the classical regression conditions (@def-classical-regression), we have that\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim t_{n - p - 1}.$$\n\nThe denominator $\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}$ is known as the _standard error_ of the estimate $\\widehat{\\beta}_j$.  This formula holds for all $j = 0, 1, \\dotsc, p$.\n\nSemiparametric Linear Model (@def-semiparametric-linear-model) \n: Suppose we no longer require that the error terms follow a Normal distribution; however, we do continue to impose the remaining conditions of the classical regression model.  Then, our model could be written as\n\n$$\n\\begin{aligned}\n  E\\left[(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i\\right]\n    &= \\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i \\\\\n  Var\\left[(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i\\right]\n    &= \\sigma^2\n\\end{aligned}\n$$\n\nwhere the responses are independent of one another given the predictors.\n\nSemiparametric Model (@def-semiparametric-model) \n: A semiparametric model specifies some components of the underlying distribution of the response using a finite set of parameters but does not fully characterize the distribution.  This generally means that we may specify the mean and/or variance of the response given the predictors, but we do not characterize the distributional family of the response.\n\nSemiparametric Nonlinear Model (@def-semiparametric-nonlinear-model) \n: A semiparametric nonlinear model specifies the _mean_ and _variance_ of the response given the predictors; we write\n\n$$\n\\begin{aligned}\n  E\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right] \n    &= f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right) \\\\\n  Var\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]\n    &= \\sigma^2\n\\end{aligned}\n$$\n\nwhere $f(\\cdot)$ is referred to as the mean response function.\n\nSlope (@def-slope) \n: The coefficient for the $j$-th predictor, denoted $\\beta_j$, is the change in the mean response associated with a one unit increase in Predictor $j$, _holding all other predictors fixed_.\n\nSpaghetti Plot (@def-spaghetti-plot) \n: A spaghetti plot is a scatterplot that displays the trends within a subject, highlighting the correlation structure by connecting points from the same subject.\n\nSpline (@def-spline) \n: A spline is a continuous piecewise polynomial used to model curvature.  The points that define the piecewise components are called _knot points_; the functional form is allowed to change at the knot points.\n\nStationarity (@def-stationarity) \n: The assumption of stationarity states that the correlation structure does not depend on time, only the distance between the observations.\n\nStatistic (@def-statistic) \n: Numeric quantity which summarizes the distribution of a variable within the observed _sample_.\n\nStatistical Inference (@def-inference) \n: The process of using a sample to characterize some aspect of the underlying population.\n\nSubgroup Analysis (@def-subgroup-analysis) \n: Refers to repeating a specified analysis (e.g., regression model) within various levels of a categorical predictor.\n  \n  - This will appropriately estimate the effect modification.\n  - This results in a loss of information because _all parameters_ are forced to vary across the subgroups.\n\nSubject Specific Models (@def-subject-specific) \n: Also known as conditional modeling, the subject-specific approach models at the subject-level and addresses the correlation indirectly through the inclusion of random effects.\n\nSubsampling (@def-subsampling) \n: Subsampling occurs when several measurements are taken on each subject under the same treatment, possibly at unique locations.\n\nSurvival Function (@def-survival) \n: Let $T$ be a random variable; the survival function $S(u)$ is defined as\n\n$$S(t) = Pr(T > t) = 1 - F(t),$$\n\ncapturing the probability of failing _after_ a time, where $F(t)$ is the CDF of $T$.\n\nFor a continuous random variable, we have that\n\n$$S(t) = \\int_{t}^{\\infty} f(u) du$$\n\nimplying that $f(t) = -\\frac{d}{dt} S(t)$.\n\nType I Censoring (@def-type-1-censoring) \n: Type-I censoring is a form of right censoring where the only source of censoring is the end of the study, for which the duration was pre-determined.  Therefore, the time at which subjects are censored is the pre-determined study duration.\n\nType II Censoring (@def-type-2-censoring) \n: Type-II censoring is a form of right censoring where the only source of censoring is the end of the study, which is determined when the $r$-th event occurs and $r$ is pre-determined.  Therefore, the time at which subjects are censored is determined by the $r$-th event.\n\nUnstructured Correlation Structure (@def-unstructured-correlation-structure) \n: An unstructured correlation structure suggests that the correlation between any two errors within a subject can take on any value.  We only require that it be a valid correlation matrix.\n\nVariable (@def-variable) \n: A measurement, or category, describing some aspect of the subject.\n\nVariance-Covariance Matrix (@def-variance-covariance-matrix) \n: Let $\\bs{\\beta}$ represent the $(p+1)$-length vector of the parameters and $\\widehat{\\bs{\\beta}}$ represent the $(p+1)$ vector of the parameter _estimates_.  The variance-covariance matrix of the parameter estimates is the $(p+1)$-by-$(p+1)$ matrix $\\bs{\\Sigma}$ where\n\n- the $j$-th diagonal element contains $Var\\left(\\widehat{\\beta}_j\\right)$, and\n- the $(i,j)$-th element contains the covariance between $\\widehat{\\beta}_i$ and $\\widehat{\\beta}_j$.\n\nWild Bootstrap (@def-wild-bootstrap) \n: Suppose we observe a sample of size $n$ and use it to fit the mean model (linear or nonlinear)\n\n$$E\\left[(\\text{Response})_i \\mid (\\text{Predictors})_i\\right]= f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right)$$\n\nto obtain the ordinary least squares estimates $\\widehat{\\bs{\\beta}}$.  The wild bootstrap proceeds along the following algorithm:\n\n  1. Compute the residuals \n  $$(\\text{Residual})_i = (\\text{Response})_i - f\\left((\\text{Predictors})_i, \\widehat{\\bs{\\beta}}\\right)$$\n  2. Construct new pseudo-residuals $e_1^*, \\dotsc, e_n^*$ by multiplying each residual by a random variable $U$ such that $E\\left(U_i\\right) = 0$ and $Var\\left(U_i\\right) = 1$, for example $U_i \\sim N(0,1)$:\n  $$e_i^* = U_i (\\text{Residual})_i$$\n  3. Form \"new\" responses $y_1^*, \\dotsc, y_n^*$ according to\n  $$y_i^* = f\\left((\\text{Predictors})_i, \\widehat{\\bs{\\beta}}\\right) + e_i^*.$$\n  4. Obtain the least squares estimates $\\widehat{\\bs{\\alpha}}$ by finding the values of $\\bs{\\alpha}$ which minimize \n  $$\\sum_{i=1}^{n} \\left(y_i^* - f\\left((\\text{Predictors})_i, \\bs{\\alpha}\\right)\\right)^2.$$\n  5. Repeat steps 2-4 $m$ times.\n\nWe often take $m$ to be large (at least 1000).  After each pass through the algorithm, we retain the least squares estimates $\\widehat{\\bs{\\alpha}}$ from the resample.  The distribution of the estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.\n\nt-Distribution (@def-t-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a t-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}} \\qquad x > 0$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim t_{\\nu}$, which is read \"X has a t-distribution with $\\nu$ degrees of freedom.\"\n",
    "supporting": [
      "app-glossary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}