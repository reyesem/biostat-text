{
  "hash": "2a4667322e5166e1c3b286d34d039062",
  "result": {
    "markdown": "# Glossary\n\n\n\n\n\\providecommand{\\norm}[1]{\\lVert#1\\rVert}\n\\providecommand{\\abs}[1]{\\lvert#1\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\nThe following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.\n\n\n\nAlternate Characterization of the Classical Regression Model (@def-alternate-characterization) \n: Under the classical regression conditions on the error term (see @def-classical-regression), we can characterize the classical regression model as\n\n$$(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i \\dist{Ind} N\\left(\\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i, \\sigma^2\\right).$$\n\nHere, the symbol $\\mid$ is read \"given\" and means that the distribution of the response is specified after knowing the values of the predictors.  That is, the distribution of the response depends on these variables.\n\nAutoregressive Correlation Structure (@def-autoregressive-correlation-structure) \n: An autoregressive correlation structure suggests the correlation between two observations diminishes as the observations get further apart (generally, further apart in time).  We generally only consider the autoregressive structure of degree 1 here; if there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & \\rho & \\rho^2 & \\rho^3 & \\rho^4 \\\\\n\\cdot & 1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\cdot & \\cdot & 1 & \\rho & \\rho^2 \\\\\n\\cdot & \\cdot & \\cdot & 1 & \\rho \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nBernoulli Distribution (@def-bernoulli-distribution) \n: Let $X$ be a discrete random variable taking the value 0 or 1.  $X$ is said to have a Bernoulli distribution with density\n\n$$f(x) = \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1\\},$$\n\nwhere $0 < \\theta < 1$ is the probability that $X$ takes the value 1.\n\n- $E(X) = \\theta$\n- $Var(X) = \\theta(1 - \\theta)$\n\nWe write $X \\sim Ber(\\theta)$, which is read \"X has a Bernoulli distribution with probability $\\theta$.\"\n\nBlocking (@def-blocking) \n: Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.\n\nIn both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.\n\nBootstrapping (@def-bootstrapping) \n: A process of constructing a sampling distribution of the parameter estimates through resampling.  The observed data is resampled repeatedly, and the parameters of interest are estimated in each resample.  The distribution of these estimates across the resamples is then used as an empirical model of the corresponding sampling distributions.\n\nCase Resampling Bootstrap (@def-case-resampling-bootstrap) \n: Suppose we observe a sample of size $n$ and use the data to compute the least squares estimates $\\widehat{\\bs{\\beta}}$ for the parameters in the model\n\n$$(\\text{Response})_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i + \\varepsilon_i.$$\n\nThe case resampling bootstrap proceeds according to the following algorithm:\n\n  1. Take a random sample of size $n$ (with replacement) of the raw data (keeping all variables from the same observation together); denote the $i$-th selected response and predictors $(\\text{Response})_i^*$ and $(\\text{Predictor } j)_i^*$, respectively.\n  2. Obtain the least squares estimates $\\widehat{\\bs{\\alpha}}$ by finding the values of $\\bs{\\alpha}$ that minimize \n  \n  $$\\sum_{i=1}^{n} \\left((\\text{Response})_i^* - \\alpha_0 - \\sum_{j=1}^{p} \\alpha_j (\\text{Predictor } j)_i^*\\right)^2.$$\n  \n  3. Repeat steps 1-2 $m$ times.\n\nWe often take $m$ to be large (at least 1000).  After each pass through the algorithm, we retain the least squares estimates $\\widehat{\\bs{\\alpha}}$ from the resample.  The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.\n\nCategorical Variable (@def-categorical-variable) \n: Also called a \"qualitative variable,\" a measurement on a subject which denotes a grouping or categorization.\n\nCentral Limit Theorem (@def-clt) \n: Let $Y_1, Y_2, \\dotsc, Y_n$ be independent and identically distributed random variables with finite mean $\\mu$ and variance $\\sigma^2$.  Then, as $n$ approaches infinity, the distribution of the ratio\n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{\\sigma}$$\n\napproaches that of a Standard Normal random variable.\n\nChi-Square Distribution (@def-chi-square-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Chi-Square distribution if the density is given by\n\n$$f(x) = \\frac{1}{2^{\\nu/2}\\Gamma (\\nu/2)}\\;x^{\\nu/2-1}e^{-x/2} \\qquad x > 0,$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim \\chi^2_{\\nu}$, which is read \"X has a Chi-Square distribution with $\\nu$ degrees of freedom.\"\n\nClassical Regression (Conditions on Predictors) (@def-classical-regression-cont) \n: The classical regression model (@def-classical-regression) places the following conditions on the predictors:\n\n  1. Each predictor is measured without error.\n  2. Each predictor has an additive linear effect on the response.\n\nClassical Regression Model (@def-classical-regression) \n: In the \"classical regression model,\" we place the following four conditions on the distribution of the error $\\varepsilon_i$:\n\n  1. The average error across all levels of the predictors is 0; mathematically, we write $E\\left(\\varepsilon_i \\mid (\\text{Predictors 1 - }p)_i\\right) = 0$.\n  2. The variance of the errors is constant across all levels of the predictors; mathematically, we write $Var\\left(\\varepsilon_i \\mid (\\text{Predictors 1 - }p)_i\\right) = \\sigma^2$ for some unknown constant $\\sigma^2 > 0$.  This is sometimes referred to as homoskedasticity.\n  3. The error terms are independent; in particular, the magnitude of the error for one observation does not influence the magnitude of the error for any other observation.\n  4. The distribution of the errors follows a Normal distribution with the above mean and variance.\n\nCluster Samples (@def-cluster-samples) \n: Stratified sampling divides a population into groups and samples from within each group; in contrast, cluster sampling divides the population into groups and randomly samples a few groups and takes measurements from within the group.\n\nCodebook (@def-codebook) \n: Also called a \"data dictionary,\" a codebook provides complete information regarding the variables contained within a dataset.\n\nCompound Symmetric Correlation Structure (@def-compound-symmetric-correlation-structure) \n: A compound symmetric correlation structure, also known as an _exchangeable_ correlation structure, suggests the correlation between any two errors within a subject is equal.  If there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & \\rho & \\rho & \\rho & \\rho \\\\\n\\cdot & 1 & \\rho & \\rho & \\rho \\\\\n\\cdot & \\cdot & 1 & \\rho & \\rho \\\\\n\\cdot & \\cdot & \\cdot & 1 & \\rho \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nConfidence Interval (@def-confidence-interval) \n: An interval (range of values) estimate of a parameter that incorporates the variability in the statistic.  The process of constructing $k$% confidence intervals results in them containing the parameter of interest in $k$% of _repeated_ studies.  The value of $k$ is called the _confidence level_.\n\nConfidence Interval for Parameters Under Classical Model (@def-classical-ci) \n: Under the classical regression conditions (@def-classical-regression), a $100c$% confidence interval for the parameter $\\beta_j$ is given by\n\n$$\\widehat{\\beta}_j \\pm t_{n-p-1, 0.5(1+c)} \\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}.$$\n\nwhere $t_{n-p-1, 0.5(1+c)}$ is the $0.5(1+c)$ quantile from the $t_{n-p-1}$ distribution, known as the critical value for the confidence interval.\n\nConfounding (@def-confounding) \n: When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.\n\nCorrelation Structure (@def-correlation-structure) \n: The correlation structure quantifies the strength and direction of the relationship between the errors in the observed responses.\n\nCross Sectional Study (@def-cross-sectional-study) \n: A cross sectional study considers data from a single snapshot in time.\n\nCross-Over Study (@def-cross-over-study) \n: A cross-over study exposes each participant to multiple treatments.  Whenever possible, the order of the treatments is randomly determined.  This is equivalent to a randomized complete block design in which the blocks are the participants.  When the treatments are believed to have a lingering effect, a wash-out period between treatments is used to minimize the impact of previous treatments on the treatment the participant is currently being exposed to.\n\nCumulative Distribution Function (CDF) (@def-cdf) \n: Let $X$ be a random variable; the cumulative distribution function (CDF) is defined as\n\n$$F(u) = Pr(X \\leq u).$$\n\nFor a continuous random variable, we have that\n\n$$F(u) = \\int_{-\\infty}^{u} f(x) dx$$\n\nimplying that the density function is the derivative of the CDF.  For a discrete random variable\n\n$$F(u) = \\sum_{x \\leq u} f(x).$$\n\nDensity Function (@def-density-function) \n: A density function $f$ relates the potential values of a random variable $X$ with the probability those values occur.  For a _continuous_ random variable, the probability the random variable $X$ falls within an interval $(a, b)$ is given by\n\n$$Pr(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx.$$\n\nFor a _discrete_ random variable, the probability the random variable $X$ is equal to the value $u$ is given by\n\n$$Pr(X = u) = f(u).$$\n\nDistribution (@def-distribution) \n: The pattern of variability corresponding to a set of values.\n\nEstimate of the Variance of the Errors (@def-estimate-sigma2) \n: The unknown variance in the linear model, which captures the variability in the response for any set of predictors (also called the residual variance), is estimated by\n\n$$\\widehat{\\sigma}^2 = \\frac{1}{n-p-1} \\sum\\limits_{i=1}^{n} \\left((\\text{Response})_i - \\widehat{\\beta}_0 - \\sum\\limits_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_{i}\\right)^2.$$\n\nF-Distribution (@def-f-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have an F-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma((r + s)/2)}{(\\Gamma(r/2) \\Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \\qquad x > 0,$$\n\nwhere $r,s > 0$ are the numerator and denominator degrees of freedom, respectively.\n\nWe write $X \\sim F_{r, s}$, which is read \"X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.\"\n\nFixed Effect (@def-fixed-effect) \n: Fixed effects are terms in the model for which we are interested in both the specific grouping levels, and we are interested in characterizing the relationship between these levels and the response.\n\nGeneral Linear Hypothesis (@def-general-linear-hypothesis) \n: The general linear hypothesis framework refers to testing hypotheses of the form \n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m} \\qquad \\text{vs.} \\qquad H_1: \\bm{K}\\bs{\\beta} \\neq \\bm{m}$$\n\nwhere\n\n  - $\\bs{\\beta}$ is the $(p+1)$-length vector of the parameters (includes the intercept),\n  - $\\bm{K}$ is an $r$-by-$(p+1)$ matrix that specifies the linear combinations defining the hypothesis of interest, and\n  - $\\bm{m}$ is a vector of length $r$ specifying the null values, the value of each linear combination under the null hypothesis (often a vector of 0's).\n\nGeneral Linear Model (@def-general-linear-model) \n: The general linear model views the response (outcome) as a linear combination of several predictors:\n  \n$$\n\\begin{aligned}\n  (\\text{Response})_i \n    &= \\beta_0 + \\beta_1 (\\text{Predictor 1})_{i} + \\beta_2 (\\text{Predictor 2})_{i} + \\dotsb + \n      \\beta_p (\\text{Predictor } p)_{i} + \\varepsilon_i \\\\\n    &= \\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_{i} + \\varepsilon_i\n\\end{aligned}\n$$\n\nwhere $n$ is the number of subjects in the sample, $p < n$ is the number of predictors in the model, and $\\varepsilon_i$ is a random variable that captures the error in the response.\n\nGeneralized Estimating Equations (GEE) (@def-gee) \n: Generalized estimating equations can be used to estimate the parameters of a model while accounting for the correlation among observations.  In addition to specifying a model for the overall average response, a \"working\" structure is specified for the correlation of observations from the same subject.  The working structure is updated during the estimation process and used to adjust the standard errors of the parameter estimates in the mean model.\n\nHierarchical Model (@def-hierarchical-model) \n: A hierarchical model breaks the data generating process into smaller stages and posits a model for each stage.  The stages are determined by defining a hierarchy of units and thereby capturing the sources of variability.\n\nIndependence Correlation Structure (@def-independence-correlation-structure) \n: An independence correlation structure suggests there is no correlation among any of the error terms within a subject.  If there are five observations within a block, this has the form\n\n$$\\Gamma = \\begin{pmatrix} \n1 & 0 & 0 & 0 & 0 \\\\\n\\cdot & 1 & 0 & 0 & 0 \\\\\n\\cdot & \\cdot & 1 & 0 & 0 \\\\\n\\cdot & \\cdot & \\cdot & 1 & 0 \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & 1 \\end{pmatrix}.$$\n\nIndicator Variables (@def-indicator-variables) \n: Also called \"dummy variables,\" these are a set of binary variables that capture the grouping defined by a categorical variable for regression modeling.\n\nIndividual-Level Model (@def-individual-model) \n: The individual-level model characterizes the response for the $i$-th subject (or block) only.\n\nInteraction (@def-interaction) \n: An interaction term allows the effect of a predictor on the response to depend on the value of a second predictor (capturing an effect modification).\n\n  - The interaction term is created by adding the product of the two predictors under consideration to the model.\n\nIntercept (@def-intercept) \n: The population intercept, denoted $\\beta_0$, is the _mean_ response when all predictors take the value zero.\n\nLarge Sample Model for the Sampling Distribution of the Least Squares Estimates (@def-ls-sampling-distribution-large-samples) \n: Suppose the classical regression conditions hold, with the exception of the errors following a Normal distribution.  As the sample size gets large, we have that the distribution of the ratio\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim N(0, 1)$$\n\nfor all $j = 0, 1, \\dotsc, p$.  Further, under the null hypothesis\n\n$$H_0: \\bm{K}\\bs{\\beta} = \\bm{m}$$\n\nwe have\n\n$$\\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim \\chi^2_r.$$\n\nLarge Sample Theory (@def-large-sample-theory) \n: The phrase \"large sample theory\" (or \"asymptotics\") is used to describe a scenario when the model for the sampling distribution (or null distribution) of an estimate (or standardized statistic) can be approximated as the sample size becomes infinitely large.  That is, as the sample size approaches infinity, the sampling distribution (or null distribution) can be easily modeled using a known probability distribution.\n\nLeast Squares Estimation (@def-least-squares) \n: The method of least squares may be used to estimate the coefficients (parameters) of a linear model.  In particular, we choose the values of the coefficients that minimize\n\n$$\\sum\\limits_{i=1}^{n} \\left((\\text{Response})_i - \\beta_0 - \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_{i}\\right)^2.$$\n\nThe resulting \"least squares\" estimates are denoted $\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\dotsc, \\widehat{\\beta}_p$.\n\nLinear Model (@def-linear-model) \n: A model is said to be linear if it can be expressed as a linear combination of the _parameters_.  That is, the linearity does not refer to the form of the predictors but the form of the parameters.\n\nLinear Spline (@def-linear-spline) \n: A linear spline is a continuous piecewise linear function.\n\nLongitudinal Study (@def-longitudinal-study) \n: A longitudinal study repeatedly measures the response on each subject at various points in time.\n\nMean and Variance of a Random Variable (@def-rv-mean-variance) \n: Suppose $X$ is a random variable with density function $f$.  If $X$ is a continuous random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\int x f(x) dx \\\\\n  Var(X) &= \\int \\left(x - E(X)\\right)^2 f(x) dx.\n\\end{aligned}\n$$\n\nIf $X$ is a discrete random variable, then the mean and variance are given by\n\n$$\n\\begin{aligned}\n  E(X) &= \\sum x f(x) \\\\\n  Var(X) &= \\sum \\left(x - E(X)\\right)^2 f(x).\n\\end{aligned}\n$$\n\nMixed-Effects Model (@def-mixed-effects-model) \n: A mixed-effects model denotes a hierarchical model for which some effects are fixed (not allowed to vary across subjects) and others are random (allowed to vary across subjects).\n\nModel for the Null Distribution with the General Linear Hypothesis (@def-general-linear-hypothesis-null) \n: Let $\\widehat{\\bs{\\beta}}$ be the $(p+1)$ vector of estimates for the parameter vector $\\bs{\\beta}$, and let the estimates have variance-covariance matrix $\\bs{\\Sigma}$.  Assuming the null hypothesis \n\n$$H_0: \\bm{K} \\bs{\\beta} = \\bm{m}$$\n\nis true, under the conditions of the classical regression model (@def-classical-regression)\n\n$$(1/r) \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right)^\\top \\left(\\bm{K}\\widehat{\\bs{\\Sigma}}\\bm{K}^\\top\\right)^{-1} \\left(\\bm{K}\\widehat{\\bs{\\beta}} - \\bm{m}\\right) \\sim F_{r, n-p-1}.$$\n\nMulticollinearity (@def-multicollinearity) \n: When two predictors are highly correlated with one another, we say that there is multicollinearity in the model.\n\nNonparametric Model (@def-nonparametric-model) \n: A nonparametric model is unable to characterize the response using a finite set of parameters; for our purposes, this generally means the model makes no assumptions about the structure of the underlying distribution of the response given the predictors.  Only minimal assumptions (such as independence between observations) are imposed.\n\nNormal (Gaussian) Distribution (@def-normal-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Normal (or Gaussian) distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\qquad -\\infty < x < \\infty,$$\n\nwhere $\\mu$ is any real number and $\\sigma^2 > 0$.  \n\n- $E(X) = \\mu$\n- $Var(X) = \\sigma^2$\n\nWe write $X \\sim N\\left(\\mu, \\sigma^2\\right)$, which is read \"X has a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\"  This short-hand implies the density above.\n\nNumeric Variable (@def-numeric-variable) \n: Also called a \"quantitative variable,\" a measurement on a subject which takes on a numeric value _and_ for which ordinary arithmetic makes sense.\n\nObservational Study (@def-observational-study) \n: A study in which each participant \"self-selects\" into one of groups being compared in the study. The phrase \"self-selects\" is used very loosely here and can include studies in which the groups are defined by an inherent characteristic, the groups are determined according to a non-random mechanism, and each participant chooses the group to which they belong.\n\nP-Value (@def-pvalue) \n: The probability, assuming the null hypothesis is true, that we would observe a statistic, from sampling variability alone, as extreme or more so as that observed in our sample.  This quantifies the strength of evidence against the null hypothesis.  Smaller values indicate stronger evidence.\n\nP-Value for Testing if Parameter Belongs in Model Under Classical Model (@def-classical-p) \n: Under the classical regression conditions (@def-classical-regression), the p-value for testing the hypotheses \n\n$$H_0: \\beta_j = 0 \\qquad \\text{vs.} \\qquad H_1: \\beta_j \\neq 0$$\n\nis given by \n\n$$Pr\\left(\\abs{T} > \\abs{\\frac{\\widehat{\\beta}_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}}}\\right)$$\n\nwhere $T \\sim t_{n-p-1}$.\n\nParameter (@def-parameter) \n: Numeric quantity which summarizes the distribution of a variable within the _population_ of interest.  Generally denoted by Greek letters in statistical formulas.\n\nParametric Model (@def-parametric-model) \n: A parametric model characterizes the distribution of the response using a finite set of parameters; for our purposes, this generally means the model _fully_ characterize the distribution of the response given the predictors.\n\nPopulation (@def-population) \n: The collection of subjects we would like to say something about.\n\nPopulation Averaged Models (@def-population-averaged) \n: Also known as marginal modeling, the population-averaged approach posits a model for the mean response directly and addresses the correlation through directly modeling its structure.\n\nPopulation-Level Model (@def-population-model) \n: The population-level model characterizes how the _parameters_ of the individual-level model vary across subjects (or blocks) in the population.\n\nRandom Effect (@def-random-effect) \n: Random effects are terms in the model that capture the correlation induced due to an inherent characteristic that varies across the population.  We are _not_ interested in the specific grouping levels, and we either are not interested in the relationship with the response.\n\nRandom Variable (@def-random-variable) \n: A random variable represents a measurement that will be collected and for which the value cannot be predicted with certainty; they are generally represented with a capital letter.  Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements.\n\nRandomization (@def-randomization) \n: Randomization can refer to random _selection_ or random _allocation_.\n\nRandom selection refers to the use of a random mechanism to select units from the population.  Random selection minimizes bias.\n\nRandom allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment.  Random allocation eliminates confounding and permits causal interpretations.\n\nRandomized Clinical Trial (@def-randomized-clinical-trial) \n: Also called a \"controlled experiment,\" a study in which each participant is randomly assigned to one of the groups being compared in the study.\n\nRandomized Complete Block Design (@def-rcbd) \n: A randomized complete block design is an example of a controlled experiment utilizing blocking. Each treatment is randomized to observations within blocks in such a way that every treatment is present within the block and the same number of observations are assigned to each treatment within each block.\n\nReduction of Noise (@def-noise-reduction) \n: Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or through blocking.  These actions reduce the number of differences between the units under study.\n\nReference Group (@def-reference-group) \n: The group defined by having all indicator variables for a particular categorical variable set to zero.\n\nRepeated Measures (@def-repeated-measures) \n: The phrase \"repeated measures\" refers to data for which the observed responses can be grouped based on some nuisance variable (typically the participant), and this grouping captures some inherent characteristic such that observations within a group tend to be more alike than observations across groups.\n\nReplication (@def-replication) \n: Replication results from taking measurements on different units (or subjects) for which you expect the results to be similar.  That is, any variability across the units is due to natural variability within the population.\n\nResidual (@def-residual) \n: A residual for the $i$-th observation is the difference between an observed value and the predicted response:\n\n$$\n\\begin{aligned}\n  (\\text{Residual})_i \n    &= (\\text{Observed Response})_i - (\\text{Predicted Response})_i \\\\\n    &= (\\text{Response})_i - \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_i\\right).\n\\end{aligned}\n$$\n\nResidual Bootstrap (@def-residual-bootstrap) \n: Suppose we observe a sample of size $n$ and use the data to compute the least squares estimates $\\widehat{\\bs{\\beta}}$ for the parameters in the model\n\n$$(\\text{Response})_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i + \\varepsilon_i.$$\n\nThe residual bootstrap proceeds according to the following algorithm:\n\n  1. Compute the residuals \n  \n  $$(\\text{Residuals})_i = (\\text{Response})_i - (\\text{Predicted Response})_i$$\n  \n  2. Take a random sample of size $n$ (with replacement) of the residuals; call these values $e_1^*, \\dotsc, e_n^*$.\n  3. Form \"new\" responses $y_1^*, \\dotsc, y_n^*$ according to\n  \n  $$y_i^* = \\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor } j)_i + e_i^*.$$\n  \n  4. Obtain the least squares estimates $\\widehat{\\bs{\\alpha}}$ by finding the values of $\\bs{\\alpha}$ that minimize \n  \n  $$\\sum_{i=1}^{n} \\left(y_i^* - \\alpha_0 - \\sum_{j=1}^{p} \\alpha_j (\\text{Predictor } j)_i\\right)^2.$$\n  \n  5. Repeat steps 2-4 $m$ times.\n\nWe often take $m$ to be large (at least 1000).  After each pass through the algorithm, we retain the least squares estimates $\\widehat{\\bs{\\alpha}}$ from the resample.  The distribution of these estimates across the resamples is a good empirical model for the sampling distribution of the original least squares estimates.\n\nResponse Variable (@def-response) \n: Also called the \"outcome,\" this is the primary variable of interest in the research question; it is the variable we either want to explain or predict.\n\nRestricted Cubic Spline (@def-restricted-cubic-spline) \n: A restricted cubic spline is a continuous function comprised of piecewise cubic polynomials for which the tails of the spline have been restricted to be linear.\n\nRobust Sandwich Estimator (@def-robust-sandwich-estimator) \n: The robust sandwich estimator of the variance-covariance matrix of the parameter estimates from the mean model balances the relationship between the parameter estimates specified by the model (and the \"working\" correlation matrix) with the relationship suggested by the observed data.  Specifically, it has the form\n\n$$\\widehat{\\bs{\\Sigma}} = \\widehat{\\bm{U}} \\widehat{\\bm{U}}^{-1/2} \\bm{R} \\widehat{\\bm{U}}^{-1/2} \\widehat{\\bm{U}}$$\n\nwhere $\\bm{U}$ represents the model-based variance-covariance matrix if the structure specified by the working correlation matrix were completely correct, and $\\bm{R}$ represents the correction factor estimated from the residuals (an empirical estimate).\n\nSample (@def-sample) \n: The collection of subjects for which we actually obtain measurements (data).\n\nSampling Distribution of the Least Squares Estimates (@def-ls-sampling-distribution) \n: Under the classical regression conditions (@def-classical-regression), we have that\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim t_{n - p - 1}.$$\n\nThe denominator $\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}$ is known as the _standard error_ of the estimate $\\widehat{\\beta}_j$.  This formula holds for all $j = 0, 1, \\dotsc, p$.\n\nSemiparametric Linear Model (@def-semiparametric-linear-model) \n: Suppose we no longer require that the error terms follow a Normal distribution; however, we do continue to impose the remaining conditions of the classical regression model.  Then, our model could be written as\n\n$$\n\\begin{aligned}\n  E\\left[(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i\\right]\n    &= \\beta_0 + \\sum\\limits_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i \\\\\n  Var\\left[(\\text{Response})_i \\mid (\\text{Predictors 1 through } p)_i\\right]\n    &= \\sigma^2\n\\end{aligned}\n$$\n\nwhere the responses are independent of one another given the predictors.\n\nSemiparametric Model (@def-semiparametric-model) \n: A semiparametric model specifies some components of the underlying distribution of the response using a finite set of parameters but does not fully characterize the distribution.  This generally means that we may specify the mean and/or variance of the response given the predictors, but we do not characterize the distributional family of the response.\n\nSlope (@def-slope) \n: The coefficient for the $j$-th predictor, denoted $\\beta_j$, is the change in the mean response associated with a one unit increase in Predictor $j$, _holding all other predictors fixed_.\n\nSpaghetti Plot (@def-spaghetti-plot) \n: A spaghetti plot is a scatterplot that displays the trends within a subject, highlighting the correlation structure by connecting points from the same subject.\n\nSpline (@def-spline) \n: A spline is a continuous piecewise polynomial used to model curvature.  The points that define the piecewise components are called _knot points_; the functional form is allowed to change at the knot points.\n\nStationarity (@def-stationarity) \n: The assumption of stationarity states that the correlation structure does not depend on time, only the distance between the observations.\n\nStatistic (@def-statistic) \n: Numeric quantity which summarizes the distribution of a variable within the observed _sample_.\n\nStatistical Inference (@def-inference) \n: The process of using a sample to characterize some aspect of the underlying population.\n\nSubgroup Analysis (@def-subgroup-analysis) \n: Refers to repeating a specified analysis (e.g., regression model) within various levels of a categorical predictor.\n  \n  - This will appropriately estimate the effect modification.\n  - This results in a loss of information because _all parameters_ are forced to vary across the subgroups.\n\nSubject Specific Models (@def-subject-specific) \n: Also known as conditional modeling, the subject-specific approach models at the subject-level and addresses the correlation indirectly through the inclusion of random effects.\n\nSubsampling (@def-subsampling) \n: Subsampling occurs when several measurements are taken on each subject under the same treatment, possibly at unique locations.\n\nUnstructured Correlation Structure (@def-unstructured-correlation-structure) \n: An unstructured correlation structure suggests that the correlation between any two errors within a subject can take on any value.  We only require that it be a valid correlation matrix.\n\nVariable (@def-variable) \n: A measurement, or category, describing some aspect of the subject.\n\nVariance-Covariance Matrix (@def-variance-covariance-matrix) \n: Let $\\bs{\\beta}$ represent the $(p+1)$-length vector of the parameters and $\\widehat{\\bs{\\beta}}$ represent the $(p+1)$ vector of the parameter _estimates_.  The variance-covariance matrix of the parameter estimates is the $(p+1)$-by-$(p+1)$ matrix $\\bs{\\Sigma}$ where\n\n- the $j$-th diagonal element contains $Var\\left(\\widehat{\\beta}_j\\right)$, and\n- the $(i,j)$-th element contains the covariance between $\\widehat{\\beta}_i$ and $\\widehat{\\beta}_j$.\n\nt-Distribution (@def-t-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a t-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}} \\qquad x > 0$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim t_{\\nu}$, which is read \"X has a t-distribution with $\\nu$ degrees of freedom.\"\n",
    "supporting": [
      "app-glossary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}