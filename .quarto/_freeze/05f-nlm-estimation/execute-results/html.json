{
  "hash": "74bb4bd66fe03cf64812970a800577f9",
  "result": {
    "markdown": "# Estimation Details for Nonlinear Models {#sec-nlm-estimation}\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have stated that numerical methods are needed to obtain estimates of the parameters for nonlinear models.  In this chapter, we introduce the primary algorithm used for obtaining these estimates and sketch out some ideas for how this is implemented in practice.  Readers may skip this chapter without loss of continuity.\n\nWhen we begin working with nonlinear models, we will often come across computational issues.  In order to begin diagnosing these issues, we must have some understanding of the underlying algorithm.  Our objective is to find the values of the parameters $\\bs{\\beta}$ such that the mean function is close to the observed response.  One way of defining \"close\" is to consider minimizing the sum of squared residuals.  That is, we desire the values of $\\bs{\\beta}$ that minimize\n\n$$\\sum_{i=1}^{n} \\left((\\text{Response})_i - f\\left((\\text{Predictors})_i, \\bs{\\beta}\\right)\\right)^2.$$ {#eq-nlm-q}\n\nWe called the value that minimizes this objective function the ordinary least squares estimate.\n\n:::{.callout-warning}\nThis chapter applies to ordinary least squares.  Logistic regression, which makes use of maximum likelihood estimation has a slightly different objective function:\n\n$$\\sum_{i=1}^{n} (\\text{Response})_i \\log\\left(\\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}\\right) + \\sum_{i=1}^{n}\\left(1 - (\\text{Response})_i\\right)\\log\\left(1 - \\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}\\right)$$\n\nwhere $(\\text{Response})_i$ is an indicator function taking the value 1 when the response is a \"success\" and 0 otherwise.\n:::\n\nFor the remainder of this chapter, let $y_i$ denote the response and $\\bm{x}_i$ the vector of predictors for the $i$-th observation.  Then, minimizing the objective function in @eq-nlm-q is equivalent to finding the values of $\\bs{\\beta}$ that solve the system of equations given by\n\n$$\\bm{0} = \\sum_{i=1}^{n} \\left(y_i - f\\left(\\bm{x}_i, \\bs{\\beta}\\right)\\right) f_{\\bs{\\beta}}\\left(\\bm{x}_i, \\bs{\\beta}\\right),$$ {#eq-nlm-ee}\n\nwhere $f_{\\bs{\\beta}}$ represents the gradient vector (with respect to the parameters) of the mean function $f$.  Solving this system of equations is our primary objective.  This is typically done via the Gauss-Newton Method.\n\n:::{#def-gauss-newton}\n## Gauss-Newton Method\nThe Gauss-Newton method is the most commonly used optimization routine for statistical applications; this method relies on linearizing the problem using a Taylor Series approximation.\n:::\n\nObserve that for a value $\\bs{\\beta}^*$ close to $\\bs{\\beta}$, a first-order Taylor Series approximation gives\n\n$$\n\\begin{aligned}\n  f\\left(\\bm{x}_i,\\bs{\\beta}\\right)\n    &\\approx f\\left(\\bm{x}_i,\\bs{\\beta}^*\\right) + f^\\top_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)\\left(\\bs{\\beta} - \\bs{\\beta}^*\\right) \\\\\n  f_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}\\right)\n    &\\approx f_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right) + f_{\\bs{\\beta}\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)\\left(\\bs{\\beta} - \\bs{\\beta}^*\\right)\n\\end{aligned}\n$$\n\nwhere $f_{\\bs{\\beta}\\bs{\\beta}}$ is a $p$-by-$p$ matrix of second partial derivatives.  We appeal to a linear approximation because if $\\bs{\\beta}^*$ is sufficiently close to $\\bs{\\beta}$, then higher order terms are negligible.  \n\nNow, we substitute these approximations into @eq-nlm-ee, the system of equations we are solving.  This results in four terms, of which the last is small because it has quadratic terms, and the third is small because on average, we expect $y_i - f(\\bm{x}_i,\\bs{\\beta}^*)$ to be small, which when multiplied by $(\\bs{\\beta} - \\bs{\\beta}^*)$ is really small.  Thus, we are left with the approximation\n\n$$\n\\begin{aligned}\n  \\bm{0} &= \\sum_{i=1}^{n} \\left(y_i - f\\left(\\bm{x}_i, \\bs{\\beta}\\right)\\right) f_{\\bs{\\beta}}\\left(\\bm{x}_i, \\bs{\\beta}\\right) \\\\\n    &\\approx \\sum\\limits_{i=1}^{n} \\left(y_i - f(\\bm{x}_i,\\bs{\\beta}^*)\\right)f_{\\bs{\\beta}}(\\bm{x}_i,\\bs{\\beta}^*) \\\\\n    &\\qquad - \\sum\\limits_{i=1}^{n} f_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)f_{\\bs{\\beta}}^\\top\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)\\left(\\bs{\\beta}-\\bs{\\beta}^*\\right),\n\\end{aligned}\n$$\n\nwhich suggests\n\n$$\\sum_{i=1}^{n} f_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)f_{\\bs{\\beta}}^\\top\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)\\left(\\bs{\\beta}-\\bs{\\beta}^*\\right) \\approx \\sum\\limits_{i=1}^{n} \\left(y_i - f(\\bm{x}_i,\\bs{\\beta}^*)\\right)f_{\\bs{\\beta}}(\\bm{x}_i,\\bs{\\beta}^*).$$\n\nSolving this for $\\bs{\\beta}$, we obtain\n\n$$\\bs{\\beta} = \\bs{\\beta}^* + \\left[\\sum_{i=1}^{n} f_{\\bs{\\beta}}\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)f_{\\bs{\\beta}}^\\top\\left(\\bm{x}_i,\\bs{\\beta}^*\\right)\\right]^{-1}\\sum\\limits_{i=1}^{n} \\left(y_i - f(\\bm{x}_i,\\bs{\\beta}^*)\\right)f_{\\bs{\\beta}}(\\bm{x}_i,\\bs{\\beta}^*).$$\n\nThat is, we have an iterative scheme for updating estimates of $\\bs{\\beta}$ given a current estimate.  Given initial estimates $\\bs{\\beta}^{(0)}$, we perform these iterative updates, terminating the algorithm when two successive iterations yield estimates which are close (iterations converged).\n\nOne of the key lessons here is that starting estimates of the parameters are always needed.  This is always the case when numerical methods are used.  There are no all-encompassing strategies here; determining starting values is unique to each problem.  Often times, starting values can be obtained by making large simplifications or transformations to the model to get a rough idea of the estimates.\n\nAs an example, consider the Michaelis-Menten model for the mean response:\n\n$$E\\left((\\text{Response})_i \\mid (\\text{Predictor})_i\\right) = f\\left((\\text{Predictor})_i, \\bs{\\theta}\\right) = \\frac{\\theta_1 (\\text{Predictor})_i}{\\theta_2 + (\\text{Predictor})_i}.$$\n\nIn this model, $\\theta_1$ represents the maximum possible response and $\\theta_2$ the shape parameter known as the inverse affinity.  Starting values for this model are obtained by considering a transformation.  Specifically observe that if we assume the observed response is near the mean response given the corresponding value of the predictor, we have that\n\n$$(\\text{Response})_i \\approx \\frac{\\theta_1 (\\text{Predictor})_i}{\\theta_2 + (\\text{Predictor})_i}.$$\n\nTaking the inverse of the observed response suggests\n\n$$\\frac{1}{(\\text{Response})_i} \\approx \\frac{\\theta_2 + (\\text{Predictor})_i}{\\theta_1 (\\text{Predictor})_i} = \\frac{\\theta_2}{\\theta_1} \\left(\\frac{1}{(\\text{Predictor})_i}\\right) + \\frac{1}{\\theta_1}.$$\n\nThis motivates considering a linear model of the form\n\n$$(\\text{Inverse Response})_i = \\beta_0 + \\beta_1 (\\text{Inverse Predictor})_i + \\varepsilon_i.$$\n\nThe estimate of $\\beta_0$ will allow us to compute an estimate for $\\theta_1$ where\n\n$$\\widehat{\\theta}_1 = \\frac{1}{\\widehat{\\beta}_0},$$\n\nand the estimate of $\\beta_1$ will allow us to compute an estimate for $\\theta_2$ where\n\n$$\\widehat{\\theta}_2 = \\widehat{\\beta}_1 \\widehat{\\theta}_1.$$\n\nWe now have starting estimates that would allow us to resume fitting the nonlinear model on the original scale.  \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}