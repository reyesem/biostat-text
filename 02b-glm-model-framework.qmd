# General Linear Model Framework {#sec-glm-model-framework}

{{< include _setupcode.qmd >}}

The most interesting scientific questions involve characterizing the relationship between a response and some predictor.  And, we know that these relationships do not exist in a vacuum.  The response we observe is typically the result of a complex data generating process involving several potential predictors (or features/characteristics) of the subjects in the population.  In order to incorporate these additional features, we need _multivariable_ models.

The development of a model should not be divorced from its intended use, and in general, there are three uses for multivariable models.  That is, the majority of scientific questions can be categorized into one of three groups: prediction, isolating an effect, or studying the interplay between variables.

:::{.callout-tip}
## Big Idea
There are primarily three uses for a multivariable model

  - __Prediction__: modeling a relationship for the purpose of estimating a future occurrence given new data.
  - __Isolating an Effect__: describing the relationship between a response and predictor after accounting for the influence of other predictors measured.
  - __Studying the Interplay__: examining how the relationship of two variables is impacted by the value of a third variable.
:::

While we introduce these elements in the context of the general linear model, note that these uses carry over into other regression models we will examine.

Consider a gardener studying two common organic fertilizers.  She could have the following questions in mind:

  A.  What do I anticipate the yield of tomatoes to be next summer when using cow manure?
  B.  Does bat guano tend to result in higher tomato yields compared with cow manure after accounting for any impact on yield that results from the amount of water the plants receive?
  C.  Does the efficacy of bat guano (compared with cow manure) depend on the amount of sunlight the plants receive?

The first question is an example of prediction; given the fertilizer applied (as well as potentially other characteristics of the garden), what does she expect the results to be in the future?  The second question examines the impact (or effect) of the fertilizer _above and beyond_ any impact of watering; she is interested in _isolating_ the effect of fertilizer from the effect of watering.  In the last question, she is not only interested in the effect of the fertilizer on the yield, but she wants to acknowledge that this impact could depend on a third variable (sunlight); for example, bat guano may be superior under low light settings but inferior under lots of sunlight.  This is an example of the interplay between the fertilizer and the sunlight.

In each of these objectives, there are multiple things at play, requiring modeling techniques that account for multiple predictors simultaneously.  The general linear model views the response as a being the result of a linear combination of several variables; our measurement of this linear combination is then subject to error.  Specifically, the framework generalizes the simple linear regression model studied in introductory statistics to characterize the average response as a function of several variables simultaneously.

:::{#def-general-linear-model}
## General Linear Model
The general linear model views the response (outcome) as a linear combination of several predictors:
  
$$
\begin{aligned}
  (\text{Response})_i 
    &= \beta_0 + \beta_1 (\text{Predictor 1})_{i} + \beta_2 (\text{Predictor 2})_{i} + \dotsb + 
      \beta_p (\text{Predictor } p)_{i} + \varepsilon_i \\
    &= \beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i} + \varepsilon_i
\end{aligned}
$$

where $n$ is the number of subjects in the sample, $p < n$ is the number of predictors in the model, and $\varepsilon_i$ is a random variable that captures the error in the response.
:::

:::{.callout-note}
Many texts use $y_i$ to denote the response of the $i$-th observation and $x_{j,i}$ to denote the value of the $j$-th predictor for the $i$-th subject, resulting in the general linear model having the form

$$y_i = \beta_0 + \sum\limits_{j=1}^{p} \beta_j x_{j, i} + \varepsilon_i.$$

This notation is helpful when discussing the underlying mathematics, but we prefer being more explicit in identifying the response and predictors when discussing the model itself.
:::

:::{.callout-note}
Some disciplines refer to the response/outcome as the "dependent variable" and the predictors as "independent variables," but we find this language a bit dated.

We will use the terms "predictor" and "covariate" interchangeably, while some disciplines distinguish between categorical predictors as factors and continuous predictors as covariates (variables that "co-vary" with the factor of interest).
:::

:::{.callout-note}
While not a theoretical requirement, we will only consider the case where $p < n$, which is common in many disciplines.  One discipline in which this is often not valid is genetics.  Special methods are required in such "high dimensional" settings that are beyond the scope of this text.
:::

The general linear model has two distinct components --- a deterministic component (the linear combination of the predictors) and a stochastic component (the error term).  We can think of the error term as the "junk drawer" for the model, capturing anything not explained by the deterministic portion of the model.  The error could include systematic error in measuring the response, biological error contributing to the fact that two subjects with the same values of the predictors have different responses, etc.  
:::{.callout-caution}
We stress that @def-general-linear-model is a _model_.  Like all models, it is a simple representation of a complex process.  It is something we posit characterizes the underlying data generating process.
:::

The key feature of this model is that it relates the response to several predictors _simultaneously_.  However, this model is currently comprised of unknown parameters (the coefficients $\beta_1, \beta_2, \dotsc, \beta_p$).  For it to be useful in practice, we need estimates of these parameters.



## Parameter Estimation
The coefficients in front of each predictor act as parameters in the model, as they are unknown and characterize the distribution of the response in some way.  Our goal is to construct estimates of these unknown quantities.  The most common method of estimation is the method of least squares.

:::{#def-least-squares}
## Least Squares Estimation
The method of least squares may be used to estimate the coefficients (parameters) of a linear model.  In particular, we choose the values of the coefficients that minimize

$$\sum\limits_{i=1}^{n} \left((\text{Response})_i - \beta_0 - \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_{i}\right)^2.$$

The resulting "least squares" estimates are denoted $\widehat{\beta}_0, \widehat{\beta}_1, \dotsc, \widehat{\beta}_p$.
:::

It is important to remember that the method of least squares results in _estimates_ of the parameters.  We are not "solving" for the parameters; the parameters will always remain unknown quantities.  We are using data to estimate the parameters.  There is really nothing statistical about least squares.  It is simply an optimization problem --- choosing coefficients to minimize some criteria.  Of course, we do not determine these estimates by hand; instead, we rely on statistical software.

We cannot stress enough that the act of obtaining these estimates is simply an optimization exercise.  While a computer can provide these estimates, we cannot yet even interpret these estimates without further assumptions on the model.  This is where it becomes a _statistical_ problem --- specifying the conditions required for the purpose of making inference on the unknown parameters.  


## Conditions on the Model 
The act of estimation alone is really a mathematical problem.  Being able to describe the properties of those estimates, quantify the variability in those estimates, and use those estimates to make inference on the population parameters is where we enter statistics.  Whenever a random variable is present in a model, inference requires us to make assumptions about its underlying distribution. As analysts, we balance making inference easy mathematically by making more assumptions (adding more structure to the model) and making the model more flexible (not making the model too restrictive).

Most software, by default, places four conditions on the distribution of the error term in the model.  We refer to this collection of conditions as the "classical regression model."

:::{#def-classical-regression}
## Classical Regression Model
In the "classical regression model," we place the following four conditions on the distribution of the error $\varepsilon_i$:

  1. The average error across all levels of the predictors is 0; mathematically, we write $E\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = 0$.
  2. The variance of the errors is constant across all levels of the predictors; mathematically, we write $Var\left(\varepsilon_i \mid (\text{Predictors 1 - }p)_i\right) = \sigma^2$ for some unknown constant $\sigma^2 > 0$.  This is sometimes referred to as homoskedasticity.
  3. The error terms are independent; in particular, the magnitude of the error for one observation does not influence the magnitude of the error for any other observation.
  4. The distribution of the errors follows a Normal distribution with the above mean and variance.

:::

It would be a mistake to consider the above conditions only from a probabilistic perspective; wrestling with what these mean in practice is critical to understanding the model.

The first condition says the structure of the model is correct; that is, no variables were omitted and the functional form of the response is really determined by a linear combination of the predictors.  Violations of this assumption are very serious and indicate a different model structure is needed.  Essentially, if we believe this condition is not met, it means we should revisit the science and rationale behind the proposed model because it is likely invalid.

The second condition considers the precision with which the response is measured.  The condition asserts that this precision is consistent across all possible values for the predictors.  For example, consider the academic performance of two classes; this condition prohibits cases in which the grades for one class have a wider range than the grades for the other.

The third condition eliminates data for which measurements are related beyond sharing common values of the predictors in the model.  For example, suppose we are modeling the height of a tree as a function of its age.  All trees of a similar age may be "related" in the sense that we expect them to have similar heights; the model allows this.  However, it does not allow for trees being "related" in the sense that trees in a similar region will share a similar height due to differences in resources among regions; this is prohibited because "region" is not captured by the model.  In the biological sciences, this condition is often called into question when we take repeated measurements on subjects or when observations are measured close together in time.  This type of data will be addressed later in the text (@sec-rm-terminology).

The last condition is a strong one; it states that we are able to fully characterize the distribution of the error terms.  While the other conditions describe certain characteristics of the distribution, this says we know the exact form of the distribution.  Historically, this condition was imposed to ensure the error terms were well behaved (and because the probability theory worked out nicely).  
        
Statistics courses (especially the introductory course) focus on these four conditions on the error.  However, the classical framework typically also imposes additional conditions on the predictors.

:::{#def-classical-regression-cont}
## Classical Regression (Conditions on Predictors)
The classical regression model (@def-classical-regression) places the following conditions on the predictors:

  1. Each predictor is measured without error.
  2. Each predictor has an additive linear effect on the response.

:::

The first condition states that there cannot be any noise present in the measurement of the _predictors_.  For example, imagine modeling the length (or height) of infants as a function of their age.  When the doctor asks for the age of the child, we are assuming that this age can be computed/measured without error.  This seems reasonable when the predictor is age.  However, consider using the temperature of the infant as a predictor in the model; if the thermometer is only accurate to within 2 tenths of a degree, than we may believe that the body temperature is measured with error.  Addressing measurement error in models is beyond the scope of this text, and it is in general a difficult problem.  Typically, even if a predictor is potentially measured with error, we are able to assume the error is negligible compared to the amount of error in the response.  Throughout the text, we will assume all predictors are measured without error.

The second condition on the predictors is very closely related to the condition on the errors that the mean of the errors is 0.  If we are empirically building a model and find evidence that the model has been mis-specified, it is generally a result of the predictors not having a linear relationship with the response.


## Alternate Characterization of the Model
Recall that a distribution is just the pattern of variability among the values of a variable; that is, a distribution describes how values differ from one another.  @sec-essential-probability presented probability tools that can be used to model these distributions.  We saw that it is possible to specify these models up to some unknown parameters; for example, we may write $X \sim N\left(\mu, \sigma^2\right)$ in order to say the density of the random variable $X$ can be modeled using the following mathematical formula:

$$\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2}.$$

We often think about these parameters as being a single value, but nothing prohibits that value from being described by a function of variables.  That is, we could let $\mu = g(\text{Predictors})$ for some function $g$.  In fact, the conditions on the error term specified in the previous section lead us to an alternate characterization of the general linear model.

:::{#def-alternate-characterization}
## Alternate Characterization of the Classical Regression Model
Under the classical regression conditions on the error term (see @def-classical-regression), we can characterize the classical regression model as

$$(\text{Response})_i \mid (\text{Predictors 1 through } p)_i \dist{Ind} N\left(\beta_0 + \sum\limits_{j=1}^{p} \beta_j (\text{Predictor } j)_i, \sigma^2\right).$$

Here, the symbol $\mid$ is read "given" and means that the distribution of the response is specified after knowing the values of the predictors.  That is, the distribution of the response depends on these variables.
:::

The alternate characterization of the regression model in @def-alternate-characterization is particularly useful in statistical theory, but that is not why we mention it here.  We mention this form because it sheds light on the true nature of regression models (beyond just the classical regression model) --- regression models characterize the distribution of the response.

:::{.callout-tip}
## Big Idea
Regression models allow the parameters characterizing the distribution of the population to depend on the predictors through some function.
:::

Closely examining @def-alternate-characterization, we see that the deterministic portion of the general linear model is actually characterizing the _mean_ of the response (for specified values of the predictors).  In fact, this realization is actually the direct result of the first ("mean 0") condition we placed on the error terms.  This is what allows us to begin interpreting the parameters in the model.


## Interpretation of Parameters
When we assume that the error in the response, on average, is 0 for all values of the predictor, we are really saying that the deterministic portion of the model defines the mean response.  We see this in the alternate characterization of the regression model above where $\mu$ in the Gaussian (Normal) model is replaced by

$$\beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i.$$

Notice what happens if we plug zero in for _every_ predictor:

$$\beta_0 + \sum_{j=1}^{p} \beta_j (0) = \beta_0.$$

Since this deterministic portion specifies the average response, then we see that the average response is $\beta_0$ when all predictors have the value zero.

:::{#def-intercept}
## Intercept
The population intercept, denoted $\beta_0$, is the _mean_ response when all predictors take the value zero.  
:::

We should point out that while this is the correct interpretation, it may not always make sense in context.  For example, if we are modeling the heart rate of patients as a function of their body temperature and weight; the model would have the form

$$(\text{Heart Rate})_i = \beta_0 + \beta_1 (\text{Body Temperature})_i + \beta_2 (\text{Weight})_i + \varepsilon_i.$$

Based on @def-intercept, we would interpret the intercept in this model as the average heart rate for individuals with a body temperature of zero degrees and a weight of zero pounds; as this group of individuals does not exist, the interpretation does not make sense in this context.

We now turn to considering an interpretation for the slope.  Consider two groups of individuals:

  - Group 1 has the value $a$ for the first predictor and value $x_j$ for Predictor $j$ (for $j = 2, \dotsc, p$).
  - Group 2 has the value $a + 1$ for the first predictor and value $x_j$ for Predictor $j$ for $j = 2, \dotsc, p$.

That is, the only way the two groups differ is that Group 2 has increased the value of the first predictor by 1.  From our model, we have that the average response for Group 1 is

$$\beta_0 + \beta_1 a + \sum_{j=2}^{p} \beta_j x_j.$$

The average response for Group 2 is

$$\beta_0 + \beta_1 (a + 1) \sum_{j=2}^{p} \beta_j x_j.$$

Consider taking the difference in these two _mean_ responses (Group 2 minus Group 1):

$$\beta_0 + \beta_1 (a + 1) \sum_{j=2}^{p} \beta_j x_j - \left(\beta_0 + \beta_1 a + \sum_{j=2}^{p} \beta_j x_j\right) = \beta_1.$$

That is, the slope is the difference in the _mean_ response between the two groups.

:::{#def-slope}
## Slope
The coefficient for the $j$-th predictor, denoted $\beta_j$, is the change in the mean response associated with a one unit increase in Predictor $j$, _holding all other predictors fixed_.
:::

The last part of @def-slope is a critical part of the interpretation, and it is critical to the full utility of regression models.  Again, while holding all other predictors fixed may not be practically feasible (for example, could we really increase an individual's height without also increasing their weight), it allows us to investigate the impact of a predictor separate from other variables.

Interpretation of the parameters is a large step beyond simply estimating the parameters.  However, we still have not developed the tools to do much beyond estimation.  We now turn our attention to inference.


## Inference About the Mean Parameters
As suggested in @sec-distributional-quartet, the key to making formal inference on the parameters of a population is to develop a model for the sampling distribution (or null distribution) of the corresponding statistics.  Under the classical regression conditions of @def-classical-regression, we are able to form an exact model for the sampling distribution of the least squares estimates.  

:::{.callout-note}
While beyond the scope of this course, it can be shown that the least squares estimates of the parameters are linear combinations of the observed responses.  This, combined with the modeling assumptions, allows us to construct a model for the sampling distribution of the estimates.
:::

:::{#def-ls-sampling-distribution}
## Sampling Distribution of the Least Squares Estimates
Under the classical regression conditions (@def-classical-regression), we have that

$$\frac{\widehat{\beta}_j - \beta_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}} \sim t_{n - p - 1}.$$

The denominator $\sqrt{Var\left(\widehat{\beta}_j\right)}$ is known as the _standard error_ of the estimate $\widehat{\beta}_j$.  This formula holds for all $j = 0, 1, \dotsc, p$.
:::

@def-ls-sampling-distribution states the standardized difference between our estimate and the parameter follows a t-distribution, where the degrees of freedom depend on the sample size and the number of parameters in the model.  The specific model is not as important as knowing that under the classical regression conditions, an exact model is known.  Nearly every software package that implements regression does so under the classical regression conditions, and the inference is based on the above model for the sampling distribution.

The detail-oriented reader will note that we did not include a formula for the standard error of an estimate.  The formula is beyond the scope of this course, but it is a function of the values of the predictor as well as the variability in the error term.  You see, the moment we specified the second condition ("constant variance"), we introduced another parameter: $\sigma^2$.  The parameter $\sigma^2$ does not govern the mean response; so, it tends to be of less direct interest for our purposes.  Instead, it characterizes the variability in the response (for a given set of predictors), and it plays a role in inference (as we see in the above model for the sampling distribution of the least squares estimates of the parameters in the mean model).  It will therefore play a role in computing confidence intervals and p-values.  Since it is unknown, it must also be estimated.

:::{#def-estimate-sigma2}
## Estimate of the Variance of the Errors
The unknown variance in the linear model, which captures the variability in the response for any set of predictors (also called the residual variance), is estimated by

$$\widehat{\sigma}^2 = \frac{1}{n-p-1} \sum\limits_{i=1}^{n} \left((\text{Response})_i - \widehat{\beta}_0 - \sum\limits_{j=1}^{p} \widehat{\beta}_j (\text{Predictor } j)_{i}\right)^2.$$
:::

Note that the estimate of the variance depends upon the least squares estimates.  Of more interest is that the scaling factor $(n - p - 1)$ is the same as the degrees of freedom for the sampling distribution; that is not an accident.

A model for the sampling distribution is the holy grail of statistical inference.  It can be updated to determine the model for the null distribution.  And, once you have a model for the sampling distribution in hand, you can wield it to construct a confidence interval (and null distributions to yield p-values).

:::{#def-classical-ci}
## Confidence Interval for Parameters Under Classical Model
Under the classical regression conditions (@def-classical-regression), a $100c$% confidence interval for the parameter $\beta_j$ is given by

$$\widehat{\beta}_j \pm t_{n-p-1, 0.5(1+c)} \sqrt{Var\left(\widehat{\beta}_j\right)}.$$

where $t_{n-p-1, 0.5(1+c)}$ is the $0.5(1+c)$ quantile from the $t_{n-p-1}$ distribution, known as the critical value for the confidence interval.  
:::

Like many confidence intervals, the idea is that we are grabbing the middle portion of the model for the sampling distribution.  The confidence interval represents the values of the parameter for which the data is consistent --- the reasonable values of the parameter based on the observed data.  Also note that this confidence interval is specified for each parameter individually.

:::{.callout-note}
For large values of $n$ relative to $p$, the critical value for a 95% confidence interval is approximately 1.96.  Hence, a rough confidence interval is therefore 2 standard errors in either direction of the point estimate.
:::

:::{#def-classical-p}
## P-Value for Testing if Parameter Belongs in Model Under Classical Model
Under the classical regression conditions (@def-classical-regression), the p-value for testing the hypotheses 

$$H_0: \beta_j = 0 \qquad \text{vs.} \qquad H_1: \beta_j \neq 0$$

is given by 

$$Pr\left(\abs{T} > \abs{\frac{\widehat{\beta}_j}{\sqrt{Var\left(\widehat{\beta}_j\right)}}}\right)$$

where $T \sim t_{n-p-1}$.
:::


@def-classical-p highlights that the null distribution for the standardized ratio is developed by taking the model for the sampling distribution and enforcing the null hypothesis $\left(\beta_j = 0\right)$.  Using this null distribution, our p-value then summarizes how likely it is we would obtain a value of the standardized statistic at least as large of that observed by chance alone when the null hypothesis is true.

The interpretation of the confidence interval and p-value follows the interpretation of the confidence intervals and p-values computed in an introductory course (and reviewed in @sec-statistical-process.  This section just establishes that the conditions we placed on the error term yield explicit formulas for their computation (even if these formulas are implemented in the background of the software).

The framework introduced here provides the basics for making inference using a statistical model.  As we consider more flexible modeling strategies, these key concepts do not leave us.  We need a model for the sampling distribution or null distribution in order to make inference.  And, the model for that distribution depends on the conditions we are willing to make.

:::{.callout-tip}
## Big Idea
A model for the sampling distribution (and/or null distribution) is needed for making inference, and that model depends on the conditions we are willing to impose on the model for the data generating process.
:::
