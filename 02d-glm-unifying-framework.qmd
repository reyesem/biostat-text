# The General Linear Model as a Unifying Framework {#sec-glm-unifying-framework}

{{< include _setupcode.qmd >}}

The general linear model (@def-general-linear-model) is much more flexible and powerful than we might initially imagine.  The full flexibility of this modeling framework is explored in the next unit.  It should be clear that simple linear regression is a special case of the general linear model for which we only have a single predictor.  In the remainder of this chapter, we outline how the methods typically studied in an introductory course also relate to the general linear model framework.

## One Sample Inference
Perhaps the most cited analysis technique from an introductory statistics course is the "1-sample t-test."  In brief, this test considers the hypotheses

$$H_0: \mu = \mu_0 \qquad \text{vs.} \qquad H_1: \mu \neq \mu_0$$

where $\mu$ is the average response in the population.  These hypotheses are making inference on the mean response of a single population (or "one sample").  A classical introductory statistics course will introduce the test statistic

$$T^* = \frac{\sqrt{n} \left(\bar{y} - \mu_0\right)}{s}$$

where $\bar{y}$ and $s$ represent the sample mean and sample standard deviation, respectively, of the response.  The one-sample t-test proceeds to model the (null) distribution of $T^*$ as a t-distribution with $n - 1$ degrees of freedom.  This test assumes that

  1. The response for one observation is independent of the response for all other observations.
  2. The responses are identically distributed.
  3. The responses follow a Normal distribution.
  
We can recover the same analysis using the general linear model.  Consider the model

$$(\text{Response})_i = \mu + \varepsilon_i.$$

This is sometimes referred to as the "intercept-only" model.  It can be shown that the least-squares estimate of $\mu$ is the sample mean of the response.  Similarly, our estimate of $\sigma^2$ is the sample variance.  Further, we have that

$$Var\left(\widehat{\mu}\right) = \frac{s^2}{n},$$

meaning the standardized statistic in @def-ls-sampling-distribution is equivalent to the ratio taught in the introductory statistics course.  

Having the same standardized statistic is one thing, but the analysis is only equivalent if the conditions imposed also agree.  If you consider the classical regression conditions (@def-classical-regression), then given that there is no predictor in the model, the conditions would translate to

  1. The response for one observation is independent of the response for all other observations.
  2. The responses are identically distributed.
  3. The responses follow a Normal distribution.
  
You may wonder where the "mean 0" condition went.  Assuming the error is 0 on average is equivalent to assuming the deterministic portion of the model is correctly specified.  In the case when the deterministic model does not have a predictor, we are essentially assuming that $\mu$ is the average (the sample is representative of the underlying population).  In particular, since we made no simplifying assumptions about the structure of the relationship between the response and predictor (since there is no predictor), those simplifying assumptions cannot be incorrect.  

:::{.callout-tip}
## Big Idea
A one-sample t-test is equivalent to running an intercept-only model in the general linear model framework under the classical conditions.
:::


## Paired t-Test
Most analyses covered in an introductory class assume independence between all observations.  The most notable exception is the "paired t-test."  In this scenario, we collect a total of $2n$ observations, but for a total of $n$ independent pairs.  For example, $n$ participants complete a pre and post test.  Or, we measure a response on both the left and right eye for $n$ participants.  In each of these examples, there are two measurements for each of the $n$ participants that we believe are related (general ability with exams means some students will naturally score higher; genetics result in some individuals having better vision; etc.).  The typical way of addressing this problem in the introductory course is to take the difference in the response within each pair, and then conduct a one-sample t-test.  That is, we consider

$$T^* = \frac{\sqrt{n}\left(\bar{y}_d - \mu_{d,0}\right)}{s_d},$$

where $\bar{y}_d$ is the sample mean of the differences in the response, $s_d$ is the sample standard deviation of the differences, and $n$ is the number of paired observations.

Since this is a one sample test, building on our discussion earlier in this chapter, we have that an equivalent approach is to consider the model

$$(\text{Difference in Responses})_i = \mu + \varepsilon_i$$

with the classical conditions imposed.  

The paired t-test is actually a special case of a "repeated measures ANOVA," which can also be viewed as a special case of the general linear model.  We discuss repeated measures and appropriate approaches for analysis in a future unit.

:::{.callout-tip}
## Big Idea
A paired t-test is equivalent to running an intercept-only model, with the pairwise differences as the response, in the general linear model framework under the classical conditions.
:::


## Group Comparisons
Suppose we are interested in comparing the average response across two or more groups.  If there are only two groups, our hypotheses take the form

$$H_0: \mu_1 = \mu_2 \qquad \text{vs.} \qquad H_1: \mu_1 \neq \mu_2,$$

where $\mu_1$ and $\mu_2$ represent the average response for each of the two groups.  The two-sample t-test is typically discussed to address this question where

$$T^* = \frac{\left(\bar{y}_2 - \bar{y}_1\right)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}},$$

where $\bar{y}_j$ and $s^2_j$ are the sample mean and sample variance of the response within group $j$ ($j = 1, 2$), respectively.  A t-distribution is used to model the distribution of this standardized statistic under the following conditions:

  1. The response from one observation is independent of the response of all other observations (implying independence both within and between groups).
  2. The responses _within_ a group are identically distributed.
  3. The responses within each group follow a Normal distribution.

When there are three or more groups, our hypotheses take the form

$$H_0: \mu_1 = \mu_2 = \dotsb = \mu_k \qquad \text{vs.} \qquad H_1: \text{at least 1 } \mu_j \text{ differs},$$

where $\mu_j$ represents the average response for group $j$, $j = 1, 2, \dotsc, k$.  Analysis of variance (ANOVA) is typically used to address this question.  Typically, ANOVA imposes the following conditions:

  1. The response from one observation is independent of the response of all other observations (implying independence both within and between groups).
  2. The variability in the response within a group is the same for each group.
  3. The responses within each group follow a Normal distribution.
  
It would seem that the two-group comparison is a special case of ANOVA; however, as stated above, they would yield slightly different inference because the conditions imposed differ.  Specifically, ANOVA assumes the variance of the response in a group is the same across groups; however, the two-sample t-test allows the variance of the response to differ across groups.

:::{.callout-note}
There is a version of the two-sample t-test which uses the "pooled" sample variance; in that case, the variance of the response is assumed to be the same within each group and the two-sample t-test is a special case of ANOVA.
:::

It would therefore seem that there is no way these methods relate to the general linear model framework.  However, the general linear model framework does encompass both approaches.  An ANOVA can be accomplished by inserting a single categorical predictor into the model capturing the grouping structure (see @sec-modeling-categorical-predictors).  Since the classical regression assumptions correspond to ANOVA, we recover the same inference.  The two-group comparison requires that we additionally relax the constant variance condition, which we discuss in @sec-nlm-heteroskedasticity (while discussed in the context of non-linear models, the same techniques apply to the general linear model as well).

:::{.callout-tip}
## Big Idea
ANOVA is equivalent to running the general linear model with a single categorical predictor under the classical conditions.
:::